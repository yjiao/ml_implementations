{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import resnet\n",
    "import imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", utils.get_num_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images, train_labels, test_labels = resnet.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe9d0965358>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([utils.lr_schedule(cur_it=i, max_it=100, max_warm_it=10) for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╭┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╮\n",
      "                         n: 9, res: True\n",
      "size of reduce_mean: (?, 1, 1, 64)\n",
      "size of squeeze: (?, 64)\n",
      "training for 78125 iterations\n",
      "                          epoch 0\n",
      "iteration 100, loss 2.0810329914093018\n",
      "iteration 200, loss 1.8318142890930176\n",
      "iteration 300, loss 1.7563254833221436\n",
      "TEST accuracy at end of epoch 0: 0.1545\n",
      "TRAIN accuracy at end of epoch 0: 0.17\n",
      "                          epoch 1\n",
      "iteration 400, loss 1.4655488729476929\n",
      "iteration 500, loss 1.4561790227890015\n",
      "iteration 600, loss 1.265447974205017\n",
      "iteration 700, loss 1.2727869749069214\n",
      "TEST accuracy at end of epoch 1: 0.3281\n",
      "TRAIN accuracy at end of epoch 1: 0.29\n",
      "                          epoch 2\n",
      "iteration 800, loss 1.2178088426589966\n",
      "iteration 900, loss 1.1996657848358154\n",
      "iteration 1000, loss 0.9955095648765564\n",
      "iteration 1100, loss 0.7054802179336548\n",
      "TEST accuracy at end of epoch 2: 0.5078\n",
      "TRAIN accuracy at end of epoch 2: 0.47\n",
      "                          epoch 3\n",
      "iteration 1200, loss 0.8328412175178528\n",
      "iteration 1300, loss 0.7229571342468262\n",
      "iteration 1400, loss 0.7418450117111206\n",
      "iteration 1500, loss 0.8363854289054871\n",
      "TEST accuracy at end of epoch 3: 0.6443\n",
      "TRAIN accuracy at end of epoch 3: 0.68\n",
      "                          epoch 4\n",
      "iteration 1600, loss 0.822921633720398\n",
      "iteration 1700, loss 0.5947386026382446\n",
      "iteration 1800, loss 0.7705923318862915\n",
      "iteration 1900, loss 0.634823203086853\n",
      "TEST accuracy at end of epoch 4: 0.6655\n",
      "TRAIN accuracy at end of epoch 4: 0.69\n",
      "                          epoch 5\n",
      "iteration 2000, loss 0.6098077893257141\n",
      "iteration 2100, loss 0.6967641711235046\n",
      "iteration 2200, loss 0.5918372273445129\n",
      "iteration 2300, loss 0.5250813961029053\n",
      "TEST accuracy at end of epoch 5: 0.6528\n",
      "TRAIN accuracy at end of epoch 5: 0.69\n",
      "                          epoch 6\n",
      "iteration 2400, loss 0.5099865198135376\n",
      "iteration 2500, loss 0.5724314451217651\n",
      "iteration 2600, loss 0.5659054517745972\n",
      "iteration 2700, loss 0.5155751705169678\n",
      "TEST accuracy at end of epoch 6: 0.6854\n",
      "TRAIN accuracy at end of epoch 6: 0.68\n",
      "                          epoch 7\n",
      "iteration 2800, loss 0.40584760904312134\n",
      "iteration 2900, loss 0.4068230092525482\n",
      "iteration 3000, loss 0.5219168663024902\n",
      "iteration 3100, loss 0.44722670316696167\n",
      "TEST accuracy at end of epoch 7: 0.7485\n",
      "TRAIN accuracy at end of epoch 7: 0.75\n",
      "                          epoch 8\n",
      "iteration 3200, loss 0.6409109830856323\n",
      "iteration 3300, loss 0.46436935663223267\n",
      "iteration 3400, loss 0.4464671015739441\n",
      "iteration 3500, loss 0.5585699677467346\n",
      "TEST accuracy at end of epoch 8: 0.7485\n",
      "TRAIN accuracy at end of epoch 8: 0.81\n",
      "                          epoch 9\n",
      "iteration 3600, loss 0.3692832589149475\n",
      "iteration 3700, loss 0.5145970582962036\n",
      "iteration 3800, loss 0.44186681509017944\n",
      "iteration 3900, loss 0.4130651652812958\n",
      "TEST accuracy at end of epoch 9: 0.8374\n",
      "TRAIN accuracy at end of epoch 9: 0.84\n",
      "                          epoch 10\n",
      "iteration 4000, loss 0.3025592565536499\n",
      "iteration 4100, loss 0.46978282928466797\n",
      "iteration 4200, loss 0.4100261628627777\n",
      "iteration 4300, loss 0.41993582248687744\n",
      "TEST accuracy at end of epoch 10: 0.786\n",
      "TRAIN accuracy at end of epoch 10: 0.75\n",
      "                          epoch 11\n",
      "iteration 4400, loss 0.33921265602111816\n",
      "iteration 4500, loss 0.4353182017803192\n",
      "iteration 4600, loss 0.36165377497673035\n",
      "TEST accuracy at end of epoch 11: 0.7818\n",
      "TRAIN accuracy at end of epoch 11: 0.81\n",
      "                          epoch 12\n",
      "iteration 4700, loss 0.4052314758300781\n",
      "iteration 4800, loss 0.2916630804538727\n",
      "iteration 4900, loss 0.50284743309021\n",
      "iteration 5000, loss 0.23543193936347961\n",
      "TEST accuracy at end of epoch 12: 0.7921\n",
      "TRAIN accuracy at end of epoch 12: 0.81\n",
      "                          epoch 13\n",
      "iteration 5100, loss 0.3004847466945648\n",
      "iteration 5200, loss 0.3528692126274109\n",
      "iteration 5300, loss 0.3932567536830902\n",
      "iteration 5400, loss 0.3527899384498596\n",
      "TEST accuracy at end of epoch 13: 0.8316\n",
      "TRAIN accuracy at end of epoch 13: 0.88\n",
      "                          epoch 14\n",
      "iteration 5500, loss 0.31207597255706787\n",
      "iteration 5600, loss 0.27927935123443604\n",
      "iteration 5700, loss 0.3669687807559967\n",
      "iteration 5800, loss 0.3707529902458191\n",
      "TEST accuracy at end of epoch 14: 0.7804\n",
      "TRAIN accuracy at end of epoch 14: 0.85\n",
      "                          epoch 15\n",
      "iteration 5900, loss 0.3705234229564667\n",
      "iteration 6000, loss 0.293038010597229\n",
      "iteration 6100, loss 0.27789705991744995\n",
      "iteration 6200, loss 0.3272175192832947\n",
      "TEST accuracy at end of epoch 15: 0.835\n",
      "TRAIN accuracy at end of epoch 15: 0.85\n",
      "                          epoch 16\n",
      "iteration 6300, loss 0.3186553716659546\n",
      "iteration 6400, loss 0.34255677461624146\n",
      "iteration 6500, loss 0.30646318197250366\n",
      "iteration 6600, loss 0.2696848213672638\n",
      "TEST accuracy at end of epoch 16: 0.8328\n",
      "TRAIN accuracy at end of epoch 16: 0.8\n",
      "                          epoch 17\n",
      "iteration 6700, loss 0.20112714171409607\n",
      "iteration 6800, loss 0.3844374120235443\n",
      "iteration 6900, loss 0.2264288365840912\n",
      "iteration 7000, loss 0.37646162509918213\n",
      "TEST accuracy at end of epoch 17: 0.8513\n",
      "TRAIN accuracy at end of epoch 17: 0.89\n",
      "                          epoch 18\n",
      "iteration 7100, loss 0.2712399661540985\n",
      "iteration 7200, loss 0.19834288954734802\n",
      "iteration 7300, loss 0.29838114976882935\n",
      "iteration 7400, loss 0.281231552362442\n",
      "TEST accuracy at end of epoch 18: 0.8381\n",
      "TRAIN accuracy at end of epoch 18: 0.88\n",
      "                          epoch 19\n",
      "iteration 7500, loss 0.2299601286649704\n",
      "iteration 7600, loss 0.22570061683654785\n",
      "iteration 7700, loss 0.32799917459487915\n",
      "iteration 7800, loss 0.3048112392425537\n",
      "TEST accuracy at end of epoch 19: 0.8625\n",
      "TRAIN accuracy at end of epoch 19: 0.9\n",
      "                          epoch 20\n",
      "iteration 7900, loss 0.2794036865234375\n",
      "iteration 8000, loss 0.3092597723007202\n",
      "iteration 8100, loss 0.3450399935245514\n",
      "iteration 8200, loss 0.26875919103622437\n",
      "TEST accuracy at end of epoch 20: 0.8568\n",
      "TRAIN accuracy at end of epoch 20: 0.91\n",
      "                          epoch 21\n",
      "iteration 8300, loss 0.34798043966293335\n",
      "iteration 8400, loss 0.32397228479385376\n",
      "iteration 8500, loss 0.21789401769638062\n",
      "iteration 8600, loss 0.41815799474716187\n",
      "TEST accuracy at end of epoch 21: 0.8683\n",
      "TRAIN accuracy at end of epoch 21: 0.94\n",
      "                          epoch 22\n",
      "iteration 8700, loss 0.17122676968574524\n",
      "iteration 8800, loss 0.27886295318603516\n",
      "iteration 8900, loss 0.2511786222457886\n",
      "TEST accuracy at end of epoch 22: 0.8652\n",
      "TRAIN accuracy at end of epoch 22: 0.85\n",
      "                          epoch 23\n",
      "iteration 9000, loss 0.1962650567293167\n",
      "iteration 9100, loss 0.22752851247787476\n",
      "iteration 9200, loss 0.2076307237148285\n",
      "iteration 9300, loss 0.27830931544303894\n",
      "TEST accuracy at end of epoch 23: 0.85\n",
      "TRAIN accuracy at end of epoch 23: 0.93\n",
      "                          epoch 24\n",
      "iteration 9400, loss 0.21221496164798737\n",
      "iteration 9500, loss 0.2656424045562744\n",
      "iteration 9600, loss 0.2447759211063385\n",
      "iteration 9700, loss 0.21943528950214386\n",
      "TEST accuracy at end of epoch 24: 0.8201\n",
      "TRAIN accuracy at end of epoch 24: 0.92\n",
      "                          epoch 25\n",
      "iteration 9800, loss 0.19380687177181244\n",
      "iteration 9900, loss 0.2705281674861908\n",
      "iteration 10000, loss 0.22041834890842438\n",
      "iteration 10100, loss 0.24979834258556366\n",
      "TEST accuracy at end of epoch 25: 0.8386\n",
      "TRAIN accuracy at end of epoch 25: 0.89\n",
      "                          epoch 26\n",
      "iteration 10200, loss 0.17003761231899261\n",
      "iteration 10300, loss 0.16750019788742065\n",
      "iteration 10400, loss 0.21803614497184753\n",
      "iteration 10500, loss 0.22454766929149628\n",
      "TEST accuracy at end of epoch 26: 0.852\n",
      "TRAIN accuracy at end of epoch 26: 0.89\n",
      "                          epoch 27\n",
      "iteration 10600, loss 0.14748650789260864\n",
      "iteration 10700, loss 0.18495365977287292\n",
      "iteration 10800, loss 0.11876727640628815\n",
      "iteration 10900, loss 0.2739853858947754\n",
      "TEST accuracy at end of epoch 27: 0.8625\n",
      "TRAIN accuracy at end of epoch 27: 0.93\n",
      "                          epoch 28\n",
      "iteration 11000, loss 0.1809672713279724\n",
      "iteration 11100, loss 0.21339444816112518\n",
      "iteration 11200, loss 0.2077578604221344\n",
      "iteration 11300, loss 0.10987953841686249\n",
      "TEST accuracy at end of epoch 28: 0.8586\n",
      "TRAIN accuracy at end of epoch 28: 0.95\n",
      "                          epoch 29\n",
      "iteration 11400, loss 0.07931053638458252\n",
      "iteration 11500, loss 0.11211446672677994\n",
      "iteration 11600, loss 0.1041434034705162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 11700, loss 0.40020331740379333\n",
      "TEST accuracy at end of epoch 29: 0.8815\n",
      "TRAIN accuracy at end of epoch 29: 0.99\n",
      "                          epoch 30\n",
      "iteration 11800, loss 0.19904224574565887\n",
      "iteration 11900, loss 0.19754143059253693\n",
      "iteration 12000, loss 0.2714303731918335\n",
      "iteration 12100, loss 0.20741890370845795\n",
      "TEST accuracy at end of epoch 30: 0.8281\n",
      "TRAIN accuracy at end of epoch 30: 0.87\n",
      "                          epoch 31\n",
      "iteration 12200, loss 0.15642255544662476\n",
      "iteration 12300, loss 0.18948513269424438\n",
      "iteration 12400, loss 0.2811538577079773\n",
      "iteration 12500, loss 0.1331200748682022\n",
      "TEST accuracy at end of epoch 31: 0.873\n",
      "TRAIN accuracy at end of epoch 31: 0.92\n",
      "                          epoch 32\n",
      "iteration 12600, loss 0.09586280584335327\n",
      "iteration 12700, loss 0.1989830881357193\n",
      "iteration 12800, loss 0.135698139667511\n",
      "iteration 12900, loss 0.14601783454418182\n",
      "TEST accuracy at end of epoch 32: 0.8821\n",
      "TRAIN accuracy at end of epoch 32: 0.95\n",
      "                          epoch 33\n",
      "iteration 13000, loss 0.17795950174331665\n",
      "iteration 13100, loss 0.19037961959838867\n",
      "iteration 13200, loss 0.30330145359039307\n",
      "TEST accuracy at end of epoch 33: 0.8733\n",
      "TRAIN accuracy at end of epoch 33: 0.95\n",
      "                          epoch 34\n",
      "iteration 13300, loss 0.12089184671640396\n",
      "iteration 13400, loss 0.10899491608142853\n",
      "iteration 13500, loss 0.16499321162700653\n",
      "iteration 13600, loss 0.16345763206481934\n",
      "TEST accuracy at end of epoch 34: 0.8728\n",
      "TRAIN accuracy at end of epoch 34: 0.97\n",
      "                          epoch 35\n",
      "iteration 13700, loss 0.15669426321983337\n",
      "iteration 13800, loss 0.20722293853759766\n",
      "iteration 13900, loss 0.11489282548427582\n",
      "iteration 14000, loss 0.08698854595422745\n",
      "TEST accuracy at end of epoch 35: 0.8774\n",
      "TRAIN accuracy at end of epoch 35: 0.96\n",
      "                          epoch 36\n",
      "iteration 14100, loss 0.12078242003917694\n",
      "iteration 14200, loss 0.20999735593795776\n",
      "iteration 14300, loss 0.16469159722328186\n",
      "iteration 14400, loss 0.19714516401290894\n",
      "TEST accuracy at end of epoch 36: 0.8421\n",
      "TRAIN accuracy at end of epoch 36: 0.93\n",
      "                          epoch 37\n",
      "iteration 14500, loss 0.14262542128562927\n",
      "iteration 14600, loss 0.16813042759895325\n",
      "iteration 14700, loss 0.11180709302425385\n",
      "iteration 14800, loss 0.07270938158035278\n",
      "TEST accuracy at end of epoch 37: 0.8804\n",
      "TRAIN accuracy at end of epoch 37: 0.97\n",
      "                          epoch 38\n",
      "iteration 14900, loss 0.11357993632555008\n",
      "iteration 15000, loss 0.16670860350131989\n",
      "iteration 15100, loss 0.07986262440681458\n",
      "iteration 15200, loss 0.13044627010822296\n",
      "TEST accuracy at end of epoch 38: 0.864\n",
      "TRAIN accuracy at end of epoch 38: 0.94\n",
      "                          epoch 39\n",
      "iteration 15300, loss 0.11407683789730072\n",
      "iteration 15400, loss 0.08659355342388153\n",
      "iteration 15500, loss 0.16173897683620453\n",
      "iteration 15600, loss 0.08418671786785126\n",
      "TEST accuracy at end of epoch 39: 0.8656\n",
      "TRAIN accuracy at end of epoch 39: 0.96\n",
      "                          epoch 40\n",
      "iteration 15700, loss 0.10457035899162292\n",
      "iteration 15800, loss 0.07475286722183228\n",
      "iteration 15900, loss 0.12937355041503906\n",
      "iteration 16000, loss 0.09295567870140076\n",
      "TEST accuracy at end of epoch 40: 0.8791\n",
      "TRAIN accuracy at end of epoch 40: 0.94\n",
      "                          epoch 41\n",
      "iteration 16100, loss 0.15954488515853882\n",
      "iteration 16200, loss 0.10337129235267639\n",
      "iteration 16300, loss 0.12719325721263885\n",
      "iteration 16400, loss 0.10929916799068451\n",
      "TEST accuracy at end of epoch 41: 0.8685\n",
      "TRAIN accuracy at end of epoch 41: 0.96\n",
      "                          epoch 42\n",
      "iteration 16500, loss 0.16425542533397675\n",
      "iteration 16600, loss 0.06167904660105705\n",
      "iteration 16700, loss 0.20941975712776184\n",
      "iteration 16800, loss 0.08858146518468857\n",
      "TEST accuracy at end of epoch 42: 0.8723\n",
      "TRAIN accuracy at end of epoch 42: 0.89\n",
      "                          epoch 43\n",
      "iteration 16900, loss 0.18294142186641693\n",
      "iteration 17000, loss 0.14160792529582977\n",
      "iteration 17100, loss 0.07962112873792648\n",
      "iteration 17200, loss 0.0772409588098526\n",
      "TEST accuracy at end of epoch 43: 0.8824\n",
      "TRAIN accuracy at end of epoch 43: 0.93\n",
      "                          epoch 44\n",
      "iteration 17300, loss 0.112956203520298\n",
      "iteration 17400, loss 0.1524885594844818\n",
      "iteration 17500, loss 0.062265124171972275\n",
      "TEST accuracy at end of epoch 44: 0.8753\n",
      "TRAIN accuracy at end of epoch 44: 0.95\n",
      "                          epoch 45\n",
      "iteration 17600, loss 0.08085654675960541\n",
      "iteration 17700, loss 0.19405615329742432\n",
      "iteration 17800, loss 0.10607528686523438\n",
      "iteration 17900, loss 0.08982419967651367\n",
      "TEST accuracy at end of epoch 45: 0.8813\n",
      "TRAIN accuracy at end of epoch 45: 0.96\n",
      "                          epoch 46\n",
      "iteration 18000, loss 0.15137819945812225\n",
      "iteration 18100, loss 0.07562130689620972\n",
      "iteration 18200, loss 0.1463075578212738\n",
      "iteration 18300, loss 0.0424736887216568\n",
      "TEST accuracy at end of epoch 46: 0.8807\n",
      "TRAIN accuracy at end of epoch 46: 0.95\n",
      "                          epoch 47\n",
      "iteration 18400, loss 0.10054714232683182\n",
      "iteration 18500, loss 0.1537463366985321\n",
      "iteration 18600, loss 0.10365107655525208\n",
      "iteration 18700, loss 0.07299989461898804\n",
      "TEST accuracy at end of epoch 47: 0.87\n",
      "TRAIN accuracy at end of epoch 47: 0.92\n",
      "                          epoch 48\n",
      "iteration 18800, loss 0.10187946259975433\n",
      "iteration 18900, loss 0.14259779453277588\n",
      "iteration 19000, loss 0.08458364009857178\n",
      "iteration 19100, loss 0.15969040989875793\n",
      "TEST accuracy at end of epoch 48: 0.8822\n",
      "TRAIN accuracy at end of epoch 48: 0.94\n",
      "                          epoch 49\n",
      "iteration 19200, loss 0.1465560793876648\n",
      "iteration 19300, loss 0.0884425938129425\n",
      "iteration 19400, loss 0.08810289204120636\n",
      "iteration 19500, loss 0.0930362194776535\n",
      "TEST accuracy at end of epoch 49: 0.8756\n",
      "TRAIN accuracy at end of epoch 49: 0.93\n",
      "                          epoch 50\n",
      "iteration 19600, loss 0.067202128469944\n",
      "iteration 19700, loss 0.1580532193183899\n",
      "iteration 19800, loss 0.08149152994155884\n",
      "iteration 19900, loss 0.07564980536699295\n",
      "TEST accuracy at end of epoch 50: 0.8887\n",
      "TRAIN accuracy at end of epoch 50: 0.94\n",
      "                          epoch 51\n",
      "iteration 20000, loss 0.11610305309295654\n",
      "iteration 20100, loss 0.1396523416042328\n",
      "iteration 20200, loss 0.12233376502990723\n",
      "iteration 20300, loss 0.05493193864822388\n",
      "TEST accuracy at end of epoch 51: 0.887\n",
      "TRAIN accuracy at end of epoch 51: 0.97\n",
      "                          epoch 52\n",
      "iteration 20400, loss 0.098501056432724\n",
      "iteration 20500, loss 0.12283182144165039\n",
      "iteration 20600, loss 0.14300085604190826\n",
      "iteration 20700, loss 0.1498863846063614\n",
      "TEST accuracy at end of epoch 52: 0.8857\n",
      "TRAIN accuracy at end of epoch 52: 0.97\n",
      "                          epoch 53\n",
      "iteration 20800, loss 0.0619942769408226\n",
      "iteration 20900, loss 0.13921573758125305\n",
      "iteration 21000, loss 0.11882179230451584\n",
      "iteration 21100, loss 0.0883721113204956\n",
      "TEST accuracy at end of epoch 53: 0.8865\n",
      "TRAIN accuracy at end of epoch 53: 0.96\n",
      "                          epoch 54\n",
      "iteration 21200, loss 0.10092458128929138\n",
      "iteration 21300, loss 0.11611394584178925\n",
      "iteration 21400, loss 0.062331464141607285\n",
      "iteration 21500, loss 0.10045233368873596\n",
      "TEST accuracy at end of epoch 54: 0.8939\n",
      "TRAIN accuracy at end of epoch 54: 0.98\n",
      "                          epoch 55\n",
      "iteration 21600, loss 0.07360612601041794\n",
      "iteration 21700, loss 0.0842050239443779\n",
      "iteration 21800, loss 0.03921053186058998\n",
      "TEST accuracy at end of epoch 55: 0.8708\n",
      "TRAIN accuracy at end of epoch 55: 0.97\n",
      "                          epoch 56\n",
      "iteration 21900, loss 0.027114734053611755\n",
      "iteration 22000, loss 0.043346572667360306\n",
      "iteration 22100, loss 0.07176346331834793\n",
      "iteration 22200, loss 0.1338750123977661\n",
      "TEST accuracy at end of epoch 56: 0.8894\n",
      "TRAIN accuracy at end of epoch 56: 0.99\n",
      "                          epoch 57\n",
      "iteration 22300, loss 0.10961645096540451\n",
      "iteration 22400, loss 0.0577554926276207\n",
      "iteration 22500, loss 0.07981544733047485\n",
      "iteration 22600, loss 0.05519701540470123\n",
      "TEST accuracy at end of epoch 57: 0.8889\n",
      "TRAIN accuracy at end of epoch 57: 0.96\n",
      "                          epoch 58\n",
      "iteration 22700, loss 0.1443006545305252\n",
      "iteration 22800, loss 0.0634705126285553\n",
      "iteration 22900, loss 0.18747349083423615\n",
      "iteration 23000, loss 0.03835196793079376\n",
      "TEST accuracy at end of epoch 58: 0.8941\n",
      "TRAIN accuracy at end of epoch 58: 0.96\n",
      "                          epoch 59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 23100, loss 0.04566128924489021\n",
      "iteration 23200, loss 0.03779805451631546\n",
      "iteration 23300, loss 0.04725618660449982\n",
      "iteration 23400, loss 0.047941479831933975\n",
      "TEST accuracy at end of epoch 59: 0.8805\n",
      "TRAIN accuracy at end of epoch 59: 0.95\n",
      "                          epoch 60\n",
      "iteration 23500, loss 0.06355144828557968\n",
      "iteration 23600, loss 0.06422510743141174\n",
      "iteration 23700, loss 0.08036823570728302\n",
      "iteration 23800, loss 0.16959065198898315\n",
      "TEST accuracy at end of epoch 60: 0.8869\n",
      "TRAIN accuracy at end of epoch 60: 0.99\n",
      "                          epoch 61\n",
      "iteration 23900, loss 0.05665426701307297\n",
      "iteration 24000, loss 0.11550198495388031\n",
      "iteration 24100, loss 0.05340544879436493\n",
      "iteration 24200, loss 0.10441017895936966\n",
      "TEST accuracy at end of epoch 61: 0.8834\n",
      "TRAIN accuracy at end of epoch 61: 1.0\n",
      "                          epoch 62\n",
      "iteration 24300, loss 0.018174290657043457\n",
      "iteration 24400, loss 0.07088577747344971\n",
      "iteration 24500, loss 0.09522999823093414\n",
      "iteration 24600, loss 0.1254514902830124\n",
      "TEST accuracy at end of epoch 62: 0.8869\n",
      "TRAIN accuracy at end of epoch 62: 0.99\n",
      "                          epoch 63\n",
      "iteration 24700, loss 0.09381212294101715\n",
      "iteration 24800, loss 0.04126754030585289\n",
      "iteration 24900, loss 0.09464371204376221\n",
      "iteration 25000, loss 0.06630392372608185\n",
      "TEST accuracy at end of epoch 63: 0.8971\n",
      "TRAIN accuracy at end of epoch 63: 0.97\n",
      "                          epoch 64\n",
      "iteration 25100, loss 0.09600163996219635\n",
      "iteration 25200, loss 0.03541583567857742\n",
      "iteration 25300, loss 0.07984866201877594\n",
      "iteration 25400, loss 0.06182439997792244\n",
      "TEST accuracy at end of epoch 64: 0.8937\n",
      "TRAIN accuracy at end of epoch 64: 1.0\n",
      "                          epoch 65\n",
      "iteration 25500, loss 0.09551441669464111\n",
      "iteration 25600, loss 0.08685789257287979\n",
      "iteration 25700, loss 0.03858229145407677\n",
      "iteration 25800, loss 0.09359835833311081\n",
      "TEST accuracy at end of epoch 65: 0.8866\n",
      "TRAIN accuracy at end of epoch 65: 0.95\n",
      "                          epoch 66\n",
      "iteration 25900, loss 0.04705619439482689\n",
      "iteration 26000, loss 0.03698904439806938\n",
      "iteration 26100, loss 0.03614877164363861\n",
      "TEST accuracy at end of epoch 66: 0.8959\n",
      "TRAIN accuracy at end of epoch 66: 0.98\n",
      "                          epoch 67\n",
      "iteration 26200, loss 0.08622583746910095\n",
      "iteration 26300, loss 0.02792077697813511\n",
      "iteration 26400, loss 0.021218225359916687\n",
      "iteration 26500, loss 0.08563148975372314\n",
      "TEST accuracy at end of epoch 67: 0.8892\n",
      "TRAIN accuracy at end of epoch 67: 0.97\n",
      "                          epoch 68\n",
      "iteration 26600, loss 0.033847417682409286\n",
      "iteration 26700, loss 0.00740784639492631\n",
      "iteration 26800, loss 0.08622987568378448\n",
      "iteration 26900, loss 0.09282593429088593\n",
      "TEST accuracy at end of epoch 68: 0.9029\n",
      "TRAIN accuracy at end of epoch 68: 1.0\n",
      "                          epoch 69\n",
      "iteration 27000, loss 0.010522205382585526\n",
      "iteration 27100, loss 0.04066251218318939\n",
      "iteration 27200, loss 0.03651262819766998\n",
      "iteration 27300, loss 0.07933193445205688\n",
      "TEST accuracy at end of epoch 69: 0.8886\n",
      "TRAIN accuracy at end of epoch 69: 0.96\n",
      "                          epoch 70\n",
      "iteration 27400, loss 0.01639319211244583\n",
      "iteration 27500, loss 0.04619103670120239\n",
      "iteration 27600, loss 0.015412023290991783\n",
      "iteration 27700, loss 0.0636066347360611\n",
      "TEST accuracy at end of epoch 70: 0.8993\n",
      "TRAIN accuracy at end of epoch 70: 0.99\n",
      "                          epoch 71\n",
      "iteration 27800, loss 0.0336768738925457\n",
      "iteration 27900, loss 0.10572866350412369\n",
      "iteration 28000, loss 0.031080137938261032\n",
      "iteration 28100, loss 0.060607846826314926\n",
      "TEST accuracy at end of epoch 71: 0.8943\n",
      "TRAIN accuracy at end of epoch 71: 0.98\n",
      "                          epoch 72\n",
      "iteration 28200, loss 0.06735324114561081\n",
      "iteration 28300, loss 0.03453057259321213\n",
      "iteration 28400, loss 0.05968806520104408\n",
      "iteration 28500, loss 0.05623821169137955\n",
      "TEST accuracy at end of epoch 72: 0.8885\n",
      "TRAIN accuracy at end of epoch 72: 0.98\n",
      "                          epoch 73\n",
      "iteration 28600, loss 0.02560756355524063\n",
      "iteration 28700, loss 0.025267913937568665\n",
      "iteration 28800, loss 0.059390246868133545\n",
      "iteration 28900, loss 0.06085051968693733\n",
      "TEST accuracy at end of epoch 73: 0.8996\n",
      "TRAIN accuracy at end of epoch 73: 0.99\n",
      "                          epoch 74\n",
      "iteration 29000, loss 0.04574333876371384\n",
      "iteration 29100, loss 0.07501130551099777\n",
      "iteration 29200, loss 0.11860557645559311\n",
      "iteration 29300, loss 0.029573630541563034\n",
      "TEST accuracy at end of epoch 74: 0.903\n",
      "TRAIN accuracy at end of epoch 74: 0.98\n",
      "                          epoch 75\n",
      "iteration 29400, loss 0.026676204055547714\n",
      "iteration 29500, loss 0.03332812711596489\n",
      "iteration 29600, loss 0.025652986019849777\n",
      "iteration 29700, loss 0.09225761890411377\n",
      "TEST accuracy at end of epoch 75: 0.8976\n",
      "TRAIN accuracy at end of epoch 75: 0.99\n",
      "                          epoch 76\n",
      "iteration 29800, loss 0.019159473478794098\n",
      "iteration 29900, loss 0.028786908835172653\n",
      "iteration 30000, loss 0.06483959406614304\n",
      "iteration 30100, loss 0.03666508570313454\n",
      "TEST accuracy at end of epoch 76: 0.9042\n",
      "TRAIN accuracy at end of epoch 76: 1.0\n",
      "                          epoch 77\n",
      "iteration 30200, loss 0.030088255181908607\n",
      "iteration 30300, loss 0.02767709270119667\n",
      "iteration 30400, loss 0.017108123749494553\n",
      "TEST accuracy at end of epoch 77: 0.9035\n",
      "TRAIN accuracy at end of epoch 77: 0.98\n",
      "                          epoch 78\n",
      "iteration 30500, loss 0.005017152056097984\n",
      "iteration 30600, loss 0.03598367050290108\n",
      "iteration 30700, loss 0.05011493340134621\n",
      "iteration 30800, loss 0.0074269138276577\n",
      "TEST accuracy at end of epoch 78: 0.903\n",
      "TRAIN accuracy at end of epoch 78: 0.98\n",
      "                          epoch 79\n",
      "iteration 30900, loss 0.007782872766256332\n",
      "iteration 31000, loss 0.02990998513996601\n",
      "iteration 31100, loss 0.10562384873628616\n",
      "iteration 31200, loss 0.027762362733483315\n",
      "TEST accuracy at end of epoch 79: 0.9034\n",
      "TRAIN accuracy at end of epoch 79: 0.99\n",
      "                          epoch 80\n",
      "iteration 31300, loss 0.02197638340294361\n",
      "iteration 31400, loss 0.025738880038261414\n",
      "iteration 31500, loss 0.05611209571361542\n",
      "iteration 31600, loss 0.008125565946102142\n",
      "TEST accuracy at end of epoch 80: 0.9054\n",
      "TRAIN accuracy at end of epoch 80: 0.99\n",
      "                          epoch 81\n",
      "iteration 31700, loss 0.04515489935874939\n",
      "iteration 31800, loss 0.04332368075847626\n",
      "iteration 31900, loss 0.0276104174554348\n",
      "iteration 32000, loss 0.05073956772685051\n",
      "TEST accuracy at end of epoch 81: 0.9039\n",
      "TRAIN accuracy at end of epoch 81: 0.99\n",
      "                          epoch 82\n",
      "iteration 32100, loss 0.03534872457385063\n",
      "iteration 32200, loss 0.0187380313873291\n",
      "iteration 32300, loss 0.07194376736879349\n",
      "iteration 32400, loss 0.040949273854494095\n",
      "TEST accuracy at end of epoch 82: 0.908\n",
      "TRAIN accuracy at end of epoch 82: 1.0\n",
      "                          epoch 83\n",
      "iteration 32500, loss 0.020020414143800735\n",
      "iteration 32600, loss 0.026117917150259018\n",
      "iteration 32700, loss 0.007736055180430412\n",
      "iteration 32800, loss 0.010606091469526291\n",
      "TEST accuracy at end of epoch 83: 0.9054\n",
      "TRAIN accuracy at end of epoch 83: 1.0\n",
      "                          epoch 84\n",
      "iteration 32900, loss 0.04497080296278\n",
      "iteration 33000, loss 0.03799860551953316\n",
      "iteration 33100, loss 0.021230464801192284\n",
      "iteration 33200, loss 0.030130667611956596\n",
      "TEST accuracy at end of epoch 84: 0.8992\n",
      "TRAIN accuracy at end of epoch 84: 0.99\n",
      "                          epoch 85\n",
      "iteration 33300, loss 0.011040485464036465\n",
      "iteration 33400, loss 0.00386139377951622\n",
      "iteration 33500, loss 0.015858307480812073\n",
      "iteration 33600, loss 0.036462172865867615\n",
      "TEST accuracy at end of epoch 85: 0.9074\n",
      "TRAIN accuracy at end of epoch 85: 0.98\n",
      "                          epoch 86\n",
      "iteration 33700, loss 0.03423094004392624\n",
      "iteration 33800, loss 0.05394269526004791\n",
      "iteration 33900, loss 0.035659223794937134\n",
      "iteration 34000, loss 0.024118315428495407\n",
      "TEST accuracy at end of epoch 86: 0.8948\n",
      "TRAIN accuracy at end of epoch 86: 0.99\n",
      "                          epoch 87\n",
      "iteration 34100, loss 0.06118329241871834\n",
      "iteration 34200, loss 0.02976357936859131\n",
      "iteration 34300, loss 0.014692218974232674\n",
      "iteration 34400, loss 0.009748823009431362\n",
      "TEST accuracy at end of epoch 87: 0.903\n",
      "TRAIN accuracy at end of epoch 87: 0.99\n",
      "                          epoch 88\n",
      "iteration 34500, loss 0.03349429368972778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 34600, loss 0.02449253387749195\n",
      "iteration 34700, loss 0.015845173969864845\n",
      "TEST accuracy at end of epoch 88: 0.911\n",
      "TRAIN accuracy at end of epoch 88: 0.99\n",
      "                          epoch 89\n",
      "iteration 34800, loss 0.015317458659410477\n",
      "iteration 34900, loss 0.04674386605620384\n",
      "iteration 35000, loss 0.007191450800746679\n",
      "iteration 35100, loss 0.023226242512464523\n",
      "TEST accuracy at end of epoch 89: 0.907\n",
      "TRAIN accuracy at end of epoch 89: 1.0\n",
      "                          epoch 90\n",
      "iteration 35200, loss 0.021563153713941574\n",
      "iteration 35300, loss 0.06272688508033752\n",
      "iteration 35400, loss 0.018978925421833992\n",
      "iteration 35500, loss 0.03128046542406082\n",
      "TEST accuracy at end of epoch 90: 0.9032\n",
      "TRAIN accuracy at end of epoch 90: 1.0\n",
      "                          epoch 91\n",
      "iteration 35600, loss 0.015858188271522522\n",
      "iteration 35700, loss 0.027510840445756912\n",
      "iteration 35800, loss 0.0346224308013916\n",
      "iteration 35900, loss 0.04514409601688385\n",
      "TEST accuracy at end of epoch 91: 0.9012\n",
      "TRAIN accuracy at end of epoch 91: 1.0\n",
      "                          epoch 92\n",
      "iteration 36000, loss 0.021981939673423767\n",
      "iteration 36100, loss 0.0023471880704164505\n",
      "iteration 36200, loss 0.019145533442497253\n",
      "iteration 36300, loss 0.014836089685559273\n",
      "TEST accuracy at end of epoch 92: 0.9023\n",
      "TRAIN accuracy at end of epoch 92: 1.0\n",
      "                          epoch 93\n",
      "iteration 36400, loss 0.00818200409412384\n",
      "iteration 36500, loss 0.06646361947059631\n",
      "iteration 36600, loss 0.04332363232970238\n",
      "iteration 36700, loss 0.03328781574964523\n",
      "TEST accuracy at end of epoch 93: 0.9031\n",
      "TRAIN accuracy at end of epoch 93: 0.98\n",
      "                          epoch 94\n",
      "iteration 36800, loss 0.00580018712207675\n",
      "iteration 36900, loss 0.006443574093282223\n",
      "iteration 37000, loss 0.003616748610511422\n",
      "iteration 37100, loss 0.021163305267691612\n",
      "TEST accuracy at end of epoch 94: 0.9058\n",
      "TRAIN accuracy at end of epoch 94: 1.0\n",
      "                          epoch 95\n",
      "iteration 37200, loss 0.06472273170948029\n",
      "iteration 37300, loss 0.0023850819561630487\n",
      "iteration 37400, loss 0.024142835289239883\n",
      "iteration 37500, loss 0.007937759160995483\n",
      "TEST accuracy at end of epoch 95: 0.9008\n",
      "TRAIN accuracy at end of epoch 95: 1.0\n",
      "                          epoch 96\n",
      "iteration 37600, loss 0.004950426053255796\n",
      "iteration 37700, loss 0.0028063063509762287\n",
      "iteration 37800, loss 0.038677483797073364\n",
      "iteration 37900, loss 0.08811129629611969\n",
      "TEST accuracy at end of epoch 96: 0.9077\n",
      "TRAIN accuracy at end of epoch 96: 1.0\n",
      "                          epoch 97\n",
      "iteration 38000, loss 0.05489145591855049\n",
      "iteration 38100, loss 0.0291545782238245\n",
      "iteration 38200, loss 0.002184678800404072\n",
      "iteration 38300, loss 0.06577709317207336\n",
      "TEST accuracy at end of epoch 97: 0.9104\n",
      "TRAIN accuracy at end of epoch 97: 0.99\n",
      "                          epoch 98\n",
      "iteration 38400, loss 0.002200509188696742\n",
      "iteration 38500, loss 0.02373403124511242\n",
      "iteration 38600, loss 0.030525237321853638\n",
      "iteration 38700, loss 0.0029514050111174583\n",
      "TEST accuracy at end of epoch 98: 0.9101\n",
      "TRAIN accuracy at end of epoch 98: 0.99\n",
      "                          epoch 99\n",
      "iteration 38800, loss 0.006142600905150175\n",
      "iteration 38900, loss 0.0192258358001709\n",
      "iteration 39000, loss 0.024825934320688248\n",
      "iteration 39100, loss 0.010545862838625908\n",
      "TEST accuracy at end of epoch 99: 0.9091\n",
      "TRAIN accuracy at end of epoch 99: 1.0\n",
      "                          epoch 100\n",
      "iteration 39200, loss 0.049674976617097855\n",
      "iteration 39300, loss 0.016106590628623962\n",
      "iteration 39400, loss 0.00587958050891757\n",
      "TEST accuracy at end of epoch 100: 0.9113\n",
      "TRAIN accuracy at end of epoch 100: 1.0\n",
      "                          epoch 101\n",
      "iteration 39500, loss 0.0271151103079319\n",
      "iteration 39600, loss 0.04339488595724106\n",
      "iteration 39700, loss 0.01633552461862564\n",
      "iteration 39800, loss 0.03834869712591171\n",
      "TEST accuracy at end of epoch 101: 0.9106\n",
      "TRAIN accuracy at end of epoch 101: 1.0\n",
      "                          epoch 102\n",
      "iteration 39900, loss 0.03974330797791481\n",
      "iteration 40000, loss 0.007861089892685413\n",
      "iteration 40100, loss 0.006296757608652115\n",
      "iteration 40200, loss 0.01355278491973877\n",
      "TEST accuracy at end of epoch 102: 0.9072\n",
      "TRAIN accuracy at end of epoch 102: 1.0\n",
      "                          epoch 103\n",
      "iteration 40300, loss 0.005000521428883076\n",
      "iteration 40400, loss 0.0016185923013836145\n",
      "iteration 40500, loss 0.011770891025662422\n",
      "iteration 40600, loss 0.019637059420347214\n",
      "TEST accuracy at end of epoch 103: 0.9114\n",
      "TRAIN accuracy at end of epoch 103: 1.0\n",
      "                          epoch 104\n",
      "iteration 40700, loss 0.010637747123837471\n",
      "iteration 40800, loss 0.0014943040441721678\n",
      "iteration 40900, loss 0.020723583176732063\n",
      "iteration 41000, loss 0.0007964456453919411\n",
      "TEST accuracy at end of epoch 104: 0.9084\n",
      "TRAIN accuracy at end of epoch 104: 1.0\n",
      "                          epoch 105\n",
      "iteration 41100, loss 0.006318359170109034\n",
      "iteration 41200, loss 0.0013892516726627946\n",
      "iteration 41300, loss 0.011697791516780853\n",
      "iteration 41400, loss 0.0032750198151916265\n",
      "TEST accuracy at end of epoch 105: 0.9119\n",
      "TRAIN accuracy at end of epoch 105: 0.99\n",
      "                          epoch 106\n",
      "iteration 41500, loss 0.04801212623715401\n",
      "iteration 41600, loss 0.0822276771068573\n",
      "iteration 41700, loss 0.005249036941677332\n",
      "iteration 41800, loss 0.004094396717846394\n",
      "TEST accuracy at end of epoch 106: 0.9106\n",
      "TRAIN accuracy at end of epoch 106: 1.0\n",
      "                          epoch 107\n",
      "iteration 41900, loss 0.015887323766946793\n",
      "iteration 42000, loss 0.013010768219828606\n",
      "iteration 42100, loss 0.004638352897018194\n",
      "iteration 42200, loss 0.014531058259308338\n",
      "TEST accuracy at end of epoch 107: 0.914\n",
      "TRAIN accuracy at end of epoch 107: 1.0\n",
      "                          epoch 108\n",
      "iteration 42300, loss 0.00968602579087019\n",
      "iteration 42400, loss 0.008266161195933819\n",
      "iteration 42500, loss 0.005233325529843569\n",
      "iteration 42600, loss 0.034185297787189484\n",
      "TEST accuracy at end of epoch 108: 0.9105\n",
      "TRAIN accuracy at end of epoch 108: 1.0\n",
      "                          epoch 109\n",
      "iteration 42700, loss 0.00595118897035718\n",
      "iteration 42800, loss 0.010039198212325573\n",
      "iteration 42900, loss 0.002247356344014406\n",
      "iteration 43000, loss 0.02421575039625168\n",
      "TEST accuracy at end of epoch 109: 0.9154\n",
      "TRAIN accuracy at end of epoch 109: 1.0\n",
      "                          epoch 110\n",
      "iteration 43100, loss 0.015363605692982674\n",
      "iteration 43200, loss 0.0044446405954658985\n",
      "iteration 43300, loss 0.0144578842446208\n",
      "iteration 43400, loss 0.004364336840808392\n",
      "TEST accuracy at end of epoch 110: 0.914\n",
      "TRAIN accuracy at end of epoch 110: 1.0\n",
      "                          epoch 111\n",
      "iteration 43500, loss 0.0009058818686753511\n",
      "iteration 43600, loss 0.021946052089333534\n",
      "iteration 43700, loss 0.0105820307508111\n",
      "TEST accuracy at end of epoch 111: 0.9135\n",
      "TRAIN accuracy at end of epoch 111: 1.0\n",
      "                          epoch 112\n",
      "iteration 43800, loss 0.009595598094165325\n",
      "iteration 43900, loss 0.003864126279950142\n",
      "iteration 44000, loss 0.013690853491425514\n",
      "iteration 44100, loss 0.003089179750531912\n",
      "TEST accuracy at end of epoch 112: 0.9104\n",
      "TRAIN accuracy at end of epoch 112: 1.0\n",
      "                          epoch 113\n",
      "iteration 44200, loss 0.0010817639995366335\n",
      "iteration 44300, loss 0.027953077107667923\n",
      "iteration 44400, loss 0.002877362072467804\n",
      "iteration 44500, loss 0.002519977279007435\n",
      "TEST accuracy at end of epoch 113: 0.9101\n",
      "TRAIN accuracy at end of epoch 113: 1.0\n",
      "                          epoch 114\n",
      "iteration 44600, loss 0.006968183908611536\n",
      "iteration 44700, loss 0.001798919285647571\n",
      "iteration 44800, loss 0.04886901378631592\n",
      "iteration 44900, loss 0.008640708401799202\n",
      "TEST accuracy at end of epoch 114: 0.9161\n",
      "TRAIN accuracy at end of epoch 114: 1.0\n",
      "                          epoch 115\n",
      "iteration 45000, loss 0.002973555587232113\n",
      "iteration 45100, loss 0.0017645946936681867\n",
      "iteration 45200, loss 0.002096382435411215\n",
      "iteration 45300, loss 0.009884999133646488\n",
      "TEST accuracy at end of epoch 115: 0.9135\n",
      "TRAIN accuracy at end of epoch 115: 1.0\n",
      "                          epoch 116\n",
      "iteration 45400, loss 0.002198979025706649\n",
      "iteration 45500, loss 0.0016696758102625608\n",
      "iteration 45600, loss 0.011679330840706825\n",
      "iteration 45700, loss 0.007098380476236343\n",
      "TEST accuracy at end of epoch 116: 0.9124\n",
      "TRAIN accuracy at end of epoch 116: 1.0\n",
      "                          epoch 117\n",
      "iteration 45800, loss 0.012018361128866673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 45900, loss 0.004887280520051718\n",
      "iteration 46000, loss 0.005627105478197336\n",
      "iteration 46100, loss 0.005659440066665411\n",
      "TEST accuracy at end of epoch 117: 0.9145\n",
      "TRAIN accuracy at end of epoch 117: 1.0\n",
      "                          epoch 118\n",
      "iteration 46200, loss 0.017004244029521942\n",
      "iteration 46300, loss 0.0016049923142418265\n",
      "iteration 46400, loss 0.0024864310398697853\n",
      "iteration 46500, loss 0.0031801508739590645\n",
      "TEST accuracy at end of epoch 118: 0.9166\n",
      "TRAIN accuracy at end of epoch 118: 1.0\n",
      "                          epoch 119\n",
      "iteration 46600, loss 0.013325942680239677\n",
      "iteration 46700, loss 0.002922707935795188\n",
      "iteration 46800, loss 0.0060675134882330894\n",
      "iteration 46900, loss 0.007114719599485397\n",
      "TEST accuracy at end of epoch 119: 0.9165\n",
      "TRAIN accuracy at end of epoch 119: 1.0\n",
      "                          epoch 120\n",
      "iteration 47000, loss 0.014645966701209545\n",
      "iteration 47100, loss 0.0010532957967370749\n",
      "iteration 47200, loss 0.002428731881082058\n",
      "iteration 47300, loss 0.01923217810690403\n",
      "TEST accuracy at end of epoch 120: 0.915\n",
      "TRAIN accuracy at end of epoch 120: 1.0\n",
      "                          epoch 121\n",
      "iteration 47400, loss 0.005411973223090172\n",
      "iteration 47500, loss 0.012525970116257668\n",
      "iteration 47600, loss 0.008966438472270966\n",
      "iteration 47700, loss 0.004371467046439648\n",
      "TEST accuracy at end of epoch 121: 0.9146\n",
      "TRAIN accuracy at end of epoch 121: 1.0\n",
      "                          epoch 122\n",
      "iteration 47800, loss 0.004488272592425346\n",
      "iteration 47900, loss 0.0108451833948493\n",
      "iteration 48000, loss 0.0025086982641369104\n",
      "TEST accuracy at end of epoch 122: 0.917\n",
      "TRAIN accuracy at end of epoch 122: 1.0\n",
      "                          epoch 123\n",
      "iteration 48100, loss 0.000655369134619832\n",
      "iteration 48200, loss 0.04736369475722313\n",
      "iteration 48300, loss 0.016549598425626755\n",
      "iteration 48400, loss 0.004794768989086151\n",
      "TEST accuracy at end of epoch 123: 0.9155\n",
      "TRAIN accuracy at end of epoch 123: 0.99\n",
      "                          epoch 124\n",
      "iteration 48500, loss 0.006158394739031792\n",
      "iteration 48600, loss 0.0006547107477672398\n",
      "iteration 48700, loss 0.002592195523902774\n",
      "iteration 48800, loss 0.03098386339843273\n",
      "TEST accuracy at end of epoch 124: 0.9166\n",
      "TRAIN accuracy at end of epoch 124: 1.0\n",
      "                          epoch 125\n",
      "iteration 48900, loss 0.004940373823046684\n",
      "iteration 49000, loss 0.0029845829121768475\n",
      "iteration 49100, loss 0.0052846092730760574\n",
      "iteration 49200, loss 0.004640818107873201\n",
      "TEST accuracy at end of epoch 125: 0.9143\n",
      "TRAIN accuracy at end of epoch 125: 1.0\n",
      "                          epoch 126\n",
      "iteration 49300, loss 0.0024164943024516106\n",
      "iteration 49400, loss 0.0015414804220199585\n",
      "iteration 49500, loss 0.002908809343352914\n",
      "iteration 49600, loss 0.059278760105371475\n",
      "TEST accuracy at end of epoch 126: 0.9137\n",
      "TRAIN accuracy at end of epoch 126: 1.0\n",
      "                          epoch 127\n",
      "iteration 49700, loss 0.001489688060246408\n",
      "iteration 49800, loss 0.0013769548386335373\n",
      "iteration 49900, loss 0.008187328465282917\n",
      "iteration 50000, loss 0.002025188412517309\n",
      "TEST accuracy at end of epoch 127: 0.915\n",
      "TRAIN accuracy at end of epoch 127: 1.0\n",
      "                          epoch 128\n",
      "iteration 50100, loss 0.0014864469412714243\n",
      "iteration 50200, loss 0.0009067666833288968\n",
      "iteration 50300, loss 0.007505269721150398\n",
      "iteration 50400, loss 0.013078848831355572\n",
      "TEST accuracy at end of epoch 128: 0.9185\n",
      "TRAIN accuracy at end of epoch 128: 1.0\n",
      "                          epoch 129\n",
      "iteration 50500, loss 0.0011838047066703439\n",
      "iteration 50600, loss 0.043200671672821045\n",
      "iteration 50700, loss 0.0010583470575511456\n",
      "iteration 50800, loss 0.0012655763421207666\n",
      "TEST accuracy at end of epoch 129: 0.9171\n",
      "TRAIN accuracy at end of epoch 129: 1.0\n",
      "                          epoch 130\n",
      "iteration 50900, loss 0.003576884977519512\n",
      "iteration 51000, loss 0.0035723356995731592\n",
      "iteration 51100, loss 0.005297311581671238\n",
      "iteration 51200, loss 0.00211451412178576\n",
      "TEST accuracy at end of epoch 130: 0.9183\n",
      "TRAIN accuracy at end of epoch 130: 1.0\n",
      "                          epoch 131\n",
      "iteration 51300, loss 0.0009430398349650204\n",
      "iteration 51400, loss 0.0034319155383855104\n",
      "iteration 51500, loss 0.02382526360452175\n",
      "iteration 51600, loss 0.0048180827870965\n",
      "TEST accuracy at end of epoch 131: 0.9156\n",
      "TRAIN accuracy at end of epoch 131: 1.0\n",
      "                          epoch 132\n",
      "iteration 51700, loss 0.006956786382943392\n",
      "iteration 51800, loss 0.0031495506409555674\n",
      "iteration 51900, loss 0.0072288778610527515\n",
      "iteration 52000, loss 0.00025940596242435277\n",
      "TEST accuracy at end of epoch 132: 0.9186\n",
      "TRAIN accuracy at end of epoch 132: 1.0\n",
      "                          epoch 133\n",
      "iteration 52100, loss 0.005962776951491833\n",
      "iteration 52200, loss 0.014721296727657318\n",
      "iteration 52300, loss 0.005928623024374247\n",
      "TEST accuracy at end of epoch 133: 0.9193\n",
      "TRAIN accuracy at end of epoch 133: 1.0\n",
      "                          epoch 134\n",
      "iteration 52400, loss 0.006344228982925415\n",
      "iteration 52500, loss 0.0018210441339761019\n",
      "iteration 52600, loss 0.017208565026521683\n",
      "iteration 52700, loss 0.0004251248319633305\n",
      "TEST accuracy at end of epoch 134: 0.9182\n",
      "TRAIN accuracy at end of epoch 134: 1.0\n",
      "                          epoch 135\n",
      "iteration 52800, loss 0.030051134526729584\n",
      "iteration 52900, loss 0.010695346631109715\n",
      "iteration 53000, loss 0.004533289931714535\n",
      "iteration 53100, loss 0.0003133972641080618\n",
      "TEST accuracy at end of epoch 135: 0.9202\n",
      "TRAIN accuracy at end of epoch 135: 1.0\n",
      "                          epoch 136\n",
      "iteration 53200, loss 0.003807239467278123\n",
      "iteration 53300, loss 0.004208863712847233\n",
      "iteration 53400, loss 0.002994456561282277\n",
      "iteration 53500, loss 0.01004027109593153\n",
      "TEST accuracy at end of epoch 136: 0.919\n",
      "TRAIN accuracy at end of epoch 136: 1.0\n",
      "                          epoch 137\n",
      "iteration 53600, loss 0.0006494780536741018\n",
      "iteration 53700, loss 0.002381453523412347\n",
      "iteration 53800, loss 0.0003209513961337507\n",
      "iteration 53900, loss 0.0012682597152888775\n",
      "TEST accuracy at end of epoch 137: 0.9167\n",
      "TRAIN accuracy at end of epoch 137: 1.0\n",
      "                          epoch 138\n",
      "iteration 54000, loss 0.00027898294501937926\n",
      "iteration 54100, loss 0.008673690259456635\n",
      "iteration 54200, loss 0.0009008963825181127\n",
      "iteration 54300, loss 0.0011761211790144444\n",
      "TEST accuracy at end of epoch 138: 0.9191\n",
      "TRAIN accuracy at end of epoch 138: 1.0\n",
      "                          epoch 139\n",
      "iteration 54400, loss 0.0004640673869289458\n",
      "iteration 54500, loss 0.0002654381387401372\n",
      "iteration 54600, loss 0.0035816924646496773\n",
      "iteration 54700, loss 0.0013469139812514186\n",
      "TEST accuracy at end of epoch 139: 0.9214\n",
      "TRAIN accuracy at end of epoch 139: 1.0\n",
      "                          epoch 140\n",
      "iteration 54800, loss 0.0008510267361998558\n",
      "iteration 54900, loss 0.0003528414817992598\n",
      "iteration 55000, loss 0.000402241013944149\n",
      "iteration 55100, loss 0.0002800892398227006\n",
      "TEST accuracy at end of epoch 140: 0.92\n",
      "TRAIN accuracy at end of epoch 140: 1.0\n",
      "                          epoch 141\n",
      "iteration 55200, loss 0.00026102858828380704\n",
      "iteration 55300, loss 0.00321196299046278\n",
      "iteration 55400, loss 0.0005871739704161882\n",
      "iteration 55500, loss 0.0377349779009819\n",
      "TEST accuracy at end of epoch 141: 0.9216\n",
      "TRAIN accuracy at end of epoch 141: 1.0\n",
      "                          epoch 142\n",
      "iteration 55600, loss 0.007568239700049162\n",
      "iteration 55700, loss 0.000510774552822113\n",
      "iteration 55800, loss 0.0021180922631174326\n",
      "iteration 55900, loss 0.0014286970254033804\n",
      "TEST accuracy at end of epoch 142: 0.9211\n",
      "TRAIN accuracy at end of epoch 142: 1.0\n",
      "                          epoch 143\n",
      "iteration 56000, loss 0.0003732607001438737\n",
      "iteration 56100, loss 0.0039139981381595135\n",
      "iteration 56200, loss 0.000618078513070941\n",
      "iteration 56300, loss 0.004738330841064453\n",
      "TEST accuracy at end of epoch 143: 0.9185\n",
      "TRAIN accuracy at end of epoch 143: 1.0\n",
      "                          epoch 144\n",
      "iteration 56400, loss 0.000501304748468101\n",
      "iteration 56500, loss 0.0018928127828985453\n",
      "iteration 56600, loss 0.003973355982452631\n",
      "TEST accuracy at end of epoch 144: 0.9191\n",
      "TRAIN accuracy at end of epoch 144: 1.0\n",
      "                          epoch 145\n",
      "iteration 56700, loss 0.0007007983513176441\n",
      "iteration 56800, loss 0.0015087644569575787\n",
      "iteration 56900, loss 6.979596219025552e-05\n",
      "iteration 57000, loss 0.005153778474777937\n",
      "TEST accuracy at end of epoch 145: 0.9213\n",
      "TRAIN accuracy at end of epoch 145: 1.0\n",
      "                          epoch 146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 57100, loss 0.0028688597958534956\n",
      "iteration 57200, loss 0.0012527363141998649\n",
      "iteration 57300, loss 0.008478914387524128\n",
      "iteration 57400, loss 0.0018420503474771976\n",
      "TEST accuracy at end of epoch 146: 0.9216\n",
      "TRAIN accuracy at end of epoch 146: 1.0\n",
      "                          epoch 147\n",
      "iteration 57500, loss 0.00020223704632371664\n",
      "iteration 57600, loss 0.0007304173777811229\n",
      "iteration 57700, loss 0.00406532222405076\n",
      "iteration 57800, loss 0.0001262716978089884\n",
      "TEST accuracy at end of epoch 147: 0.9212\n",
      "TRAIN accuracy at end of epoch 147: 1.0\n",
      "                          epoch 148\n",
      "iteration 57900, loss 0.0003851859364658594\n",
      "iteration 58000, loss 0.000279370229691267\n",
      "iteration 58100, loss 0.006829455494880676\n",
      "iteration 58200, loss 0.002035475103184581\n",
      "TEST accuracy at end of epoch 148: 0.9199\n",
      "TRAIN accuracy at end of epoch 148: 1.0\n",
      "                          epoch 149\n",
      "iteration 58300, loss 0.004781784489750862\n",
      "iteration 58400, loss 0.0017929907189682126\n",
      "iteration 58500, loss 0.006187432911247015\n",
      "iteration 58600, loss 0.0016629622550681233\n",
      "TEST accuracy at end of epoch 149: 0.9201\n",
      "TRAIN accuracy at end of epoch 149: 1.0\n",
      "                          epoch 150\n",
      "iteration 58700, loss 0.0005352511652745306\n",
      "iteration 58800, loss 0.0010640791151672602\n",
      "iteration 58900, loss 0.00031427692738361657\n",
      "iteration 59000, loss 0.0004085448454134166\n",
      "TEST accuracy at end of epoch 150: 0.9211\n",
      "TRAIN accuracy at end of epoch 150: 1.0\n",
      "                          epoch 151\n",
      "iteration 59100, loss 0.0029657124541699886\n",
      "iteration 59200, loss 0.0009670223225839436\n",
      "iteration 59300, loss 0.0029993094503879547\n",
      "iteration 59400, loss 0.027696769684553146\n",
      "TEST accuracy at end of epoch 151: 0.9226\n",
      "TRAIN accuracy at end of epoch 151: 1.0\n",
      "                          epoch 152\n",
      "iteration 59500, loss 0.0032040050718933344\n",
      "iteration 59600, loss 0.0012798737734556198\n",
      "iteration 59700, loss 0.0002690896508283913\n",
      "iteration 59800, loss 0.0007167065632529557\n",
      "TEST accuracy at end of epoch 152: 0.923\n",
      "TRAIN accuracy at end of epoch 152: 1.0\n",
      "                          epoch 153\n",
      "iteration 59900, loss 0.00027989863883703947\n",
      "iteration 60000, loss 0.00023801391944289207\n",
      "iteration 60100, loss 0.002272793324664235\n",
      "iteration 60200, loss 0.0005929493927396834\n",
      "TEST accuracy at end of epoch 153: 0.9211\n",
      "TRAIN accuracy at end of epoch 153: 1.0\n",
      "                          epoch 154\n",
      "iteration 60300, loss 0.00015454039385076612\n",
      "iteration 60400, loss 0.00030651470297016203\n",
      "iteration 60500, loss 0.0008067848975770175\n",
      "iteration 60600, loss 0.0004632164491340518\n",
      "TEST accuracy at end of epoch 154: 0.9221\n",
      "TRAIN accuracy at end of epoch 154: 1.0\n",
      "                          epoch 155\n",
      "iteration 60700, loss 0.003937528934329748\n",
      "iteration 60800, loss 0.00032555265352129936\n",
      "iteration 60900, loss 0.0005340712377801538\n",
      "TEST accuracy at end of epoch 155: 0.9218\n",
      "TRAIN accuracy at end of epoch 155: 1.0\n",
      "                          epoch 156\n",
      "iteration 61000, loss 0.0006090825190767646\n",
      "iteration 61100, loss 0.0018250809516757727\n",
      "iteration 61200, loss 8.845559932524338e-05\n",
      "iteration 61300, loss 0.0008318661712110043\n",
      "TEST accuracy at end of epoch 156: 0.922\n",
      "TRAIN accuracy at end of epoch 156: 1.0\n",
      "                          epoch 157\n",
      "iteration 61400, loss 0.0001697508996585384\n",
      "iteration 61500, loss 0.00024862459395080805\n",
      "iteration 61600, loss 0.00010279647540301085\n",
      "iteration 61700, loss 0.014726364985108376\n",
      "TEST accuracy at end of epoch 157: 0.9218\n",
      "TRAIN accuracy at end of epoch 157: 1.0\n",
      "                          epoch 158\n",
      "iteration 61800, loss 0.0003846906474791467\n",
      "iteration 61900, loss 0.004232410807162523\n",
      "iteration 62000, loss 0.0008980582933872938\n",
      "iteration 62100, loss 0.00022279491531662643\n",
      "TEST accuracy at end of epoch 158: 0.9225\n",
      "TRAIN accuracy at end of epoch 158: 1.0\n",
      "                          epoch 159\n",
      "iteration 62200, loss 0.00048241388867609203\n",
      "iteration 62300, loss 0.0024440716952085495\n",
      "iteration 62400, loss 0.004486773628741503\n",
      "iteration 62500, loss 0.0044226679019629955\n",
      "TEST accuracy at end of epoch 159: 0.9214\n",
      "TRAIN accuracy at end of epoch 159: 1.0\n",
      "                          epoch 160\n",
      "iteration 62600, loss 0.00034671640605665743\n",
      "iteration 62700, loss 0.00040005496703088284\n",
      "iteration 62800, loss 0.00485226372256875\n",
      "iteration 62900, loss 0.00034293284988962114\n",
      "TEST accuracy at end of epoch 160: 0.9217\n",
      "TRAIN accuracy at end of epoch 160: 1.0\n",
      "                          epoch 161\n",
      "iteration 63000, loss 0.00012589324614964426\n",
      "iteration 63100, loss 0.0002211828250437975\n",
      "iteration 63200, loss 0.00013742304872721434\n",
      "iteration 63300, loss 0.00026911505847238004\n",
      "TEST accuracy at end of epoch 161: 0.9209\n",
      "TRAIN accuracy at end of epoch 161: 1.0\n",
      "                          epoch 162\n",
      "iteration 63400, loss 0.0009249887079931796\n",
      "iteration 63500, loss 0.0009093151893466711\n",
      "iteration 63600, loss 5.215363489696756e-05\n",
      "iteration 63700, loss 0.015967343002557755\n",
      "TEST accuracy at end of epoch 162: 0.9208\n",
      "TRAIN accuracy at end of epoch 162: 1.0\n",
      "                          epoch 163\n",
      "iteration 63800, loss 0.0040135071612894535\n",
      "iteration 63900, loss 0.0001412609126418829\n",
      "iteration 64000, loss 0.0003428542986512184\n",
      "iteration 64100, loss 0.00114883400965482\n",
      "TEST accuracy at end of epoch 163: 0.922\n",
      "TRAIN accuracy at end of epoch 163: 1.0\n",
      "                          epoch 164\n",
      "iteration 64200, loss 8.261796028818935e-05\n",
      "iteration 64300, loss 0.008475667797029018\n",
      "iteration 64400, loss 0.00044888112461194396\n",
      "iteration 64500, loss 0.00431900704279542\n",
      "TEST accuracy at end of epoch 164: 0.9215\n",
      "TRAIN accuracy at end of epoch 164: 1.0\n",
      "                          epoch 165\n",
      "iteration 64600, loss 0.0003090109094046056\n",
      "iteration 64700, loss 6.668443529633805e-05\n",
      "iteration 64800, loss 8.290002006106079e-05\n",
      "iteration 64900, loss 0.0005120416171848774\n",
      "TEST accuracy at end of epoch 165: 0.9217\n",
      "TRAIN accuracy at end of epoch 165: 1.0\n",
      "                          epoch 166\n",
      "iteration 65000, loss 0.02611769363284111\n",
      "iteration 65100, loss 0.0039230408146977425\n",
      "iteration 65200, loss 0.0005383000243455172\n",
      "TEST accuracy at end of epoch 166: 0.9217\n",
      "TRAIN accuracy at end of epoch 166: 1.0\n",
      "                          epoch 167\n",
      "iteration 65300, loss 0.00023848428099881858\n",
      "iteration 65400, loss 0.0010217480594292283\n",
      "iteration 65500, loss 0.00015434464148711413\n",
      "iteration 65600, loss 0.0009687332203611732\n",
      "TEST accuracy at end of epoch 167: 0.9213\n",
      "TRAIN accuracy at end of epoch 167: 1.0\n",
      "                          epoch 168\n",
      "iteration 65700, loss 0.0017551470082253218\n",
      "iteration 65800, loss 0.0009705645497888327\n",
      "iteration 65900, loss 0.0010265619494020939\n",
      "iteration 66000, loss 7.282145088538527e-05\n",
      "TEST accuracy at end of epoch 168: 0.9228\n",
      "TRAIN accuracy at end of epoch 168: 1.0\n",
      "                          epoch 169\n",
      "iteration 66100, loss 8.251812687376514e-05\n",
      "iteration 66200, loss 0.0021125550847500563\n",
      "iteration 66300, loss 0.002532413462176919\n",
      "iteration 66400, loss 9.849689377006143e-05\n",
      "TEST accuracy at end of epoch 169: 0.9221\n",
      "TRAIN accuracy at end of epoch 169: 1.0\n",
      "                          epoch 170\n",
      "iteration 66500, loss 0.0018390992190688848\n",
      "iteration 66600, loss 0.00022352772066369653\n",
      "iteration 66700, loss 0.0027139706071466208\n",
      "iteration 66800, loss 0.0006129167741164565\n",
      "TEST accuracy at end of epoch 170: 0.923\n",
      "TRAIN accuracy at end of epoch 170: 1.0\n",
      "                          epoch 171\n",
      "iteration 66900, loss 0.0009808869799599051\n",
      "iteration 67000, loss 0.00033991405507549644\n",
      "iteration 67100, loss 0.0004965958069078624\n",
      "iteration 67200, loss 0.00047610464389435947\n",
      "TEST accuracy at end of epoch 171: 0.9217\n",
      "TRAIN accuracy at end of epoch 171: 1.0\n",
      "                          epoch 172\n",
      "iteration 67300, loss 0.000603532767854631\n",
      "iteration 67400, loss 0.00017253585974685848\n",
      "iteration 67500, loss 0.0020625493489205837\n",
      "iteration 67600, loss 0.00972700770944357\n",
      "TEST accuracy at end of epoch 172: 0.9216\n",
      "TRAIN accuracy at end of epoch 172: 1.0\n",
      "                          epoch 173\n",
      "iteration 67700, loss 0.002130037173628807\n",
      "iteration 67800, loss 0.000123384699691087\n",
      "iteration 67900, loss 0.0021346784196794033\n",
      "iteration 68000, loss 0.00016335936379618943\n",
      "TEST accuracy at end of epoch 173: 0.921\n",
      "TRAIN accuracy at end of epoch 173: 1.0\n",
      "                          epoch 174\n",
      "iteration 68100, loss 0.00018883544544223696\n",
      "iteration 68200, loss 0.00042234003194607794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 68300, loss 8.025336137507111e-05\n",
      "iteration 68400, loss 0.0003046756028197706\n",
      "TEST accuracy at end of epoch 174: 0.9222\n",
      "TRAIN accuracy at end of epoch 174: 1.0\n",
      "                          epoch 175\n",
      "iteration 68500, loss 0.0002871924079954624\n",
      "iteration 68600, loss 0.003817026736214757\n",
      "iteration 68700, loss 0.0031345540191978216\n",
      "iteration 68800, loss 0.0001822484628064558\n",
      "TEST accuracy at end of epoch 175: 0.9224\n",
      "TRAIN accuracy at end of epoch 175: 1.0\n",
      "                          epoch 176\n",
      "iteration 68900, loss 5.528495239559561e-05\n",
      "iteration 69000, loss 0.00021889641357120126\n",
      "iteration 69100, loss 0.00021572349942289293\n",
      "iteration 69200, loss 0.009245411492884159\n",
      "TEST accuracy at end of epoch 176: 0.9222\n",
      "TRAIN accuracy at end of epoch 176: 1.0\n",
      "                          epoch 177\n",
      "iteration 69300, loss 0.00037943749339319766\n",
      "iteration 69400, loss 0.0009598530596122146\n",
      "iteration 69500, loss 0.001479900791309774\n",
      "TEST accuracy at end of epoch 177: 0.9224\n",
      "TRAIN accuracy at end of epoch 177: 1.0\n",
      "                          epoch 178\n",
      "iteration 69600, loss 7.938089402159676e-05\n",
      "iteration 69700, loss 0.00018401749548502266\n",
      "iteration 69800, loss 0.000420342170400545\n",
      "iteration 69900, loss 0.00018178950995206833\n",
      "TEST accuracy at end of epoch 178: 0.9213\n",
      "TRAIN accuracy at end of epoch 178: 1.0\n",
      "                          epoch 179\n",
      "iteration 70000, loss 0.00013555170153267682\n",
      "iteration 70100, loss 0.00034869922092184424\n",
      "iteration 70200, loss 9.65100625762716e-05\n",
      "iteration 70300, loss 0.00044458667980507016\n",
      "TEST accuracy at end of epoch 179: 0.9215\n",
      "TRAIN accuracy at end of epoch 179: 1.0\n",
      "                          epoch 180\n",
      "iteration 70400, loss 0.000805722433142364\n",
      "iteration 70500, loss 9.366981248604134e-05\n",
      "iteration 70600, loss 0.0003739514504559338\n",
      "iteration 70700, loss 0.00024167857191059738\n",
      "TEST accuracy at end of epoch 180: 0.9217\n",
      "TRAIN accuracy at end of epoch 180: 1.0\n",
      "                          epoch 181\n",
      "iteration 70800, loss 0.01992826908826828\n",
      "iteration 70900, loss 0.00018470370559953153\n",
      "iteration 71000, loss 0.0004853529972024262\n",
      "iteration 71100, loss 0.0012089741649106145\n",
      "TEST accuracy at end of epoch 181: 0.9218\n",
      "TRAIN accuracy at end of epoch 181: 1.0\n",
      "                          epoch 182\n",
      "iteration 71200, loss 0.004144611302763224\n",
      "iteration 71300, loss 0.00016737100668251514\n",
      "iteration 71400, loss 4.963681203662418e-05\n",
      "iteration 71500, loss 0.00025249633472412825\n",
      "TEST accuracy at end of epoch 182: 0.922\n",
      "TRAIN accuracy at end of epoch 182: 1.0\n",
      "                          epoch 183\n",
      "iteration 71600, loss 0.00024519042926840484\n",
      "iteration 71700, loss 0.0007459286134690046\n",
      "iteration 71800, loss 0.0031850975938141346\n",
      "iteration 71900, loss 0.003105319105088711\n",
      "TEST accuracy at end of epoch 183: 0.9215\n",
      "TRAIN accuracy at end of epoch 183: 1.0\n",
      "                          epoch 184\n",
      "iteration 72000, loss 0.0005052486667409539\n",
      "iteration 72100, loss 0.0004068423295393586\n",
      "iteration 72200, loss 0.00785705167800188\n",
      "iteration 72300, loss 0.0010236051166430116\n",
      "TEST accuracy at end of epoch 184: 0.9224\n",
      "TRAIN accuracy at end of epoch 184: 1.0\n",
      "                          epoch 185\n",
      "iteration 72400, loss 0.016986267641186714\n",
      "iteration 72500, loss 0.009826495312154293\n",
      "iteration 72600, loss 0.00017015704361256212\n",
      "iteration 72700, loss 0.0001233939256053418\n",
      "TEST accuracy at end of epoch 185: 0.9224\n",
      "TRAIN accuracy at end of epoch 185: 1.0\n",
      "                          epoch 186\n",
      "iteration 72800, loss 0.0001474111049901694\n",
      "iteration 72900, loss 0.0035953130573034286\n",
      "iteration 73000, loss 0.0011592624941840768\n",
      "iteration 73100, loss 0.02230369672179222\n",
      "TEST accuracy at end of epoch 186: 0.922\n",
      "TRAIN accuracy at end of epoch 186: 1.0\n",
      "                          epoch 187\n",
      "iteration 73200, loss 0.00018652656581252813\n",
      "iteration 73300, loss 0.0009644092060625553\n",
      "iteration 73400, loss 0.0010820369934663177\n",
      "iteration 73500, loss 3.636913243099116e-05\n",
      "TEST accuracy at end of epoch 187: 0.9222\n",
      "TRAIN accuracy at end of epoch 187: 1.0\n",
      "                          epoch 188\n",
      "iteration 73600, loss 0.0003686714917421341\n",
      "iteration 73700, loss 8.968137990450487e-05\n",
      "iteration 73800, loss 0.0003028856299351901\n",
      "TEST accuracy at end of epoch 188: 0.9226\n",
      "TRAIN accuracy at end of epoch 188: 1.0\n",
      "                          epoch 189\n",
      "iteration 73900, loss 0.0001212061324622482\n",
      "iteration 74000, loss 0.00031952018616721034\n",
      "iteration 74100, loss 0.0002468348538968712\n",
      "iteration 74200, loss 0.0009959617163985968\n",
      "TEST accuracy at end of epoch 189: 0.9223\n",
      "TRAIN accuracy at end of epoch 189: 1.0\n",
      "                          epoch 190\n",
      "iteration 74300, loss 0.0012334685306996107\n",
      "iteration 74400, loss 0.0006283764378167689\n",
      "iteration 74500, loss 6.681174272671342e-05\n",
      "iteration 74600, loss 0.00036784951225854456\n",
      "TEST accuracy at end of epoch 190: 0.9229\n",
      "TRAIN accuracy at end of epoch 190: 1.0\n",
      "                          epoch 191\n",
      "iteration 74700, loss 8.6160798673518e-05\n",
      "iteration 74800, loss 0.0012820819392800331\n",
      "iteration 74900, loss 0.000215588923310861\n",
      "iteration 75000, loss 0.0003147276584059\n",
      "TEST accuracy at end of epoch 191: 0.9229\n",
      "TRAIN accuracy at end of epoch 191: 1.0\n",
      "                          epoch 192\n",
      "iteration 75100, loss 0.0001431357959518209\n",
      "iteration 75200, loss 0.0017279025632888079\n",
      "iteration 75300, loss 6.726053106831387e-05\n",
      "iteration 75400, loss 0.00020989874610677361\n",
      "TEST accuracy at end of epoch 192: 0.9222\n",
      "TRAIN accuracy at end of epoch 192: 1.0\n",
      "                          epoch 193\n",
      "iteration 75500, loss 9.52395930653438e-05\n",
      "iteration 75600, loss 0.002713658381253481\n",
      "iteration 75700, loss 6.646395922871307e-05\n",
      "iteration 75800, loss 0.0011938257375732064\n",
      "TEST accuracy at end of epoch 193: 0.9218\n",
      "TRAIN accuracy at end of epoch 193: 1.0\n",
      "                          epoch 194\n",
      "iteration 75900, loss 0.0007115636253729463\n",
      "iteration 76000, loss 0.00010295487300027162\n",
      "iteration 76100, loss 9.997933375416324e-05\n",
      "iteration 76200, loss 0.008836273103952408\n",
      "TEST accuracy at end of epoch 194: 0.9221\n",
      "TRAIN accuracy at end of epoch 194: 1.0\n",
      "                          epoch 195\n",
      "iteration 76300, loss 5.814836185891181e-05\n",
      "iteration 76400, loss 0.00018385947623755783\n",
      "iteration 76500, loss 0.0014634442050009966\n",
      "iteration 76600, loss 0.0005335339810699224\n",
      "TEST accuracy at end of epoch 195: 0.9227\n",
      "TRAIN accuracy at end of epoch 195: 1.0\n",
      "                          epoch 196\n",
      "iteration 76700, loss 0.0008933187928050756\n",
      "iteration 76800, loss 9.306857464252971e-06\n",
      "iteration 76900, loss 5.096376844448969e-05\n",
      "iteration 77000, loss 0.018684254959225655\n",
      "TEST accuracy at end of epoch 196: 0.9224\n",
      "TRAIN accuracy at end of epoch 196: 1.0\n",
      "                          epoch 197\n",
      "iteration 77100, loss 0.00034614981268532574\n",
      "iteration 77200, loss 0.0010118450736626983\n",
      "iteration 77300, loss 0.000492067018058151\n",
      "iteration 77400, loss 0.00019367587810847908\n",
      "TEST accuracy at end of epoch 197: 0.9222\n",
      "TRAIN accuracy at end of epoch 197: 1.0\n",
      "                          epoch 198\n",
      "iteration 77500, loss 0.0008911095792427659\n",
      "iteration 77600, loss 0.001497799064964056\n",
      "iteration 77700, loss 0.00032783186179585755\n",
      "iteration 77800, loss 3.604674566304311e-05\n",
      "TEST accuracy at end of epoch 198: 0.9225\n",
      "TRAIN accuracy at end of epoch 198: 1.0\n",
      "                          epoch 199\n",
      "iteration 77900, loss 0.00016565875557716936\n",
      "iteration 78000, loss 0.0005307991523295641\n",
      "iteration 78100, loss 0.0024418889079242945\n",
      "iteration 78200, loss 0.00551235768944025\n",
      "TEST accuracy at end of epoch 199: 0.9221\n",
      "TRAIN accuracy at end of epoch 199: 1.0\n",
      "╰┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╯\n",
      "╭┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╮\n",
      "                         n: 7, res: True\n",
      "size of reduce_mean: (?, 1, 1, 64)\n",
      "size of squeeze: (?, 64)\n",
      "training for 78125 iterations\n",
      "                          epoch 0\n",
      "iteration 100, loss 1.9328292608261108\n",
      "iteration 200, loss 1.8420597314834595\n",
      "iteration 300, loss 1.8692224025726318\n",
      "TEST accuracy at end of epoch 0: 0.1122\n",
      "TRAIN accuracy at end of epoch 0: 0.13\n",
      "                          epoch 1\n",
      "iteration 400, loss 1.5435411930084229\n",
      "iteration 500, loss 1.2926030158996582\n",
      "iteration 600, loss 1.2609367370605469\n",
      "iteration 700, loss 1.167506217956543\n",
      "TEST accuracy at end of epoch 1: 0.4682\n",
      "TRAIN accuracy at end of epoch 1: 0.46\n",
      "                          epoch 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 800, loss 1.1254215240478516\n",
      "iteration 900, loss 1.0164515972137451\n",
      "iteration 1000, loss 0.8273277878761292\n",
      "iteration 1100, loss 0.8553223609924316\n",
      "TEST accuracy at end of epoch 2: 0.4012\n",
      "TRAIN accuracy at end of epoch 2: 0.42\n",
      "                          epoch 3\n",
      "iteration 1200, loss 0.7407993674278259\n",
      "iteration 1300, loss 0.7619147300720215\n",
      "iteration 1400, loss 0.7569274306297302\n",
      "iteration 1500, loss 0.7028195858001709\n",
      "TEST accuracy at end of epoch 3: 0.4678\n",
      "TRAIN accuracy at end of epoch 3: 0.46\n",
      "                          epoch 4\n",
      "iteration 1600, loss 0.5860752463340759\n",
      "iteration 1700, loss 0.7102830410003662\n",
      "iteration 1800, loss 0.5658128261566162\n",
      "iteration 1900, loss 0.5838279724121094\n",
      "TEST accuracy at end of epoch 4: 0.5446\n",
      "TRAIN accuracy at end of epoch 4: 0.47\n",
      "                          epoch 5\n",
      "iteration 2000, loss 0.5327438116073608\n",
      "iteration 2100, loss 0.7145617604255676\n",
      "iteration 2200, loss 0.535852313041687\n",
      "iteration 2300, loss 0.5228555798530579\n",
      "TEST accuracy at end of epoch 5: 0.751\n",
      "TRAIN accuracy at end of epoch 5: 0.72\n",
      "                          epoch 6\n",
      "iteration 2400, loss 0.5372163653373718\n",
      "iteration 2500, loss 0.4977654218673706\n",
      "iteration 2600, loss 0.3745628893375397\n",
      "iteration 2700, loss 0.5182732939720154\n",
      "TEST accuracy at end of epoch 6: 0.7644\n",
      "TRAIN accuracy at end of epoch 6: 0.79\n",
      "                          epoch 7\n",
      "iteration 2800, loss 0.5723677277565002\n",
      "iteration 2900, loss 0.4815800189971924\n",
      "iteration 3000, loss 0.5378000736236572\n",
      "iteration 3100, loss 0.5906755924224854\n",
      "TEST accuracy at end of epoch 7: 0.7497\n",
      "TRAIN accuracy at end of epoch 7: 0.8\n",
      "                          epoch 8\n",
      "iteration 3200, loss 0.764928936958313\n",
      "iteration 3300, loss 0.4290543794631958\n",
      "iteration 3400, loss 0.6499891877174377\n",
      "iteration 3500, loss 0.5327038168907166\n",
      "TEST accuracy at end of epoch 8: 0.789\n",
      "TRAIN accuracy at end of epoch 8: 0.82\n",
      "                          epoch 9\n",
      "iteration 3600, loss 0.30198296904563904\n",
      "iteration 3700, loss 0.5564846992492676\n",
      "iteration 3800, loss 0.41822823882102966\n",
      "iteration 3900, loss 0.3889394998550415\n",
      "TEST accuracy at end of epoch 9: 0.7975\n",
      "TRAIN accuracy at end of epoch 9: 0.84\n",
      "                          epoch 10\n",
      "iteration 4000, loss 0.46088361740112305\n",
      "iteration 4100, loss 0.2400774508714676\n",
      "iteration 4200, loss 0.43834900856018066\n",
      "iteration 4300, loss 0.5153380632400513\n",
      "TEST accuracy at end of epoch 10: 0.7856\n",
      "TRAIN accuracy at end of epoch 10: 0.8\n",
      "                          epoch 11\n",
      "iteration 4400, loss 0.41509586572647095\n",
      "iteration 4500, loss 0.4410902261734009\n",
      "iteration 4600, loss 0.5503605008125305\n",
      "TEST accuracy at end of epoch 11: 0.8208\n",
      "TRAIN accuracy at end of epoch 11: 0.87\n",
      "                          epoch 12\n",
      "iteration 4700, loss 0.44813328981399536\n",
      "iteration 4800, loss 0.3266006410121918\n",
      "iteration 4900, loss 0.5210244059562683\n",
      "iteration 5000, loss 0.513033390045166\n",
      "TEST accuracy at end of epoch 12: 0.8107\n",
      "TRAIN accuracy at end of epoch 12: 0.87\n",
      "                          epoch 13\n",
      "iteration 5100, loss 0.4001658856868744\n",
      "iteration 5200, loss 0.5107487440109253\n",
      "iteration 5300, loss 0.2928365170955658\n",
      "iteration 5400, loss 0.4359930753707886\n",
      "TEST accuracy at end of epoch 13: 0.8044\n",
      "TRAIN accuracy at end of epoch 13: 0.86\n",
      "                          epoch 14\n",
      "iteration 5500, loss 0.3378603458404541\n",
      "iteration 5600, loss 0.3942039906978607\n",
      "iteration 5700, loss 0.49792447686195374\n",
      "iteration 5800, loss 0.2766304016113281\n",
      "TEST accuracy at end of epoch 14: 0.8361\n",
      "TRAIN accuracy at end of epoch 14: 0.91\n",
      "                          epoch 15\n",
      "iteration 5900, loss 0.3262481689453125\n",
      "iteration 6000, loss 0.37269455194473267\n",
      "iteration 6100, loss 0.1877913773059845\n",
      "iteration 6200, loss 0.30190056562423706\n",
      "TEST accuracy at end of epoch 15: 0.7894\n",
      "TRAIN accuracy at end of epoch 15: 0.87\n",
      "                          epoch 16\n",
      "iteration 6300, loss 0.2697099447250366\n",
      "iteration 6400, loss 0.329087495803833\n",
      "iteration 6500, loss 0.34540385007858276\n",
      "iteration 6600, loss 0.4347856342792511\n",
      "TEST accuracy at end of epoch 16: 0.85\n",
      "TRAIN accuracy at end of epoch 16: 0.89\n",
      "                          epoch 17\n",
      "iteration 6700, loss 0.2935837507247925\n",
      "iteration 6800, loss 0.4324205815792084\n",
      "iteration 6900, loss 0.26344776153564453\n",
      "iteration 7000, loss 0.15023109316825867\n",
      "TEST accuracy at end of epoch 17: 0.866\n",
      "TRAIN accuracy at end of epoch 17: 0.96\n",
      "                          epoch 18\n",
      "iteration 7100, loss 0.336047887802124\n",
      "iteration 7200, loss 0.23673520982265472\n",
      "iteration 7300, loss 0.21776112914085388\n",
      "iteration 7400, loss 0.3568708300590515\n",
      "TEST accuracy at end of epoch 18: 0.8393\n",
      "TRAIN accuracy at end of epoch 18: 0.87\n",
      "                          epoch 19\n",
      "iteration 7500, loss 0.27055877447128296\n",
      "iteration 7600, loss 0.26989179849624634\n",
      "iteration 7700, loss 0.2364952266216278\n",
      "iteration 7800, loss 0.30329716205596924\n",
      "TEST accuracy at end of epoch 19: 0.8417\n",
      "TRAIN accuracy at end of epoch 19: 0.86\n",
      "                          epoch 20\n",
      "iteration 7900, loss 0.3573283553123474\n",
      "iteration 8000, loss 0.1838298738002777\n",
      "iteration 8100, loss 0.4141940176486969\n",
      "iteration 8200, loss 0.24294933676719666\n",
      "TEST accuracy at end of epoch 20: 0.8459\n",
      "TRAIN accuracy at end of epoch 20: 0.89\n",
      "                          epoch 21\n",
      "iteration 8300, loss 0.1932649314403534\n",
      "iteration 8400, loss 0.3523247539997101\n",
      "iteration 8500, loss 0.276854008436203\n",
      "iteration 8600, loss 0.2328115552663803\n",
      "TEST accuracy at end of epoch 21: 0.8364\n",
      "TRAIN accuracy at end of epoch 21: 0.92\n",
      "                          epoch 22\n",
      "iteration 8700, loss 0.26516056060791016\n",
      "iteration 8800, loss 0.21620658040046692\n",
      "iteration 8900, loss 0.2331784963607788\n",
      "TEST accuracy at end of epoch 22: 0.8353\n",
      "TRAIN accuracy at end of epoch 22: 0.82\n",
      "                          epoch 23\n",
      "iteration 9000, loss 0.26154229044914246\n",
      "iteration 9100, loss 0.23694896697998047\n",
      "iteration 9200, loss 0.12769794464111328\n",
      "iteration 9300, loss 0.3454441726207733\n",
      "TEST accuracy at end of epoch 23: 0.8379\n",
      "TRAIN accuracy at end of epoch 23: 0.86\n",
      "                          epoch 24\n",
      "iteration 9400, loss 0.20028872787952423\n",
      "iteration 9500, loss 0.34041088819503784\n",
      "iteration 9600, loss 0.24715548753738403\n",
      "iteration 9700, loss 0.19488602876663208\n",
      "TEST accuracy at end of epoch 24: 0.8573\n",
      "TRAIN accuracy at end of epoch 24: 0.89\n",
      "                          epoch 25\n",
      "iteration 9800, loss 0.1751330941915512\n",
      "iteration 9900, loss 0.15484127402305603\n",
      "iteration 10000, loss 0.20804457366466522\n",
      "iteration 10100, loss 0.17318491637706757\n",
      "TEST accuracy at end of epoch 25: 0.8637\n",
      "TRAIN accuracy at end of epoch 25: 0.88\n",
      "                          epoch 26\n",
      "iteration 10200, loss 0.15627926588058472\n",
      "iteration 10300, loss 0.2764183282852173\n",
      "iteration 10400, loss 0.24422697722911835\n",
      "iteration 10500, loss 0.3817403316497803\n",
      "TEST accuracy at end of epoch 26: 0.8673\n",
      "TRAIN accuracy at end of epoch 26: 0.92\n",
      "                          epoch 27\n",
      "iteration 10600, loss 0.15387649834156036\n",
      "iteration 10700, loss 0.2542230784893036\n",
      "iteration 10800, loss 0.24556151032447815\n",
      "iteration 10900, loss 0.1175851970911026\n",
      "TEST accuracy at end of epoch 27: 0.8355\n",
      "TRAIN accuracy at end of epoch 27: 0.9\n",
      "                          epoch 28\n",
      "iteration 11000, loss 0.19199144840240479\n",
      "iteration 11100, loss 0.21602678298950195\n",
      "iteration 11200, loss 0.17383956909179688\n",
      "iteration 11300, loss 0.15555277466773987\n",
      "TEST accuracy at end of epoch 28: 0.8446\n",
      "TRAIN accuracy at end of epoch 28: 0.87\n",
      "                          epoch 29\n",
      "iteration 11400, loss 0.15615640580654144\n",
      "iteration 11500, loss 0.19072049856185913\n",
      "iteration 11600, loss 0.21789801120758057\n",
      "iteration 11700, loss 0.10432124137878418\n",
      "TEST accuracy at end of epoch 29: 0.8708\n",
      "TRAIN accuracy at end of epoch 29: 0.93\n",
      "                          epoch 30\n",
      "iteration 11800, loss 0.24088872969150543\n",
      "iteration 11900, loss 0.2042119950056076\n",
      "iteration 12000, loss 0.16190016269683838\n",
      "iteration 12100, loss 0.2468012571334839\n",
      "TEST accuracy at end of epoch 30: 0.8385\n",
      "TRAIN accuracy at end of epoch 30: 0.87\n",
      "                          epoch 31\n",
      "iteration 12200, loss 0.17268384993076324\n",
      "iteration 12300, loss 0.17521753907203674\n",
      "iteration 12400, loss 0.23605377972126007\n",
      "iteration 12500, loss 0.24194417893886566\n",
      "TEST accuracy at end of epoch 31: 0.8761\n",
      "TRAIN accuracy at end of epoch 31: 0.97\n",
      "                          epoch 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 12600, loss 0.10322687774896622\n",
      "iteration 12700, loss 0.18066081404685974\n",
      "iteration 12800, loss 0.22291377186775208\n",
      "iteration 12900, loss 0.19179604947566986\n",
      "TEST accuracy at end of epoch 32: 0.863\n",
      "TRAIN accuracy at end of epoch 32: 0.93\n",
      "                          epoch 33\n",
      "iteration 13000, loss 0.19555775821208954\n",
      "iteration 13100, loss 0.252197802066803\n",
      "iteration 13200, loss 0.16165035963058472\n",
      "TEST accuracy at end of epoch 33: 0.8653\n",
      "TRAIN accuracy at end of epoch 33: 0.9\n",
      "                          epoch 34\n",
      "iteration 13300, loss 0.1990007609128952\n",
      "iteration 13400, loss 0.236985981464386\n",
      "iteration 13500, loss 0.11761771887540817\n",
      "iteration 13600, loss 0.24739831686019897\n",
      "TEST accuracy at end of epoch 34: 0.8715\n",
      "TRAIN accuracy at end of epoch 34: 0.88\n",
      "                          epoch 35\n",
      "iteration 13700, loss 0.08946104347705841\n",
      "iteration 13800, loss 0.176212877035141\n",
      "iteration 13900, loss 0.15676680207252502\n",
      "iteration 14000, loss 0.1347455233335495\n",
      "TEST accuracy at end of epoch 35: 0.8712\n",
      "TRAIN accuracy at end of epoch 35: 0.92\n",
      "                          epoch 36\n",
      "iteration 14100, loss 0.19033503532409668\n",
      "iteration 14200, loss 0.13078992068767548\n",
      "iteration 14300, loss 0.13764630258083344\n",
      "iteration 14400, loss 0.14913159608840942\n",
      "TEST accuracy at end of epoch 36: 0.8525\n",
      "TRAIN accuracy at end of epoch 36: 0.94\n",
      "                          epoch 37\n",
      "iteration 14500, loss 0.13427582383155823\n",
      "iteration 14600, loss 0.1729135811328888\n",
      "iteration 14700, loss 0.1549302190542221\n",
      "iteration 14800, loss 0.18302926421165466\n",
      "TEST accuracy at end of epoch 37: 0.8788\n",
      "TRAIN accuracy at end of epoch 37: 0.91\n",
      "                          epoch 38\n",
      "iteration 14900, loss 0.2772728204727173\n",
      "iteration 15000, loss 0.07779216766357422\n",
      "iteration 15100, loss 0.14004680514335632\n",
      "iteration 15200, loss 0.1862703561782837\n",
      "TEST accuracy at end of epoch 38: 0.8834\n",
      "TRAIN accuracy at end of epoch 38: 0.96\n",
      "                          epoch 39\n",
      "iteration 15300, loss 0.1810227781534195\n",
      "iteration 15400, loss 0.14012399315834045\n",
      "iteration 15500, loss 0.14494186639785767\n",
      "iteration 15600, loss 0.17293110489845276\n",
      "TEST accuracy at end of epoch 39: 0.8603\n",
      "TRAIN accuracy at end of epoch 39: 0.9\n",
      "                          epoch 40\n",
      "iteration 15700, loss 0.11791424453258514\n",
      "iteration 15800, loss 0.12086060643196106\n",
      "iteration 15900, loss 0.15885744988918304\n",
      "iteration 16000, loss 0.20180779695510864\n",
      "TEST accuracy at end of epoch 40: 0.8794\n",
      "TRAIN accuracy at end of epoch 40: 0.93\n",
      "                          epoch 41\n",
      "iteration 16100, loss 0.11991849541664124\n",
      "iteration 16200, loss 0.30960044264793396\n",
      "iteration 16300, loss 0.1132260411977768\n",
      "iteration 16400, loss 0.16624654829502106\n",
      "TEST accuracy at end of epoch 41: 0.875\n",
      "TRAIN accuracy at end of epoch 41: 0.97\n",
      "                          epoch 42\n",
      "iteration 16500, loss 0.2066127061843872\n",
      "iteration 16600, loss 0.2549271583557129\n",
      "iteration 16700, loss 0.1500668227672577\n",
      "iteration 16800, loss 0.13683483004570007\n",
      "TEST accuracy at end of epoch 42: 0.8431\n",
      "TRAIN accuracy at end of epoch 42: 0.88\n",
      "                          epoch 43\n",
      "iteration 16900, loss 0.2081151306629181\n",
      "iteration 17000, loss 0.07888694107532501\n",
      "iteration 17100, loss 0.08010636270046234\n",
      "iteration 17200, loss 0.1483514904975891\n",
      "TEST accuracy at end of epoch 43: 0.8833\n",
      "TRAIN accuracy at end of epoch 43: 0.96\n",
      "                          epoch 44\n",
      "iteration 17300, loss 0.19772088527679443\n",
      "iteration 17400, loss 0.08754786849021912\n",
      "iteration 17500, loss 0.13328248262405396\n",
      "TEST accuracy at end of epoch 44: 0.8871\n",
      "TRAIN accuracy at end of epoch 44: 0.94\n",
      "                          epoch 45\n",
      "iteration 17600, loss 0.14652779698371887\n",
      "iteration 17700, loss 0.055469244718551636\n",
      "iteration 17800, loss 0.10975721478462219\n",
      "iteration 17900, loss 0.16282588243484497\n",
      "TEST accuracy at end of epoch 45: 0.8895\n",
      "TRAIN accuracy at end of epoch 45: 0.94\n",
      "                          epoch 46\n",
      "iteration 18000, loss 0.07306195795536041\n",
      "iteration 18100, loss 0.1413109004497528\n",
      "iteration 18200, loss 0.07714853435754776\n",
      "iteration 18300, loss 0.0910472571849823\n",
      "TEST accuracy at end of epoch 46: 0.8807\n",
      "TRAIN accuracy at end of epoch 46: 0.98\n",
      "                          epoch 47\n",
      "iteration 18400, loss 0.1367795765399933\n",
      "iteration 18500, loss 0.15421244502067566\n",
      "iteration 18600, loss 0.19039660692214966\n",
      "iteration 18700, loss 0.11194919049739838\n",
      "TEST accuracy at end of epoch 47: 0.8793\n",
      "TRAIN accuracy at end of epoch 47: 0.95\n",
      "                          epoch 48\n",
      "iteration 18800, loss 0.08656446635723114\n",
      "iteration 18900, loss 0.10296934843063354\n",
      "iteration 19000, loss 0.1341867297887802\n",
      "iteration 19100, loss 0.05841520428657532\n",
      "TEST accuracy at end of epoch 48: 0.8642\n",
      "TRAIN accuracy at end of epoch 48: 0.95\n",
      "                          epoch 49\n",
      "iteration 19200, loss 0.1472492665052414\n",
      "iteration 19300, loss 0.07412338256835938\n",
      "iteration 19400, loss 0.10761328041553497\n",
      "iteration 19500, loss 0.06971823424100876\n",
      "TEST accuracy at end of epoch 49: 0.8895\n",
      "TRAIN accuracy at end of epoch 49: 0.97\n",
      "                          epoch 50\n",
      "iteration 19600, loss 0.07130713760852814\n",
      "iteration 19700, loss 0.21932798624038696\n",
      "iteration 19800, loss 0.12622520327568054\n",
      "iteration 19900, loss 0.09409115463495255\n",
      "TEST accuracy at end of epoch 50: 0.8723\n",
      "TRAIN accuracy at end of epoch 50: 0.96\n",
      "                          epoch 51\n",
      "iteration 20000, loss 0.164936363697052\n",
      "iteration 20100, loss 0.05738885700702667\n",
      "iteration 20200, loss 0.2294289767742157\n",
      "iteration 20300, loss 0.09846201539039612\n",
      "TEST accuracy at end of epoch 51: 0.8654\n",
      "TRAIN accuracy at end of epoch 51: 0.92\n",
      "                          epoch 52\n",
      "iteration 20400, loss 0.07531976699829102\n",
      "iteration 20500, loss 0.10089901089668274\n",
      "iteration 20600, loss 0.0715147852897644\n",
      "iteration 20700, loss 0.0776064321398735\n",
      "TEST accuracy at end of epoch 52: 0.8817\n",
      "TRAIN accuracy at end of epoch 52: 0.94\n",
      "                          epoch 53\n",
      "iteration 20800, loss 0.086625836789608\n",
      "iteration 20900, loss 0.041265759617090225\n",
      "iteration 21000, loss 0.0791231319308281\n",
      "iteration 21100, loss 0.1505930870771408\n",
      "TEST accuracy at end of epoch 53: 0.8779\n",
      "TRAIN accuracy at end of epoch 53: 0.93\n",
      "                          epoch 54\n",
      "iteration 21200, loss 0.10502882301807404\n",
      "iteration 21300, loss 0.08697877079248428\n",
      "iteration 21400, loss 0.2059561163187027\n",
      "iteration 21500, loss 0.10810726135969162\n",
      "TEST accuracy at end of epoch 54: 0.8809\n",
      "TRAIN accuracy at end of epoch 54: 0.92\n",
      "                          epoch 55\n",
      "iteration 21600, loss 0.10014546662569046\n",
      "iteration 21700, loss 0.06427022814750671\n",
      "iteration 21800, loss 0.05533992499113083\n",
      "TEST accuracy at end of epoch 55: 0.8851\n",
      "TRAIN accuracy at end of epoch 55: 0.89\n",
      "                          epoch 56\n",
      "iteration 21900, loss 0.0761094018816948\n",
      "iteration 22000, loss 0.1124025210738182\n",
      "iteration 22100, loss 0.15694862604141235\n",
      "iteration 22200, loss 0.10458242148160934\n",
      "TEST accuracy at end of epoch 56: 0.8857\n",
      "TRAIN accuracy at end of epoch 56: 0.96\n",
      "                          epoch 57\n",
      "iteration 22300, loss 0.09507983177900314\n",
      "iteration 22400, loss 0.1434064656496048\n",
      "iteration 22500, loss 0.08140894770622253\n",
      "iteration 22600, loss 0.07392121851444244\n",
      "TEST accuracy at end of epoch 57: 0.8896\n",
      "TRAIN accuracy at end of epoch 57: 0.96\n",
      "                          epoch 58\n",
      "iteration 22700, loss 0.05839991942048073\n",
      "iteration 22800, loss 0.05623582750558853\n",
      "iteration 22900, loss 0.11286864429712296\n",
      "iteration 23000, loss 0.053492628037929535\n",
      "TEST accuracy at end of epoch 58: 0.8863\n",
      "TRAIN accuracy at end of epoch 58: 0.98\n",
      "                          epoch 59\n",
      "iteration 23100, loss 0.05892689526081085\n",
      "iteration 23200, loss 0.06483633071184158\n",
      "iteration 23300, loss 0.07360319793224335\n",
      "iteration 23400, loss 0.037319786846637726\n",
      "TEST accuracy at end of epoch 59: 0.8879\n",
      "TRAIN accuracy at end of epoch 59: 1.0\n",
      "                          epoch 60\n",
      "iteration 23500, loss 0.05936847999691963\n",
      "iteration 23600, loss 0.07828141748905182\n",
      "iteration 23700, loss 0.12649226188659668\n",
      "iteration 23800, loss 0.05454971641302109\n",
      "TEST accuracy at end of epoch 60: 0.8861\n",
      "TRAIN accuracy at end of epoch 60: 0.99\n",
      "                          epoch 61\n",
      "iteration 23900, loss 0.12891939282417297\n",
      "iteration 24000, loss 0.13101470470428467\n",
      "iteration 24100, loss 0.15983696281909943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 24200, loss 0.11732751131057739\n",
      "TEST accuracy at end of epoch 61: 0.8939\n",
      "TRAIN accuracy at end of epoch 61: 0.94\n",
      "                          epoch 62\n",
      "iteration 24300, loss 0.14372923970222473\n",
      "iteration 24400, loss 0.02706982009112835\n",
      "iteration 24500, loss 0.08093586564064026\n",
      "iteration 24600, loss 0.09725435823202133\n",
      "TEST accuracy at end of epoch 62: 0.8787\n",
      "TRAIN accuracy at end of epoch 62: 0.96\n",
      "                          epoch 63\n",
      "iteration 24700, loss 0.11552482098340988\n",
      "iteration 24800, loss 0.05924686789512634\n",
      "iteration 24900, loss 0.10762086510658264\n",
      "iteration 25000, loss 0.08212699741125107\n",
      "TEST accuracy at end of epoch 63: 0.8826\n",
      "TRAIN accuracy at end of epoch 63: 0.98\n",
      "                          epoch 64\n",
      "iteration 25100, loss 0.06519590318202972\n",
      "iteration 25200, loss 0.0585344061255455\n",
      "iteration 25300, loss 0.09637526422739029\n",
      "iteration 25400, loss 0.10670156031847\n",
      "TEST accuracy at end of epoch 64: 0.8874\n",
      "TRAIN accuracy at end of epoch 64: 0.97\n",
      "                          epoch 65\n",
      "iteration 25500, loss 0.054726492613554\n",
      "iteration 25600, loss 0.07133981585502625\n",
      "iteration 25700, loss 0.03887870907783508\n",
      "iteration 25800, loss 0.07915602624416351\n",
      "TEST accuracy at end of epoch 65: 0.8863\n",
      "TRAIN accuracy at end of epoch 65: 0.93\n",
      "                          epoch 66\n",
      "iteration 25900, loss 0.07046539336442947\n",
      "iteration 26000, loss 0.07397030293941498\n",
      "iteration 26100, loss 0.08497659116983414\n",
      "TEST accuracy at end of epoch 66: 0.8819\n",
      "TRAIN accuracy at end of epoch 66: 0.99\n",
      "                          epoch 67\n",
      "iteration 26200, loss 0.07671312242746353\n",
      "iteration 26300, loss 0.08105994760990143\n",
      "iteration 26400, loss 0.0416269414126873\n",
      "iteration 26500, loss 0.033753372728824615\n",
      "TEST accuracy at end of epoch 67: 0.8859\n",
      "TRAIN accuracy at end of epoch 67: 0.95\n",
      "                          epoch 68\n",
      "iteration 26600, loss 0.06901411712169647\n",
      "iteration 26700, loss 0.09153667092323303\n",
      "iteration 26800, loss 0.10953181982040405\n",
      "iteration 26900, loss 0.0505484938621521\n",
      "TEST accuracy at end of epoch 68: 0.8928\n",
      "TRAIN accuracy at end of epoch 68: 0.95\n",
      "                          epoch 69\n",
      "iteration 27000, loss 0.03796900808811188\n",
      "iteration 27100, loss 0.10173992067575455\n",
      "iteration 27200, loss 0.07593552768230438\n",
      "iteration 27300, loss 0.04282549396157265\n",
      "TEST accuracy at end of epoch 69: 0.8851\n",
      "TRAIN accuracy at end of epoch 69: 0.97\n",
      "                          epoch 70\n",
      "iteration 27400, loss 0.0632304921746254\n",
      "iteration 27500, loss 0.0653243213891983\n",
      "iteration 27600, loss 0.06091760843992233\n",
      "iteration 27700, loss 0.09884269535541534\n",
      "TEST accuracy at end of epoch 70: 0.8906\n",
      "TRAIN accuracy at end of epoch 70: 0.98\n",
      "                          epoch 71\n",
      "iteration 27800, loss 0.029462989419698715\n",
      "iteration 27900, loss 0.09561126679182053\n",
      "iteration 28000, loss 0.0545375794172287\n",
      "iteration 28100, loss 0.07505763322114944\n",
      "TEST accuracy at end of epoch 71: 0.8903\n",
      "TRAIN accuracy at end of epoch 71: 0.96\n",
      "                          epoch 72\n",
      "iteration 28200, loss 0.11324845254421234\n",
      "iteration 28300, loss 0.03404620289802551\n",
      "iteration 28400, loss 0.039606764912605286\n",
      "iteration 28500, loss 0.08722729980945587\n",
      "TEST accuracy at end of epoch 72: 0.8954\n",
      "TRAIN accuracy at end of epoch 72: 1.0\n",
      "                          epoch 73\n",
      "iteration 28600, loss 0.07489221543073654\n",
      "iteration 28700, loss 0.07884696125984192\n",
      "iteration 28800, loss 0.03640763461589813\n",
      "iteration 28900, loss 0.09230130165815353\n",
      "TEST accuracy at end of epoch 73: 0.9018\n",
      "TRAIN accuracy at end of epoch 73: 0.99\n",
      "                          epoch 74\n",
      "iteration 29000, loss 0.03924447298049927\n",
      "iteration 29100, loss 0.03173738718032837\n",
      "iteration 29200, loss 0.018007416278123856\n",
      "iteration 29300, loss 0.053962886333465576\n",
      "TEST accuracy at end of epoch 74: 0.9015\n",
      "TRAIN accuracy at end of epoch 74: 0.99\n",
      "                          epoch 75\n",
      "iteration 29400, loss 0.02381640113890171\n",
      "iteration 29500, loss 0.1008722260594368\n",
      "iteration 29600, loss 0.06303220242261887\n",
      "iteration 29700, loss 0.0714314877986908\n",
      "TEST accuracy at end of epoch 75: 0.8949\n",
      "TRAIN accuracy at end of epoch 75: 0.97\n",
      "                          epoch 76\n",
      "iteration 29800, loss 0.020245011895895004\n",
      "iteration 29900, loss 0.10205486416816711\n",
      "iteration 30000, loss 0.028360139578580856\n",
      "iteration 30100, loss 0.10712078958749771\n",
      "TEST accuracy at end of epoch 76: 0.8868\n",
      "TRAIN accuracy at end of epoch 76: 0.98\n",
      "                          epoch 77\n",
      "iteration 30200, loss 0.032384008169174194\n",
      "iteration 30300, loss 0.06614606827497482\n",
      "iteration 30400, loss 0.1062607541680336\n",
      "TEST accuracy at end of epoch 77: 0.8936\n",
      "TRAIN accuracy at end of epoch 77: 1.0\n",
      "                          epoch 78\n",
      "iteration 30500, loss 0.04795759171247482\n",
      "iteration 30600, loss 0.018308736383914948\n",
      "iteration 30700, loss 0.04962373524904251\n",
      "iteration 30800, loss 0.05584202706813812\n",
      "TEST accuracy at end of epoch 78: 0.8946\n",
      "TRAIN accuracy at end of epoch 78: 0.99\n",
      "                          epoch 79\n",
      "iteration 30900, loss 0.10311861336231232\n",
      "iteration 31000, loss 0.03094932995736599\n",
      "iteration 31100, loss 0.02952347695827484\n",
      "iteration 31200, loss 0.033470503985881805\n",
      "TEST accuracy at end of epoch 79: 0.8916\n",
      "TRAIN accuracy at end of epoch 79: 0.98\n",
      "                          epoch 80\n",
      "iteration 31300, loss 0.07877891510725021\n",
      "iteration 31400, loss 0.0979747474193573\n",
      "iteration 31500, loss 0.02110842429101467\n",
      "iteration 31600, loss 0.11445122957229614\n",
      "TEST accuracy at end of epoch 80: 0.8969\n",
      "TRAIN accuracy at end of epoch 80: 0.97\n",
      "                          epoch 81\n",
      "iteration 31700, loss 0.019590813666582108\n",
      "iteration 31800, loss 0.06082529574632645\n",
      "iteration 31900, loss 0.04055077210068703\n",
      "iteration 32000, loss 0.09373205900192261\n",
      "TEST accuracy at end of epoch 81: 0.8976\n",
      "TRAIN accuracy at end of epoch 81: 0.99\n",
      "                          epoch 82\n",
      "iteration 32100, loss 0.02531331032514572\n",
      "iteration 32200, loss 0.11037850379943848\n",
      "iteration 32300, loss 0.040382564067840576\n",
      "iteration 32400, loss 0.07471314072608948\n",
      "TEST accuracy at end of epoch 82: 0.9025\n",
      "TRAIN accuracy at end of epoch 82: 0.99\n",
      "                          epoch 83\n",
      "iteration 32500, loss 0.05933674797415733\n",
      "iteration 32600, loss 0.04656264930963516\n",
      "iteration 32700, loss 0.09473791718482971\n",
      "iteration 32800, loss 0.05706687271595001\n",
      "TEST accuracy at end of epoch 83: 0.903\n",
      "TRAIN accuracy at end of epoch 83: 0.98\n",
      "                          epoch 84\n",
      "iteration 32900, loss 0.07386026531457901\n",
      "iteration 33000, loss 0.031253959983587265\n",
      "iteration 33100, loss 0.1210499107837677\n",
      "iteration 33200, loss 0.039279866963624954\n",
      "TEST accuracy at end of epoch 84: 0.9018\n",
      "TRAIN accuracy at end of epoch 84: 1.0\n",
      "                          epoch 85\n",
      "iteration 33300, loss 0.13503746688365936\n",
      "iteration 33400, loss 0.034533899277448654\n",
      "iteration 33500, loss 0.036812275648117065\n",
      "iteration 33600, loss 0.021824544295668602\n",
      "TEST accuracy at end of epoch 85: 0.8966\n",
      "TRAIN accuracy at end of epoch 85: 0.98\n",
      "                          epoch 86\n",
      "iteration 33700, loss 0.013605072163045406\n",
      "iteration 33800, loss 0.02378191612660885\n",
      "iteration 33900, loss 0.05461754649877548\n",
      "iteration 34000, loss 0.03754453361034393\n",
      "TEST accuracy at end of epoch 86: 0.9051\n",
      "TRAIN accuracy at end of epoch 86: 0.98\n",
      "                          epoch 87\n",
      "iteration 34100, loss 0.03516406565904617\n",
      "iteration 34200, loss 0.031583987176418304\n",
      "iteration 34300, loss 0.0384131595492363\n",
      "iteration 34400, loss 0.03007921762764454\n",
      "TEST accuracy at end of epoch 87: 0.9024\n",
      "TRAIN accuracy at end of epoch 87: 0.99\n",
      "                          epoch 88\n",
      "iteration 34500, loss 0.03075466863811016\n",
      "iteration 34600, loss 0.017640013247728348\n",
      "iteration 34700, loss 0.018014488741755486\n",
      "TEST accuracy at end of epoch 88: 0.9021\n",
      "TRAIN accuracy at end of epoch 88: 1.0\n",
      "                          epoch 89\n",
      "iteration 34800, loss 0.045841313898563385\n",
      "iteration 34900, loss 0.06940080970525742\n",
      "iteration 35000, loss 0.051212128251791\n",
      "iteration 35100, loss 0.027794403955340385\n",
      "TEST accuracy at end of epoch 89: 0.8983\n",
      "TRAIN accuracy at end of epoch 89: 0.95\n",
      "                          epoch 90\n",
      "iteration 35200, loss 0.0625496357679367\n",
      "iteration 35300, loss 0.02360248565673828\n",
      "iteration 35400, loss 0.026261966675519943\n",
      "iteration 35500, loss 0.04441756010055542\n",
      "TEST accuracy at end of epoch 90: 0.9041\n",
      "TRAIN accuracy at end of epoch 90: 1.0\n",
      "                          epoch 91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 35600, loss 0.04274766892194748\n",
      "iteration 35700, loss 0.0039933775551617146\n",
      "iteration 35800, loss 0.019551873207092285\n",
      "iteration 35900, loss 0.058329109102487564\n",
      "TEST accuracy at end of epoch 91: 0.9024\n",
      "TRAIN accuracy at end of epoch 91: 1.0\n",
      "                          epoch 92\n",
      "iteration 36000, loss 0.0025123595260083675\n",
      "iteration 36100, loss 0.009392226114869118\n",
      "iteration 36200, loss 0.10311968624591827\n",
      "iteration 36300, loss 0.03397025540471077\n",
      "TEST accuracy at end of epoch 92: 0.9082\n",
      "TRAIN accuracy at end of epoch 92: 0.99\n",
      "                          epoch 93\n",
      "iteration 36400, loss 0.004442608915269375\n",
      "iteration 36500, loss 0.02798750437796116\n",
      "iteration 36600, loss 0.011620501056313515\n",
      "iteration 36700, loss 0.02060341089963913\n",
      "TEST accuracy at end of epoch 93: 0.9069\n",
      "TRAIN accuracy at end of epoch 93: 0.98\n",
      "                          epoch 94\n",
      "iteration 36800, loss 0.005724034737795591\n",
      "iteration 36900, loss 0.017366211861371994\n",
      "iteration 37000, loss 0.06270013749599457\n",
      "iteration 37100, loss 0.03389838710427284\n",
      "TEST accuracy at end of epoch 94: 0.9016\n",
      "TRAIN accuracy at end of epoch 94: 1.0\n",
      "                          epoch 95\n",
      "iteration 37200, loss 0.02926362305879593\n",
      "iteration 37300, loss 0.0394759327173233\n",
      "iteration 37400, loss 0.029762614518404007\n",
      "iteration 37500, loss 0.07048600167036057\n",
      "TEST accuracy at end of epoch 95: 0.8948\n",
      "TRAIN accuracy at end of epoch 95: 0.99\n",
      "                          epoch 96\n",
      "iteration 37600, loss 0.024145621806383133\n",
      "iteration 37700, loss 0.018903521820902824\n",
      "iteration 37800, loss 0.03837965428829193\n",
      "iteration 37900, loss 0.014802007004618645\n",
      "TEST accuracy at end of epoch 96: 0.9072\n",
      "TRAIN accuracy at end of epoch 96: 1.0\n",
      "                          epoch 97\n",
      "iteration 38000, loss 0.06508424878120422\n",
      "iteration 38100, loss 0.00417814776301384\n",
      "iteration 38200, loss 0.023997433483600616\n",
      "iteration 38300, loss 0.07129926234483719\n",
      "TEST accuracy at end of epoch 97: 0.8994\n",
      "TRAIN accuracy at end of epoch 97: 0.99\n",
      "                          epoch 98\n",
      "iteration 38400, loss 0.038898028433322906\n",
      "iteration 38500, loss 0.02524120733141899\n",
      "iteration 38600, loss 0.008504017256200314\n",
      "iteration 38700, loss 0.004829648416489363\n",
      "TEST accuracy at end of epoch 98: 0.895\n",
      "TRAIN accuracy at end of epoch 98: 0.99\n",
      "                          epoch 99\n",
      "iteration 38800, loss 0.05149970203638077\n",
      "iteration 38900, loss 0.005031226202845573\n",
      "iteration 39000, loss 0.00986857246607542\n",
      "iteration 39100, loss 0.10036424547433853\n",
      "TEST accuracy at end of epoch 99: 0.9019\n",
      "TRAIN accuracy at end of epoch 99: 1.0\n",
      "                          epoch 100\n",
      "iteration 39200, loss 0.009370255284011364\n",
      "iteration 39300, loss 0.013906610198318958\n",
      "iteration 39400, loss 0.014305038377642632\n",
      "TEST accuracy at end of epoch 100: 0.9016\n",
      "TRAIN accuracy at end of epoch 100: 0.99\n",
      "                          epoch 101\n",
      "iteration 39500, loss 0.02663179114460945\n",
      "iteration 39600, loss 0.08323925733566284\n",
      "iteration 39700, loss 0.033654775470495224\n",
      "iteration 39800, loss 0.01387016847729683\n",
      "TEST accuracy at end of epoch 101: 0.9075\n",
      "TRAIN accuracy at end of epoch 101: 1.0\n",
      "                          epoch 102\n",
      "iteration 39900, loss 0.006744519807398319\n",
      "iteration 40000, loss 0.043146487325429916\n",
      "iteration 40100, loss 0.06843645125627518\n",
      "iteration 40200, loss 0.015447430312633514\n",
      "TEST accuracy at end of epoch 102: 0.9032\n",
      "TRAIN accuracy at end of epoch 102: 0.99\n",
      "                          epoch 103\n",
      "iteration 40300, loss 0.010907072573900223\n",
      "iteration 40400, loss 0.036996662616729736\n",
      "iteration 40500, loss 0.04661596566438675\n",
      "iteration 40600, loss 0.049427568912506104\n",
      "TEST accuracy at end of epoch 103: 0.9031\n",
      "TRAIN accuracy at end of epoch 103: 1.0\n",
      "                          epoch 104\n",
      "iteration 40700, loss 0.023456038907170296\n",
      "iteration 40800, loss 0.017596744000911713\n",
      "iteration 40900, loss 0.015891486778855324\n",
      "iteration 41000, loss 0.013130588456988335\n",
      "TEST accuracy at end of epoch 104: 0.9026\n",
      "TRAIN accuracy at end of epoch 104: 1.0\n",
      "                          epoch 105\n",
      "iteration 41100, loss 0.044701553881168365\n",
      "iteration 41200, loss 0.06779848039150238\n",
      "iteration 41300, loss 0.012640831992030144\n",
      "iteration 41400, loss 0.011855768971145153\n",
      "TEST accuracy at end of epoch 105: 0.909\n",
      "TRAIN accuracy at end of epoch 105: 1.0\n",
      "                          epoch 106\n",
      "iteration 41500, loss 0.024138472974300385\n",
      "iteration 41600, loss 0.010803936049342155\n",
      "iteration 41700, loss 0.015165054239332676\n",
      "iteration 41800, loss 0.05369780957698822\n",
      "TEST accuracy at end of epoch 106: 0.9047\n",
      "TRAIN accuracy at end of epoch 106: 0.98\n",
      "                          epoch 107\n",
      "iteration 41900, loss 0.04690215364098549\n",
      "iteration 42000, loss 0.042229726910591125\n",
      "iteration 42100, loss 0.016283251345157623\n",
      "iteration 42200, loss 0.041951391845941544\n",
      "TEST accuracy at end of epoch 107: 0.8978\n",
      "TRAIN accuracy at end of epoch 107: 0.99\n",
      "                          epoch 108\n",
      "iteration 42300, loss 0.010734076611697674\n",
      "iteration 42400, loss 0.0347464457154274\n",
      "iteration 42500, loss 0.002703412901610136\n",
      "iteration 42600, loss 0.07099680602550507\n",
      "TEST accuracy at end of epoch 108: 0.9075\n",
      "TRAIN accuracy at end of epoch 108: 0.99\n",
      "                          epoch 109\n",
      "iteration 42700, loss 0.008110597729682922\n",
      "iteration 42800, loss 0.04149872809648514\n",
      "iteration 42900, loss 0.013503951951861382\n",
      "iteration 43000, loss 0.02684801071882248\n",
      "TEST accuracy at end of epoch 109: 0.9096\n",
      "TRAIN accuracy at end of epoch 109: 1.0\n",
      "                          epoch 110\n",
      "iteration 43100, loss 0.043199315667152405\n",
      "iteration 43200, loss 0.019294176250696182\n",
      "iteration 43300, loss 0.01861445978283882\n",
      "iteration 43400, loss 0.006649911403656006\n",
      "TEST accuracy at end of epoch 110: 0.9092\n",
      "TRAIN accuracy at end of epoch 110: 1.0\n",
      "                          epoch 111\n",
      "iteration 43500, loss 0.012651319615542889\n",
      "iteration 43600, loss 0.004454443231225014\n",
      "iteration 43700, loss 0.038754746317863464\n",
      "TEST accuracy at end of epoch 111: 0.9093\n",
      "TRAIN accuracy at end of epoch 111: 0.98\n",
      "                          epoch 112\n",
      "iteration 43800, loss 0.003726538270711899\n",
      "iteration 43900, loss 0.020494287833571434\n",
      "iteration 44000, loss 0.014347284100949764\n",
      "iteration 44100, loss 0.029988260939717293\n",
      "TEST accuracy at end of epoch 112: 0.9009\n",
      "TRAIN accuracy at end of epoch 112: 1.0\n",
      "                          epoch 113\n",
      "iteration 44200, loss 0.011827373877167702\n",
      "iteration 44300, loss 0.0052102478221058846\n",
      "iteration 44400, loss 0.04050426930189133\n",
      "iteration 44500, loss 0.037390392273664474\n",
      "TEST accuracy at end of epoch 113: 0.9112\n",
      "TRAIN accuracy at end of epoch 113: 1.0\n",
      "                          epoch 114\n",
      "iteration 44600, loss 0.006817376706749201\n",
      "iteration 44700, loss 0.01140053104609251\n",
      "iteration 44800, loss 0.0007453891448676586\n",
      "iteration 44900, loss 0.005406802520155907\n",
      "TEST accuracy at end of epoch 114: 0.9128\n",
      "TRAIN accuracy at end of epoch 114: 1.0\n",
      "                          epoch 115\n",
      "iteration 45000, loss 0.003456456819549203\n",
      "iteration 45100, loss 0.02864052541553974\n",
      "iteration 45200, loss 0.027057431638240814\n",
      "iteration 45300, loss 0.03905067220330238\n",
      "TEST accuracy at end of epoch 115: 0.916\n",
      "TRAIN accuracy at end of epoch 115: 1.0\n",
      "                          epoch 116\n",
      "iteration 45400, loss 0.014394158497452736\n",
      "iteration 45500, loss 0.002019930398091674\n",
      "iteration 45600, loss 0.003193080658093095\n",
      "iteration 45700, loss 0.007468815892934799\n",
      "TEST accuracy at end of epoch 116: 0.9125\n",
      "TRAIN accuracy at end of epoch 116: 0.98\n",
      "                          epoch 117\n",
      "iteration 45800, loss 0.012683930806815624\n",
      "iteration 45900, loss 0.03156162053346634\n",
      "iteration 46000, loss 0.029531579464673996\n",
      "iteration 46100, loss 0.011675415560603142\n",
      "TEST accuracy at end of epoch 117: 0.9141\n",
      "TRAIN accuracy at end of epoch 117: 1.0\n",
      "                          epoch 118\n",
      "iteration 46200, loss 0.007855568081140518\n",
      "iteration 46300, loss 0.008855478838086128\n",
      "iteration 46400, loss 0.0020983885042369366\n",
      "iteration 46500, loss 0.016616059467196465\n",
      "TEST accuracy at end of epoch 118: 0.9166\n",
      "TRAIN accuracy at end of epoch 118: 1.0\n",
      "                          epoch 119\n",
      "iteration 46600, loss 0.011820059269666672\n",
      "iteration 46700, loss 0.00854835007339716\n",
      "iteration 46800, loss 0.0072474973276257515\n",
      "iteration 46900, loss 0.025974111631512642\n",
      "TEST accuracy at end of epoch 119: 0.9114\n",
      "TRAIN accuracy at end of epoch 119: 1.0\n",
      "                          epoch 120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 47000, loss 0.007979114539921284\n",
      "iteration 47100, loss 0.003855672664940357\n",
      "iteration 47200, loss 0.042015496641397476\n",
      "iteration 47300, loss 0.025184571743011475\n",
      "TEST accuracy at end of epoch 120: 0.9146\n",
      "TRAIN accuracy at end of epoch 120: 1.0\n",
      "                          epoch 121\n",
      "iteration 47400, loss 0.002494396176189184\n",
      "iteration 47500, loss 0.0009203733643516898\n",
      "iteration 47600, loss 0.003539903089404106\n",
      "iteration 47700, loss 0.010255426168441772\n",
      "TEST accuracy at end of epoch 121: 0.9117\n",
      "TRAIN accuracy at end of epoch 121: 1.0\n",
      "                          epoch 122\n",
      "iteration 47800, loss 0.01618807762861252\n",
      "iteration 47900, loss 0.026001499965786934\n",
      "iteration 48000, loss 0.06796173751354218\n",
      "TEST accuracy at end of epoch 122: 0.9126\n",
      "TRAIN accuracy at end of epoch 122: 1.0\n",
      "                          epoch 123\n",
      "iteration 48100, loss 0.026954444125294685\n",
      "iteration 48200, loss 0.004287140443921089\n",
      "iteration 48300, loss 0.014398063533008099\n",
      "iteration 48400, loss 0.002771844854578376\n",
      "TEST accuracy at end of epoch 123: 0.9149\n",
      "TRAIN accuracy at end of epoch 123: 1.0\n",
      "                          epoch 124\n",
      "iteration 48500, loss 0.027752617374062538\n",
      "iteration 48600, loss 0.0022794653195887804\n",
      "iteration 48700, loss 0.015986286103725433\n",
      "iteration 48800, loss 0.0017860516672953963\n",
      "TEST accuracy at end of epoch 124: 0.9188\n",
      "TRAIN accuracy at end of epoch 124: 1.0\n",
      "                          epoch 125\n",
      "iteration 48900, loss 0.022289499640464783\n",
      "iteration 49000, loss 0.02053239941596985\n",
      "iteration 49100, loss 0.006737865507602692\n",
      "iteration 49200, loss 0.010868560522794724\n",
      "TEST accuracy at end of epoch 125: 0.9156\n",
      "TRAIN accuracy at end of epoch 125: 1.0\n",
      "                          epoch 126\n",
      "iteration 49300, loss 0.019383996725082397\n",
      "iteration 49400, loss 0.00792163796722889\n",
      "iteration 49500, loss 0.0023539229296147823\n",
      "iteration 49600, loss 0.00351586751639843\n",
      "TEST accuracy at end of epoch 126: 0.9132\n",
      "TRAIN accuracy at end of epoch 126: 0.99\n",
      "                          epoch 127\n",
      "iteration 49700, loss 0.02159116230905056\n",
      "iteration 49800, loss 0.009530628100037575\n",
      "iteration 49900, loss 0.008233338594436646\n",
      "iteration 50000, loss 0.0099874846637249\n",
      "TEST accuracy at end of epoch 127: 0.9155\n",
      "TRAIN accuracy at end of epoch 127: 1.0\n",
      "                          epoch 128\n",
      "iteration 50100, loss 0.018435627222061157\n",
      "iteration 50200, loss 0.004685991443693638\n",
      "iteration 50300, loss 0.002184540033340454\n",
      "iteration 50400, loss 0.0016776751726865768\n",
      "TEST accuracy at end of epoch 128: 0.9166\n",
      "TRAIN accuracy at end of epoch 128: 1.0\n",
      "                          epoch 129\n",
      "iteration 50500, loss 0.0034653497859835625\n",
      "iteration 50600, loss 0.001650025136768818\n",
      "iteration 50700, loss 0.022006360813975334\n",
      "iteration 50800, loss 0.013264179229736328\n",
      "TEST accuracy at end of epoch 129: 0.9166\n",
      "TRAIN accuracy at end of epoch 129: 1.0\n",
      "                          epoch 130\n",
      "iteration 50900, loss 0.0015370356850326061\n",
      "iteration 51000, loss 0.01204084511846304\n",
      "iteration 51100, loss 0.020991671830415726\n",
      "iteration 51200, loss 0.00032641581492498517\n",
      "TEST accuracy at end of epoch 130: 0.9182\n",
      "TRAIN accuracy at end of epoch 130: 1.0\n",
      "                          epoch 131\n",
      "iteration 51300, loss 0.002249743789434433\n",
      "iteration 51400, loss 0.0008089868351817131\n",
      "iteration 51500, loss 0.0015781540423631668\n",
      "iteration 51600, loss 0.012523278594017029\n",
      "TEST accuracy at end of epoch 131: 0.9188\n",
      "TRAIN accuracy at end of epoch 131: 1.0\n",
      "                          epoch 132\n",
      "iteration 51700, loss 0.013799619860947132\n",
      "iteration 51800, loss 0.008074887096881866\n",
      "iteration 51900, loss 0.05804329365491867\n",
      "iteration 52000, loss 0.0066863661631941795\n",
      "TEST accuracy at end of epoch 132: 0.916\n",
      "TRAIN accuracy at end of epoch 132: 1.0\n",
      "                          epoch 133\n",
      "iteration 52100, loss 0.003189690178260207\n",
      "iteration 52200, loss 0.018740197643637657\n",
      "iteration 52300, loss 0.0023174509406089783\n",
      "TEST accuracy at end of epoch 133: 0.9176\n",
      "TRAIN accuracy at end of epoch 133: 1.0\n",
      "                          epoch 134\n",
      "iteration 52400, loss 0.002935715252533555\n",
      "iteration 52500, loss 0.007184029556810856\n",
      "iteration 52600, loss 0.0019207110162824392\n",
      "iteration 52700, loss 0.008219213224947453\n",
      "TEST accuracy at end of epoch 134: 0.9164\n",
      "TRAIN accuracy at end of epoch 134: 1.0\n",
      "                          epoch 135\n",
      "iteration 52800, loss 0.0010311653604730964\n",
      "iteration 52900, loss 0.005729721859097481\n",
      "iteration 53000, loss 0.009970923885703087\n",
      "iteration 53100, loss 0.015509393997490406\n",
      "TEST accuracy at end of epoch 135: 0.9139\n",
      "TRAIN accuracy at end of epoch 135: 1.0\n",
      "                          epoch 136\n",
      "iteration 53200, loss 0.012690502218902111\n",
      "iteration 53300, loss 0.0027421675622463226\n",
      "iteration 53400, loss 0.002752854023128748\n",
      "iteration 53500, loss 0.01525843795388937\n",
      "TEST accuracy at end of epoch 136: 0.9179\n",
      "TRAIN accuracy at end of epoch 136: 1.0\n",
      "                          epoch 137\n",
      "iteration 53600, loss 0.002270039636641741\n",
      "iteration 53700, loss 0.0026970715261995792\n",
      "iteration 53800, loss 0.022279908880591393\n",
      "iteration 53900, loss 0.001523202983662486\n",
      "TEST accuracy at end of epoch 137: 0.9149\n",
      "TRAIN accuracy at end of epoch 137: 1.0\n",
      "                          epoch 138\n",
      "iteration 54000, loss 0.0008841407834552228\n",
      "iteration 54100, loss 0.0023781326599419117\n",
      "iteration 54200, loss 0.0006161799537949264\n",
      "iteration 54300, loss 0.0014095194637775421\n",
      "TEST accuracy at end of epoch 138: 0.9166\n",
      "TRAIN accuracy at end of epoch 138: 1.0\n",
      "                          epoch 139\n",
      "iteration 54400, loss 0.0008165427716448903\n",
      "iteration 54500, loss 0.0006568097742274404\n",
      "iteration 54600, loss 0.004193214233964682\n",
      "iteration 54700, loss 0.0009728432632982731\n",
      "TEST accuracy at end of epoch 139: 0.9162\n",
      "TRAIN accuracy at end of epoch 139: 1.0\n",
      "                          epoch 140\n",
      "iteration 54800, loss 0.0011916563380509615\n",
      "iteration 54900, loss 0.003365535056218505\n",
      "iteration 55000, loss 0.002240394940599799\n",
      "iteration 55100, loss 0.014630334451794624\n",
      "TEST accuracy at end of epoch 140: 0.917\n",
      "TRAIN accuracy at end of epoch 140: 1.0\n",
      "                          epoch 141\n",
      "iteration 55200, loss 0.0035125189460814\n",
      "iteration 55300, loss 0.030356328934431076\n",
      "iteration 55400, loss 0.002691752277314663\n",
      "iteration 55500, loss 0.0008395790937356651\n",
      "TEST accuracy at end of epoch 141: 0.9152\n",
      "TRAIN accuracy at end of epoch 141: 1.0\n",
      "                          epoch 142\n",
      "iteration 55600, loss 0.011560207232832909\n",
      "iteration 55700, loss 0.012743987143039703\n",
      "iteration 55800, loss 0.0028847085777670145\n",
      "iteration 55900, loss 0.00016252849309239537\n",
      "TEST accuracy at end of epoch 142: 0.9185\n",
      "TRAIN accuracy at end of epoch 142: 1.0\n",
      "                          epoch 143\n",
      "iteration 56000, loss 0.004063667729496956\n",
      "iteration 56100, loss 0.0003050616942346096\n",
      "iteration 56200, loss 0.024679062888026237\n",
      "iteration 56300, loss 0.00039850303437560797\n",
      "TEST accuracy at end of epoch 143: 0.9171\n",
      "TRAIN accuracy at end of epoch 143: 1.0\n",
      "                          epoch 144\n",
      "iteration 56400, loss 0.03285237401723862\n",
      "iteration 56500, loss 0.009272467344999313\n",
      "iteration 56600, loss 0.002163088880479336\n",
      "TEST accuracy at end of epoch 144: 0.9189\n",
      "TRAIN accuracy at end of epoch 144: 1.0\n",
      "                          epoch 145\n",
      "iteration 56700, loss 0.002658347599208355\n",
      "iteration 56800, loss 0.006225422956049442\n",
      "iteration 56900, loss 0.0011716505978256464\n",
      "iteration 57000, loss 0.001471871160902083\n",
      "TEST accuracy at end of epoch 145: 0.9184\n",
      "TRAIN accuracy at end of epoch 145: 1.0\n",
      "                          epoch 146\n",
      "iteration 57100, loss 0.00043929420644417405\n",
      "iteration 57200, loss 0.0004307278722990304\n",
      "iteration 57300, loss 0.00989625509828329\n",
      "iteration 57400, loss 0.0010113369207829237\n",
      "TEST accuracy at end of epoch 146: 0.9179\n",
      "TRAIN accuracy at end of epoch 146: 1.0\n",
      "                          epoch 147\n",
      "iteration 57500, loss 0.019203979521989822\n",
      "iteration 57600, loss 0.0033364759292453527\n",
      "iteration 57700, loss 0.00024526964989490807\n",
      "iteration 57800, loss 0.007061146665364504\n",
      "TEST accuracy at end of epoch 147: 0.9189\n",
      "TRAIN accuracy at end of epoch 147: 1.0\n",
      "                          epoch 148\n",
      "iteration 57900, loss 0.003641510382294655\n",
      "iteration 58000, loss 0.0029380875639617443\n",
      "iteration 58100, loss 0.014947434887290001\n",
      "iteration 58200, loss 0.001142000313848257\n",
      "TEST accuracy at end of epoch 148: 0.9181\n",
      "TRAIN accuracy at end of epoch 148: 0.99\n",
      "                          epoch 149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 58300, loss 0.0017886837013065815\n",
      "iteration 58400, loss 0.0020964983850717545\n",
      "iteration 58500, loss 0.0019907504320144653\n",
      "iteration 58600, loss 0.0017627859488129616\n",
      "TEST accuracy at end of epoch 149: 0.9179\n",
      "TRAIN accuracy at end of epoch 149: 1.0\n",
      "                          epoch 150\n",
      "iteration 58700, loss 0.0007792949327267706\n",
      "iteration 58800, loss 0.00029466566047631204\n",
      "iteration 58900, loss 0.0019830188248306513\n",
      "iteration 59000, loss 0.0019575415644794703\n",
      "TEST accuracy at end of epoch 150: 0.9186\n",
      "TRAIN accuracy at end of epoch 150: 1.0\n",
      "                          epoch 151\n",
      "iteration 59100, loss 0.00018828392785508186\n",
      "iteration 59200, loss 0.0028390439692884684\n",
      "iteration 59300, loss 0.016981732100248337\n",
      "iteration 59400, loss 0.008599518798291683\n",
      "TEST accuracy at end of epoch 151: 0.9187\n",
      "TRAIN accuracy at end of epoch 151: 1.0\n",
      "                          epoch 152\n",
      "iteration 59500, loss 0.02579294517636299\n",
      "iteration 59600, loss 0.0014328241813927889\n",
      "iteration 59700, loss 0.04842266067862511\n",
      "iteration 59800, loss 0.0021916229743510485\n",
      "TEST accuracy at end of epoch 152: 0.9187\n",
      "TRAIN accuracy at end of epoch 152: 1.0\n",
      "                          epoch 153\n",
      "iteration 59900, loss 0.001440681517124176\n",
      "iteration 60000, loss 0.0012571713887155056\n",
      "iteration 60100, loss 0.0023269078228622675\n",
      "iteration 60200, loss 0.010428674519062042\n",
      "TEST accuracy at end of epoch 153: 0.9181\n",
      "TRAIN accuracy at end of epoch 153: 1.0\n",
      "                          epoch 154\n",
      "iteration 60300, loss 0.00045593446702696383\n",
      "iteration 60400, loss 0.004019510000944138\n",
      "iteration 60500, loss 0.006780487019568682\n",
      "iteration 60600, loss 0.005426997318863869\n",
      "TEST accuracy at end of epoch 154: 0.9192\n",
      "TRAIN accuracy at end of epoch 154: 1.0\n",
      "                          epoch 155\n",
      "iteration 60700, loss 0.0006479028961621225\n",
      "iteration 60800, loss 0.00021171345724724233\n",
      "iteration 60900, loss 0.0005233464762568474\n",
      "TEST accuracy at end of epoch 155: 0.9202\n",
      "TRAIN accuracy at end of epoch 155: 1.0\n",
      "                          epoch 156\n",
      "iteration 61000, loss 0.001986720599234104\n",
      "iteration 61100, loss 0.0031099303159862757\n",
      "iteration 61200, loss 0.002456521848216653\n",
      "iteration 61300, loss 0.003410910489037633\n",
      "TEST accuracy at end of epoch 156: 0.9185\n",
      "TRAIN accuracy at end of epoch 156: 1.0\n",
      "                          epoch 157\n",
      "iteration 61400, loss 0.010691988281905651\n",
      "iteration 61500, loss 0.0007719849236309528\n",
      "iteration 61600, loss 0.0023503098636865616\n",
      "iteration 61700, loss 0.0008817227790132165\n",
      "TEST accuracy at end of epoch 157: 0.919\n",
      "TRAIN accuracy at end of epoch 157: 1.0\n",
      "                          epoch 158\n",
      "iteration 61800, loss 0.0027727442793548107\n",
      "iteration 61900, loss 0.00020039058290421963\n",
      "iteration 62000, loss 0.0009208478732034564\n",
      "iteration 62100, loss 0.0017903740517795086\n",
      "TEST accuracy at end of epoch 158: 0.9171\n",
      "TRAIN accuracy at end of epoch 158: 1.0\n",
      "                          epoch 159\n",
      "iteration 62200, loss 0.0004618408274836838\n",
      "iteration 62300, loss 0.0011452813632786274\n",
      "iteration 62400, loss 0.0005644694902002811\n",
      "iteration 62500, loss 0.007014505565166473\n",
      "TEST accuracy at end of epoch 159: 0.9195\n",
      "TRAIN accuracy at end of epoch 159: 1.0\n",
      "                          epoch 160\n",
      "iteration 62600, loss 0.007574640680104494\n",
      "iteration 62700, loss 0.00030195448198355734\n",
      "iteration 62800, loss 0.002932984381914139\n",
      "iteration 62900, loss 0.0008857476059347391\n",
      "TEST accuracy at end of epoch 160: 0.9178\n",
      "TRAIN accuracy at end of epoch 160: 1.0\n",
      "                          epoch 161\n",
      "iteration 63000, loss 0.0067981574684381485\n",
      "iteration 63100, loss 0.00014533899957314134\n",
      "iteration 63200, loss 0.005766619928181171\n",
      "iteration 63300, loss 0.010672067292034626\n",
      "TEST accuracy at end of epoch 161: 0.9187\n",
      "TRAIN accuracy at end of epoch 161: 1.0\n",
      "                          epoch 162\n",
      "iteration 63400, loss 0.000479914277093485\n",
      "iteration 63500, loss 0.000723313249181956\n",
      "iteration 63600, loss 0.0002814748149830848\n",
      "iteration 63700, loss 0.0011000726372003555\n",
      "TEST accuracy at end of epoch 162: 0.9189\n",
      "TRAIN accuracy at end of epoch 162: 1.0\n",
      "                          epoch 163\n",
      "iteration 63800, loss 0.0015680924989283085\n",
      "iteration 63900, loss 0.0005612956010736525\n",
      "iteration 64000, loss 0.0009853702504187822\n",
      "iteration 64100, loss 0.0008616478880867362\n",
      "TEST accuracy at end of epoch 163: 0.9179\n",
      "TRAIN accuracy at end of epoch 163: 1.0\n",
      "                          epoch 164\n",
      "iteration 64200, loss 0.0020059188827872276\n",
      "iteration 64300, loss 0.0006107948138378561\n",
      "iteration 64400, loss 0.005122540984302759\n",
      "iteration 64500, loss 0.0006996492738835514\n",
      "TEST accuracy at end of epoch 164: 0.9184\n",
      "TRAIN accuracy at end of epoch 164: 1.0\n",
      "                          epoch 165\n",
      "iteration 64600, loss 0.0015571911353617907\n",
      "iteration 64700, loss 0.020969519391655922\n",
      "iteration 64800, loss 0.0003313136112410575\n",
      "iteration 64900, loss 0.016114436089992523\n",
      "TEST accuracy at end of epoch 165: 0.9193\n",
      "TRAIN accuracy at end of epoch 165: 1.0\n",
      "                          epoch 166\n",
      "iteration 65000, loss 0.0019934384617954493\n",
      "iteration 65100, loss 0.000503338233102113\n",
      "iteration 65200, loss 0.00040996246389113367\n",
      "TEST accuracy at end of epoch 166: 0.9185\n",
      "TRAIN accuracy at end of epoch 166: 1.0\n",
      "                          epoch 167\n",
      "iteration 65300, loss 0.0011349993292242289\n",
      "iteration 65400, loss 0.0006172052817419171\n",
      "iteration 65500, loss 0.008647484704852104\n",
      "iteration 65600, loss 0.0002515515952836722\n",
      "TEST accuracy at end of epoch 167: 0.9194\n",
      "TRAIN accuracy at end of epoch 167: 1.0\n",
      "                          epoch 168\n",
      "iteration 65700, loss 0.000367620843462646\n",
      "iteration 65800, loss 0.0006995723815634847\n",
      "iteration 65900, loss 0.007171886041760445\n",
      "iteration 66000, loss 0.006899720523506403\n",
      "TEST accuracy at end of epoch 168: 0.9189\n",
      "TRAIN accuracy at end of epoch 168: 1.0\n",
      "                          epoch 169\n",
      "iteration 66100, loss 0.010916260071098804\n",
      "iteration 66200, loss 0.00014864947297610343\n",
      "iteration 66300, loss 0.000700152653735131\n",
      "iteration 66400, loss 0.002275093225762248\n",
      "TEST accuracy at end of epoch 169: 0.9202\n",
      "TRAIN accuracy at end of epoch 169: 1.0\n",
      "                          epoch 170\n",
      "iteration 66500, loss 0.0014171793591231108\n",
      "iteration 66600, loss 0.005460446700453758\n",
      "iteration 66700, loss 0.0013231767807155848\n",
      "iteration 66800, loss 0.0006253226893022656\n",
      "TEST accuracy at end of epoch 170: 0.9197\n",
      "TRAIN accuracy at end of epoch 170: 1.0\n",
      "                          epoch 171\n",
      "iteration 66900, loss 0.0001776411954779178\n",
      "iteration 67000, loss 0.0008751724380999804\n",
      "iteration 67100, loss 0.00020667239732574672\n",
      "iteration 67200, loss 0.003292518202215433\n",
      "TEST accuracy at end of epoch 171: 0.9196\n",
      "TRAIN accuracy at end of epoch 171: 1.0\n",
      "                          epoch 172\n",
      "iteration 67300, loss 0.0007420429028570652\n",
      "iteration 67400, loss 0.01137549802660942\n",
      "iteration 67500, loss 0.0035559777170419693\n",
      "iteration 67600, loss 0.0002034394274232909\n",
      "TEST accuracy at end of epoch 172: 0.9201\n",
      "TRAIN accuracy at end of epoch 172: 1.0\n",
      "                          epoch 173\n",
      "iteration 67700, loss 0.00015049739158712327\n",
      "iteration 67800, loss 0.0003803374420385808\n",
      "iteration 67900, loss 0.0012585010845214128\n",
      "iteration 68000, loss 0.0002613881661091\n",
      "TEST accuracy at end of epoch 173: 0.9206\n",
      "TRAIN accuracy at end of epoch 173: 1.0\n",
      "                          epoch 174\n",
      "iteration 68100, loss 0.0014853787142783403\n",
      "iteration 68200, loss 0.00021205807570368052\n",
      "iteration 68300, loss 0.008513792417943478\n",
      "iteration 68400, loss 0.0006335564539767802\n",
      "TEST accuracy at end of epoch 174: 0.9206\n",
      "TRAIN accuracy at end of epoch 174: 1.0\n",
      "                          epoch 175\n",
      "iteration 68500, loss 0.00028871322865597904\n",
      "iteration 68600, loss 0.0029766769148409367\n",
      "iteration 68700, loss 8.656430873088539e-05\n",
      "iteration 68800, loss 0.0006531747058033943\n",
      "TEST accuracy at end of epoch 175: 0.9207\n",
      "TRAIN accuracy at end of epoch 175: 1.0\n",
      "                          epoch 176\n",
      "iteration 68900, loss 0.011532477103173733\n",
      "iteration 69000, loss 0.0020694055128842592\n",
      "iteration 69100, loss 0.00010625053255353123\n",
      "iteration 69200, loss 0.00031932242563925683\n",
      "TEST accuracy at end of epoch 176: 0.92\n",
      "TRAIN accuracy at end of epoch 176: 1.0\n",
      "                          epoch 177\n",
      "iteration 69300, loss 0.0017675673589110374\n",
      "iteration 69400, loss 0.00071994453901425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 69500, loss 0.0011019790545105934\n",
      "TEST accuracy at end of epoch 177: 0.9195\n",
      "TRAIN accuracy at end of epoch 177: 1.0\n",
      "                          epoch 178\n",
      "iteration 69600, loss 0.0006427316111512482\n",
      "iteration 69700, loss 8.350241114385426e-05\n",
      "iteration 69800, loss 0.0038167955353856087\n",
      "iteration 69900, loss 0.0006352027412503958\n",
      "TEST accuracy at end of epoch 178: 0.9195\n",
      "TRAIN accuracy at end of epoch 178: 1.0\n",
      "                          epoch 179\n",
      "iteration 70000, loss 0.008890892378985882\n",
      "iteration 70100, loss 0.0031575036700814962\n",
      "iteration 70200, loss 0.0003927784273400903\n",
      "iteration 70300, loss 0.000801917165517807\n",
      "TEST accuracy at end of epoch 179: 0.9196\n",
      "TRAIN accuracy at end of epoch 179: 1.0\n",
      "                          epoch 180\n",
      "iteration 70400, loss 9.888417844194919e-05\n",
      "iteration 70500, loss 0.0014553614892065525\n",
      "iteration 70600, loss 0.00017519999528303742\n",
      "iteration 70700, loss 0.000286987517029047\n",
      "TEST accuracy at end of epoch 180: 0.9194\n",
      "TRAIN accuracy at end of epoch 180: 1.0\n",
      "                          epoch 181\n",
      "iteration 70800, loss 0.006092600524425507\n",
      "iteration 70900, loss 0.00023261670139618218\n",
      "iteration 71000, loss 0.00044237272231839597\n",
      "iteration 71100, loss 0.0066510592587292194\n",
      "TEST accuracy at end of epoch 181: 0.9195\n",
      "TRAIN accuracy at end of epoch 181: 1.0\n",
      "                          epoch 182\n",
      "iteration 71200, loss 0.0016221744008362293\n",
      "iteration 71300, loss 0.00246643484570086\n",
      "iteration 71400, loss 0.0018564265919849277\n",
      "iteration 71500, loss 0.0003484236658550799\n",
      "TEST accuracy at end of epoch 182: 0.9195\n",
      "TRAIN accuracy at end of epoch 182: 1.0\n",
      "                          epoch 183\n",
      "iteration 71600, loss 0.0001150607131421566\n",
      "iteration 71700, loss 0.00039217175799421966\n",
      "iteration 71800, loss 0.0002329937124159187\n",
      "iteration 71900, loss 0.017421867698431015\n",
      "TEST accuracy at end of epoch 183: 0.92\n",
      "TRAIN accuracy at end of epoch 183: 1.0\n",
      "                          epoch 184\n",
      "iteration 72000, loss 0.00011920416727662086\n",
      "iteration 72100, loss 0.001838842872530222\n",
      "iteration 72200, loss 0.00017135444795712829\n",
      "iteration 72300, loss 0.00010830578685272485\n",
      "TEST accuracy at end of epoch 184: 0.9193\n",
      "TRAIN accuracy at end of epoch 184: 1.0\n",
      "                          epoch 185\n",
      "iteration 72400, loss 0.018490200862288475\n",
      "iteration 72500, loss 0.001139917178079486\n",
      "iteration 72600, loss 0.0014419847866520286\n",
      "iteration 72700, loss 0.0029419995844364166\n",
      "TEST accuracy at end of epoch 185: 0.9201\n",
      "TRAIN accuracy at end of epoch 185: 1.0\n",
      "                          epoch 186\n",
      "iteration 72800, loss 0.0023624582681804895\n",
      "iteration 72900, loss 0.0001769902155501768\n",
      "iteration 73000, loss 0.04759804531931877\n",
      "iteration 73100, loss 0.0002767234109342098\n",
      "TEST accuracy at end of epoch 186: 0.9195\n",
      "TRAIN accuracy at end of epoch 186: 1.0\n",
      "                          epoch 187\n",
      "iteration 73200, loss 0.004356948658823967\n",
      "iteration 73300, loss 0.0017958321841433644\n",
      "iteration 73400, loss 0.001769489492289722\n",
      "iteration 73500, loss 0.014444425702095032\n",
      "TEST accuracy at end of epoch 187: 0.9199\n",
      "TRAIN accuracy at end of epoch 187: 1.0\n",
      "                          epoch 188\n",
      "iteration 73600, loss 0.0014379514614120126\n",
      "iteration 73700, loss 9.091568063013256e-05\n",
      "iteration 73800, loss 0.0007869045366533101\n",
      "TEST accuracy at end of epoch 188: 0.9203\n",
      "TRAIN accuracy at end of epoch 188: 1.0\n",
      "                          epoch 189\n",
      "iteration 73900, loss 0.0009601099300198257\n",
      "iteration 74000, loss 0.0013771288795396686\n",
      "iteration 74100, loss 0.0007063932134769857\n",
      "iteration 74200, loss 0.0033241359051316977\n",
      "TEST accuracy at end of epoch 189: 0.9202\n",
      "TRAIN accuracy at end of epoch 189: 1.0\n",
      "                          epoch 190\n",
      "iteration 74300, loss 0.0011375648900866508\n",
      "iteration 74400, loss 8.916827937355265e-05\n",
      "iteration 74500, loss 0.00030680239433422685\n",
      "iteration 74600, loss 0.001353127066977322\n",
      "TEST accuracy at end of epoch 190: 0.92\n",
      "TRAIN accuracy at end of epoch 190: 1.0\n",
      "                          epoch 191\n",
      "iteration 74700, loss 0.0013313989620655775\n",
      "iteration 74800, loss 0.0008193389512598515\n",
      "iteration 74900, loss 0.0014613706152886152\n",
      "iteration 75000, loss 8.529290789738297e-05\n",
      "TEST accuracy at end of epoch 191: 0.9202\n",
      "TRAIN accuracy at end of epoch 191: 1.0\n",
      "                          epoch 192\n",
      "iteration 75100, loss 0.006438627373427153\n",
      "iteration 75200, loss 0.0009198469342663884\n",
      "iteration 75300, loss 0.003095731372013688\n",
      "iteration 75400, loss 0.0010900984052568674\n",
      "TEST accuracy at end of epoch 192: 0.9201\n",
      "TRAIN accuracy at end of epoch 192: 1.0\n",
      "                          epoch 193\n",
      "iteration 75500, loss 0.001601419528014958\n",
      "iteration 75600, loss 0.0002732212597038597\n",
      "iteration 75700, loss 0.0005549511988647282\n",
      "iteration 75800, loss 0.006219544913619757\n",
      "TEST accuracy at end of epoch 193: 0.9198\n",
      "TRAIN accuracy at end of epoch 193: 1.0\n",
      "                          epoch 194\n",
      "iteration 75900, loss 0.0009189785341732204\n",
      "iteration 76000, loss 0.0273151658475399\n",
      "iteration 76100, loss 0.00043173047015443444\n",
      "iteration 76200, loss 0.00017195986583828926\n",
      "TEST accuracy at end of epoch 194: 0.9197\n",
      "TRAIN accuracy at end of epoch 194: 1.0\n",
      "                          epoch 195\n",
      "iteration 76300, loss 0.0017376987962052226\n",
      "iteration 76400, loss 0.00031404709443449974\n",
      "iteration 76500, loss 0.005416371859610081\n",
      "iteration 76600, loss 0.0006124560022726655\n",
      "TEST accuracy at end of epoch 195: 0.9195\n",
      "TRAIN accuracy at end of epoch 195: 1.0\n",
      "                          epoch 196\n",
      "iteration 76700, loss 6.466676131822169e-05\n",
      "iteration 76800, loss 0.0007126035634428263\n",
      "iteration 76900, loss 0.0003001439035870135\n",
      "iteration 77000, loss 0.00243658060207963\n",
      "TEST accuracy at end of epoch 196: 0.9202\n",
      "TRAIN accuracy at end of epoch 196: 1.0\n",
      "                          epoch 197\n",
      "iteration 77100, loss 7.901350181782618e-05\n",
      "iteration 77200, loss 0.0001899629132822156\n",
      "iteration 77300, loss 0.0018649152480065823\n",
      "iteration 77400, loss 0.0011712857522070408\n",
      "TEST accuracy at end of epoch 197: 0.9199\n",
      "TRAIN accuracy at end of epoch 197: 1.0\n",
      "                          epoch 198\n",
      "iteration 77500, loss 0.0003015934198629111\n",
      "iteration 77600, loss 0.00017045850108843297\n",
      "iteration 77700, loss 0.001615507178939879\n",
      "iteration 77800, loss 0.0006207709666341543\n",
      "TEST accuracy at end of epoch 198: 0.9196\n",
      "TRAIN accuracy at end of epoch 198: 1.0\n",
      "                          epoch 199\n",
      "iteration 77900, loss 0.008471640758216381\n",
      "iteration 78000, loss 0.0020788100082427263\n",
      "iteration 78100, loss 0.005604550242424011\n",
      "iteration 78200, loss 0.0048779072239995\n",
      "TEST accuracy at end of epoch 199: 0.9197\n",
      "TRAIN accuracy at end of epoch 199: 1.0\n",
      "╰┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╯\n",
      "╭┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╮\n",
      "                         n: 5, res: True\n",
      "size of reduce_mean: (?, 1, 1, 64)\n",
      "size of squeeze: (?, 64)\n",
      "training for 78125 iterations\n",
      "                          epoch 0\n",
      "iteration 100, loss 1.8370394706726074\n",
      "iteration 200, loss 1.6799991130828857\n",
      "iteration 300, loss 1.391183614730835\n",
      "TEST accuracy at end of epoch 0: 0.2423\n",
      "TRAIN accuracy at end of epoch 0: 0.21\n",
      "                          epoch 1\n",
      "iteration 400, loss 1.201768398284912\n",
      "iteration 500, loss 1.226693034172058\n",
      "iteration 600, loss 1.0549451112747192\n",
      "iteration 700, loss 0.9890216588973999\n",
      "TEST accuracy at end of epoch 1: 0.3819\n",
      "TRAIN accuracy at end of epoch 1: 0.41\n",
      "                          epoch 2\n",
      "iteration 800, loss 0.8991003036499023\n",
      "iteration 900, loss 0.8010584115982056\n",
      "iteration 1000, loss 0.9441187381744385\n",
      "iteration 1100, loss 0.9150947332382202\n",
      "TEST accuracy at end of epoch 2: 0.6012\n",
      "TRAIN accuracy at end of epoch 2: 0.62\n",
      "                          epoch 3\n",
      "iteration 1200, loss 0.7979994416236877\n",
      "iteration 1300, loss 0.7212636470794678\n",
      "iteration 1400, loss 0.5458717942237854\n",
      "iteration 1500, loss 0.7366560697555542\n",
      "TEST accuracy at end of epoch 3: 0.6877\n",
      "TRAIN accuracy at end of epoch 3: 0.63\n",
      "                          epoch 4\n",
      "iteration 1600, loss 0.6234121322631836\n",
      "iteration 1700, loss 0.5163125991821289\n",
      "iteration 1800, loss 0.7218544483184814\n",
      "iteration 1900, loss 0.6861680746078491\n",
      "TEST accuracy at end of epoch 4: 0.7712\n",
      "TRAIN accuracy at end of epoch 4: 0.8\n",
      "                          epoch 5\n",
      "iteration 2000, loss 0.5415926575660706\n",
      "iteration 2100, loss 0.5502562522888184\n",
      "iteration 2200, loss 0.5478942394256592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2300, loss 0.5032786726951599\n",
      "TEST accuracy at end of epoch 5: 0.7753\n",
      "TRAIN accuracy at end of epoch 5: 0.81\n",
      "                          epoch 6\n",
      "iteration 2400, loss 0.4937388300895691\n",
      "iteration 2500, loss 0.5523918867111206\n",
      "iteration 2600, loss 0.6593533754348755\n",
      "iteration 2700, loss 0.435311496257782\n",
      "TEST accuracy at end of epoch 6: 0.7621\n",
      "TRAIN accuracy at end of epoch 6: 0.76\n",
      "                          epoch 7\n",
      "iteration 2800, loss 0.46962612867355347\n",
      "iteration 2900, loss 0.4659841060638428\n",
      "iteration 3000, loss 0.3801578879356384\n",
      "iteration 3100, loss 0.61115962266922\n",
      "TEST accuracy at end of epoch 7: 0.7488\n",
      "TRAIN accuracy at end of epoch 7: 0.74\n",
      "                          epoch 8\n",
      "iteration 3200, loss 0.524552583694458\n",
      "iteration 3300, loss 0.689268171787262\n",
      "iteration 3400, loss 0.39646756649017334\n",
      "iteration 3500, loss 0.4589395523071289\n",
      "TEST accuracy at end of epoch 8: 0.7535\n",
      "TRAIN accuracy at end of epoch 8: 0.82\n",
      "                          epoch 9\n",
      "iteration 3600, loss 0.37531334161758423\n",
      "iteration 3700, loss 0.45021307468414307\n",
      "iteration 3800, loss 0.33561137318611145\n",
      "iteration 3900, loss 0.2884330749511719\n",
      "TEST accuracy at end of epoch 9: 0.7942\n",
      "TRAIN accuracy at end of epoch 9: 0.84\n",
      "                          epoch 10\n",
      "iteration 4000, loss 0.4754331111907959\n",
      "iteration 4100, loss 0.37707242369651794\n",
      "iteration 4200, loss 0.35994261503219604\n",
      "iteration 4300, loss 0.4381825923919678\n",
      "TEST accuracy at end of epoch 10: 0.7892\n",
      "TRAIN accuracy at end of epoch 10: 0.75\n",
      "                          epoch 11\n",
      "iteration 4400, loss 0.39647844433784485\n",
      "iteration 4500, loss 0.38012999296188354\n",
      "iteration 4600, loss 0.3823166489601135\n",
      "TEST accuracy at end of epoch 11: 0.8161\n",
      "TRAIN accuracy at end of epoch 11: 0.83\n",
      "                          epoch 12\n",
      "iteration 4700, loss 0.46141353249549866\n",
      "iteration 4800, loss 0.4899775981903076\n",
      "iteration 4900, loss 0.2877016067504883\n",
      "iteration 5000, loss 0.409712553024292\n",
      "TEST accuracy at end of epoch 12: 0.8198\n",
      "TRAIN accuracy at end of epoch 12: 0.9\n",
      "                          epoch 13\n",
      "iteration 5100, loss 0.31825247406959534\n",
      "iteration 5200, loss 0.5073743462562561\n",
      "iteration 5300, loss 0.40590929985046387\n",
      "iteration 5400, loss 0.2818191647529602\n",
      "TEST accuracy at end of epoch 13: 0.8411\n",
      "TRAIN accuracy at end of epoch 13: 0.94\n",
      "                          epoch 14\n",
      "iteration 5500, loss 0.2790956497192383\n",
      "iteration 5600, loss 0.2915489673614502\n",
      "iteration 5700, loss 0.3602825105190277\n",
      "iteration 5800, loss 0.33039090037345886\n",
      "TEST accuracy at end of epoch 14: 0.8384\n",
      "TRAIN accuracy at end of epoch 14: 0.87\n",
      "                          epoch 15\n",
      "iteration 5900, loss 0.3854767084121704\n",
      "iteration 6000, loss 0.31616759300231934\n",
      "iteration 6100, loss 0.2972681522369385\n",
      "iteration 6200, loss 0.18950575590133667\n",
      "TEST accuracy at end of epoch 15: 0.8379\n",
      "TRAIN accuracy at end of epoch 15: 0.87\n",
      "                          epoch 16\n",
      "iteration 6300, loss 0.2214524745941162\n",
      "iteration 6400, loss 0.2888445258140564\n",
      "iteration 6500, loss 0.31397145986557007\n",
      "iteration 6600, loss 0.3275424838066101\n",
      "TEST accuracy at end of epoch 16: 0.8399\n",
      "TRAIN accuracy at end of epoch 16: 0.85\n",
      "                          epoch 17\n",
      "iteration 6700, loss 0.22520408034324646\n",
      "iteration 6800, loss 0.3224252462387085\n",
      "iteration 6900, loss 0.3021825850009918\n",
      "iteration 7000, loss 0.2808338403701782\n",
      "TEST accuracy at end of epoch 17: 0.8158\n",
      "TRAIN accuracy at end of epoch 17: 0.88\n",
      "                          epoch 18\n",
      "iteration 7100, loss 0.34855884313583374\n",
      "iteration 7200, loss 0.28126537799835205\n",
      "iteration 7300, loss 0.397224098443985\n",
      "iteration 7400, loss 0.39403247833251953\n",
      "TEST accuracy at end of epoch 18: 0.8563\n",
      "TRAIN accuracy at end of epoch 18: 0.92\n",
      "                          epoch 19\n",
      "iteration 7500, loss 0.21546779572963715\n",
      "iteration 7600, loss 0.3528187870979309\n",
      "iteration 7700, loss 0.2057945430278778\n",
      "iteration 7800, loss 0.3045867085456848\n",
      "TEST accuracy at end of epoch 19: 0.8567\n",
      "TRAIN accuracy at end of epoch 19: 0.93\n",
      "                          epoch 20\n",
      "iteration 7900, loss 0.28613850474357605\n",
      "iteration 8000, loss 0.2015349566936493\n",
      "iteration 8100, loss 0.2000570297241211\n",
      "iteration 8200, loss 0.21606463193893433\n",
      "TEST accuracy at end of epoch 20: 0.8692\n",
      "TRAIN accuracy at end of epoch 20: 0.95\n",
      "                          epoch 21\n",
      "iteration 8300, loss 0.32354652881622314\n",
      "iteration 8400, loss 0.3022949695587158\n",
      "iteration 8500, loss 0.31596505641937256\n",
      "iteration 8600, loss 0.29991614818573\n",
      "TEST accuracy at end of epoch 21: 0.843\n",
      "TRAIN accuracy at end of epoch 21: 0.87\n",
      "                          epoch 22\n",
      "iteration 8700, loss 0.19930598139762878\n",
      "iteration 8800, loss 0.19568821787834167\n",
      "iteration 8900, loss 0.22052423655986786\n",
      "TEST accuracy at end of epoch 22: 0.8536\n",
      "TRAIN accuracy at end of epoch 22: 0.87\n",
      "                          epoch 23\n",
      "iteration 9000, loss 0.2286764234304428\n",
      "iteration 9100, loss 0.16377511620521545\n",
      "iteration 9200, loss 0.2326618731021881\n",
      "iteration 9300, loss 0.3531581163406372\n",
      "TEST accuracy at end of epoch 23: 0.8295\n",
      "TRAIN accuracy at end of epoch 23: 0.87\n",
      "                          epoch 24\n",
      "iteration 9400, loss 0.19986583292484283\n",
      "iteration 9500, loss 0.2784033417701721\n",
      "iteration 9600, loss 0.27663654088974\n",
      "iteration 9700, loss 0.23923534154891968\n",
      "TEST accuracy at end of epoch 24: 0.8444\n",
      "TRAIN accuracy at end of epoch 24: 0.91\n",
      "                          epoch 25\n",
      "iteration 9800, loss 0.27654558420181274\n",
      "iteration 9900, loss 0.14004385471343994\n",
      "iteration 10000, loss 0.21935196220874786\n",
      "iteration 10100, loss 0.25799882411956787\n",
      "TEST accuracy at end of epoch 25: 0.8388\n",
      "TRAIN accuracy at end of epoch 25: 0.91\n",
      "                          epoch 26\n",
      "iteration 10200, loss 0.19048011302947998\n",
      "iteration 10300, loss 0.22653493285179138\n",
      "iteration 10400, loss 0.2862538993358612\n",
      "iteration 10500, loss 0.25375890731811523\n",
      "TEST accuracy at end of epoch 26: 0.8445\n",
      "TRAIN accuracy at end of epoch 26: 0.88\n",
      "                          epoch 27\n",
      "iteration 10600, loss 0.3362604081630707\n",
      "iteration 10700, loss 0.16489028930664062\n",
      "iteration 10800, loss 0.21565943956375122\n",
      "iteration 10900, loss 0.15606984496116638\n",
      "TEST accuracy at end of epoch 27: 0.8541\n",
      "TRAIN accuracy at end of epoch 27: 0.91\n",
      "                          epoch 28\n",
      "iteration 11000, loss 0.2110312581062317\n",
      "iteration 11100, loss 0.23670785129070282\n",
      "iteration 11200, loss 0.22850358486175537\n",
      "iteration 11300, loss 0.1738637089729309\n",
      "TEST accuracy at end of epoch 28: 0.8437\n",
      "TRAIN accuracy at end of epoch 28: 0.92\n",
      "                          epoch 29\n",
      "iteration 11400, loss 0.15528550744056702\n",
      "iteration 11500, loss 0.26032307744026184\n",
      "iteration 11600, loss 0.21627098321914673\n",
      "iteration 11700, loss 0.14592856168746948\n",
      "TEST accuracy at end of epoch 29: 0.8554\n",
      "TRAIN accuracy at end of epoch 29: 0.91\n",
      "                          epoch 30\n",
      "iteration 11800, loss 0.29456663131713867\n",
      "iteration 11900, loss 0.19646130502223969\n",
      "iteration 12000, loss 0.12289320677518845\n",
      "iteration 12100, loss 0.15501102805137634\n",
      "TEST accuracy at end of epoch 30: 0.847\n",
      "TRAIN accuracy at end of epoch 30: 0.94\n",
      "                          epoch 31\n",
      "iteration 12200, loss 0.14961442351341248\n",
      "iteration 12300, loss 0.24002206325531006\n",
      "iteration 12400, loss 0.1330876648426056\n",
      "iteration 12500, loss 0.24891918897628784\n",
      "TEST accuracy at end of epoch 31: 0.8219\n",
      "TRAIN accuracy at end of epoch 31: 0.91\n",
      "                          epoch 32\n",
      "iteration 12600, loss 0.333163857460022\n",
      "iteration 12700, loss 0.14097782969474792\n",
      "iteration 12800, loss 0.2598643898963928\n",
      "iteration 12900, loss 0.17370471358299255\n",
      "TEST accuracy at end of epoch 32: 0.8651\n",
      "TRAIN accuracy at end of epoch 32: 0.93\n",
      "                          epoch 33\n",
      "iteration 13000, loss 0.1653570532798767\n",
      "iteration 13100, loss 0.21702013909816742\n",
      "iteration 13200, loss 0.1573658287525177\n",
      "TEST accuracy at end of epoch 33: 0.848\n",
      "TRAIN accuracy at end of epoch 33: 0.9\n",
      "                          epoch 34\n",
      "iteration 13300, loss 0.15451303124427795\n",
      "iteration 13400, loss 0.22056961059570312\n",
      "iteration 13500, loss 0.18447312712669373\n",
      "iteration 13600, loss 0.1642736792564392\n",
      "TEST accuracy at end of epoch 34: 0.861\n",
      "TRAIN accuracy at end of epoch 34: 0.94\n",
      "                          epoch 35\n",
      "iteration 13700, loss 0.2523954510688782\n",
      "iteration 13800, loss 0.16398078203201294\n",
      "iteration 13900, loss 0.14558616280555725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 14000, loss 0.12923961877822876\n",
      "TEST accuracy at end of epoch 35: 0.858\n",
      "TRAIN accuracy at end of epoch 35: 0.93\n",
      "                          epoch 36\n",
      "iteration 14100, loss 0.2044396996498108\n",
      "iteration 14200, loss 0.06780799478292465\n",
      "iteration 14300, loss 0.22302551567554474\n",
      "iteration 14400, loss 0.1693088710308075\n",
      "TEST accuracy at end of epoch 36: 0.8602\n",
      "TRAIN accuracy at end of epoch 36: 0.91\n",
      "                          epoch 37\n",
      "iteration 14500, loss 0.1254201978445053\n",
      "iteration 14600, loss 0.14002138376235962\n",
      "iteration 14700, loss 0.19026872515678406\n",
      "iteration 14800, loss 0.2926125228404999\n",
      "TEST accuracy at end of epoch 37: 0.8747\n",
      "TRAIN accuracy at end of epoch 37: 0.91\n",
      "                          epoch 38\n",
      "iteration 14900, loss 0.17730233073234558\n",
      "iteration 15000, loss 0.13309362530708313\n",
      "iteration 15100, loss 0.24002638459205627\n",
      "iteration 15200, loss 0.10226090252399445\n",
      "TEST accuracy at end of epoch 38: 0.8654\n",
      "TRAIN accuracy at end of epoch 38: 0.92\n",
      "                          epoch 39\n",
      "iteration 15300, loss 0.12610211968421936\n",
      "iteration 15400, loss 0.2099532186985016\n",
      "iteration 15500, loss 0.12213939428329468\n",
      "iteration 15600, loss 0.2776632606983185\n",
      "TEST accuracy at end of epoch 39: 0.8818\n",
      "TRAIN accuracy at end of epoch 39: 0.95\n",
      "                          epoch 40\n",
      "iteration 15700, loss 0.15749311447143555\n",
      "iteration 15800, loss 0.1515812873840332\n",
      "iteration 15900, loss 0.12689144909381866\n",
      "iteration 16000, loss 0.14678001403808594\n",
      "TEST accuracy at end of epoch 40: 0.87\n",
      "TRAIN accuracy at end of epoch 40: 0.96\n",
      "                          epoch 41\n",
      "iteration 16100, loss 0.10988128185272217\n",
      "iteration 16200, loss 0.12305586040019989\n",
      "iteration 16300, loss 0.19234806299209595\n",
      "iteration 16400, loss 0.1308557093143463\n",
      "TEST accuracy at end of epoch 41: 0.8708\n",
      "TRAIN accuracy at end of epoch 41: 0.93\n",
      "                          epoch 42\n",
      "iteration 16500, loss 0.13430389761924744\n",
      "iteration 16600, loss 0.09937162697315216\n",
      "iteration 16700, loss 0.153500497341156\n",
      "iteration 16800, loss 0.287040114402771\n",
      "TEST accuracy at end of epoch 42: 0.8647\n",
      "TRAIN accuracy at end of epoch 42: 0.94\n",
      "                          epoch 43\n",
      "iteration 16900, loss 0.07103151082992554\n",
      "iteration 17000, loss 0.11919872462749481\n",
      "iteration 17100, loss 0.14975881576538086\n",
      "iteration 17200, loss 0.15112406015396118\n",
      "TEST accuracy at end of epoch 43: 0.872\n",
      "TRAIN accuracy at end of epoch 43: 0.93\n",
      "                          epoch 44\n",
      "iteration 17300, loss 0.21533459424972534\n",
      "iteration 17400, loss 0.13606423139572144\n",
      "iteration 17500, loss 0.25623059272766113\n",
      "TEST accuracy at end of epoch 44: 0.8742\n",
      "TRAIN accuracy at end of epoch 44: 0.94\n",
      "                          epoch 45\n",
      "iteration 17600, loss 0.08126933872699738\n",
      "iteration 17700, loss 0.05175585672259331\n",
      "iteration 17800, loss 0.1458297222852707\n",
      "iteration 17900, loss 0.14375491440296173\n",
      "TEST accuracy at end of epoch 45: 0.8756\n",
      "TRAIN accuracy at end of epoch 45: 0.94\n",
      "                          epoch 46\n",
      "iteration 18000, loss 0.09837515652179718\n",
      "iteration 18100, loss 0.10114884376525879\n",
      "iteration 18200, loss 0.15149950981140137\n",
      "iteration 18300, loss 0.0724606066942215\n",
      "TEST accuracy at end of epoch 46: 0.8485\n",
      "TRAIN accuracy at end of epoch 46: 0.88\n",
      "                          epoch 47\n",
      "iteration 18400, loss 0.08857697248458862\n",
      "iteration 18500, loss 0.14248356223106384\n",
      "iteration 18600, loss 0.11771243810653687\n",
      "iteration 18700, loss 0.1282123178243637\n",
      "TEST accuracy at end of epoch 47: 0.876\n",
      "TRAIN accuracy at end of epoch 47: 0.94\n",
      "                          epoch 48\n",
      "iteration 18800, loss 0.10878226161003113\n",
      "iteration 18900, loss 0.2094007432460785\n",
      "iteration 19000, loss 0.10995952039957047\n",
      "iteration 19100, loss 0.13913360238075256\n",
      "TEST accuracy at end of epoch 48: 0.8853\n",
      "TRAIN accuracy at end of epoch 48: 0.98\n",
      "                          epoch 49\n",
      "iteration 19200, loss 0.1457740217447281\n",
      "iteration 19300, loss 0.056469183415174484\n",
      "iteration 19400, loss 0.12237724661827087\n",
      "iteration 19500, loss 0.09469383955001831\n",
      "TEST accuracy at end of epoch 49: 0.8815\n",
      "TRAIN accuracy at end of epoch 49: 0.91\n",
      "                          epoch 50\n",
      "iteration 19600, loss 0.07787054777145386\n",
      "iteration 19700, loss 0.07689202576875687\n",
      "iteration 19800, loss 0.13670963048934937\n",
      "iteration 19900, loss 0.13350284099578857\n",
      "TEST accuracy at end of epoch 50: 0.8743\n",
      "TRAIN accuracy at end of epoch 50: 0.96\n",
      "                          epoch 51\n",
      "iteration 20000, loss 0.15369664132595062\n",
      "iteration 20100, loss 0.1509581208229065\n",
      "iteration 20200, loss 0.1526802033185959\n",
      "iteration 20300, loss 0.06652673333883286\n",
      "TEST accuracy at end of epoch 51: 0.8929\n",
      "TRAIN accuracy at end of epoch 51: 0.98\n",
      "                          epoch 52\n",
      "iteration 20400, loss 0.0923430323600769\n",
      "iteration 20500, loss 0.111459881067276\n",
      "iteration 20600, loss 0.0753752738237381\n",
      "iteration 20700, loss 0.14859122037887573\n",
      "TEST accuracy at end of epoch 52: 0.8881\n",
      "TRAIN accuracy at end of epoch 52: 0.99\n",
      "                          epoch 53\n",
      "iteration 20800, loss 0.13798800110816956\n",
      "iteration 20900, loss 0.08985589444637299\n",
      "iteration 21000, loss 0.1045217365026474\n",
      "iteration 21100, loss 0.08523484319448471\n",
      "TEST accuracy at end of epoch 53: 0.8863\n",
      "TRAIN accuracy at end of epoch 53: 0.97\n",
      "                          epoch 54\n",
      "iteration 21200, loss 0.11724436283111572\n",
      "iteration 21300, loss 0.20454394817352295\n",
      "iteration 21400, loss 0.08128277957439423\n",
      "iteration 21500, loss 0.1613103747367859\n",
      "TEST accuracy at end of epoch 54: 0.8759\n",
      "TRAIN accuracy at end of epoch 54: 0.92\n",
      "                          epoch 55\n",
      "iteration 21600, loss 0.10393479466438293\n",
      "iteration 21700, loss 0.10198257863521576\n",
      "iteration 21800, loss 0.15371637046337128\n",
      "TEST accuracy at end of epoch 55: 0.8779\n",
      "TRAIN accuracy at end of epoch 55: 0.97\n",
      "                          epoch 56\n",
      "iteration 21900, loss 0.07015305012464523\n",
      "iteration 22000, loss 0.036171384155750275\n",
      "iteration 22100, loss 0.061226509511470795\n",
      "iteration 22200, loss 0.06883754581212997\n",
      "TEST accuracy at end of epoch 56: 0.8871\n",
      "TRAIN accuracy at end of epoch 56: 0.96\n",
      "                          epoch 57\n",
      "iteration 22300, loss 0.11372114717960358\n",
      "iteration 22400, loss 0.05758444964885712\n",
      "iteration 22500, loss 0.06861500442028046\n",
      "iteration 22600, loss 0.05852671340107918\n",
      "TEST accuracy at end of epoch 57: 0.8867\n",
      "TRAIN accuracy at end of epoch 57: 0.97\n",
      "                          epoch 58\n",
      "iteration 22700, loss 0.1126401275396347\n",
      "iteration 22800, loss 0.16450555622577667\n",
      "iteration 22900, loss 0.05347743630409241\n",
      "iteration 23000, loss 0.08309221267700195\n",
      "TEST accuracy at end of epoch 58: 0.8876\n",
      "TRAIN accuracy at end of epoch 58: 0.98\n",
      "                          epoch 59\n",
      "iteration 23100, loss 0.038434870541095734\n",
      "iteration 23200, loss 0.0730147659778595\n",
      "iteration 23300, loss 0.06616611033678055\n",
      "iteration 23400, loss 0.08822333067655563\n",
      "TEST accuracy at end of epoch 59: 0.8819\n",
      "TRAIN accuracy at end of epoch 59: 0.99\n",
      "                          epoch 60\n",
      "iteration 23500, loss 0.042741551995277405\n",
      "iteration 23600, loss 0.04675256460905075\n",
      "iteration 23700, loss 0.031136156991124153\n",
      "iteration 23800, loss 0.12892881035804749\n",
      "TEST accuracy at end of epoch 60: 0.8878\n",
      "TRAIN accuracy at end of epoch 60: 0.96\n",
      "                          epoch 61\n",
      "iteration 23900, loss 0.04704182222485542\n",
      "iteration 24000, loss 0.05345674604177475\n",
      "iteration 24100, loss 0.04174232482910156\n",
      "iteration 24200, loss 0.09825517982244492\n",
      "TEST accuracy at end of epoch 61: 0.8829\n",
      "TRAIN accuracy at end of epoch 61: 0.98\n",
      "                          epoch 62\n",
      "iteration 24300, loss 0.12217061221599579\n",
      "iteration 24400, loss 0.1363358199596405\n",
      "iteration 24500, loss 0.07370166480541229\n",
      "iteration 24600, loss 0.13807152211666107\n",
      "TEST accuracy at end of epoch 62: 0.8919\n",
      "TRAIN accuracy at end of epoch 62: 0.99\n",
      "                          epoch 63\n",
      "iteration 24700, loss 0.0826033502817154\n",
      "iteration 24800, loss 0.11582183837890625\n",
      "iteration 24900, loss 0.062390170991420746\n",
      "iteration 25000, loss 0.11643290519714355\n",
      "TEST accuracy at end of epoch 63: 0.8619\n",
      "TRAIN accuracy at end of epoch 63: 0.93\n",
      "                          epoch 64\n",
      "iteration 25100, loss 0.051110878586769104\n",
      "iteration 25200, loss 0.13839957118034363\n",
      "iteration 25300, loss 0.06033243238925934\n",
      "iteration 25400, loss 0.09689246118068695\n",
      "TEST accuracy at end of epoch 64: 0.887\n",
      "TRAIN accuracy at end of epoch 64: 0.97\n",
      "                          epoch 65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 25500, loss 0.1660030037164688\n",
      "iteration 25600, loss 0.08452585339546204\n",
      "iteration 25700, loss 0.09394781291484833\n",
      "iteration 25800, loss 0.050407588481903076\n",
      "TEST accuracy at end of epoch 65: 0.8857\n",
      "TRAIN accuracy at end of epoch 65: 0.95\n",
      "                          epoch 66\n",
      "iteration 25900, loss 0.026325538754463196\n",
      "iteration 26000, loss 0.053486451506614685\n",
      "iteration 26100, loss 0.028118927031755447\n",
      "TEST accuracy at end of epoch 66: 0.8845\n",
      "TRAIN accuracy at end of epoch 66: 0.98\n",
      "                          epoch 67\n",
      "iteration 26200, loss 0.04364520311355591\n",
      "iteration 26300, loss 0.050713006407022476\n",
      "iteration 26400, loss 0.11843496561050415\n",
      "iteration 26500, loss 0.059024229645729065\n",
      "TEST accuracy at end of epoch 67: 0.8859\n",
      "TRAIN accuracy at end of epoch 67: 0.96\n",
      "                          epoch 68\n",
      "iteration 26600, loss 0.042461879551410675\n",
      "iteration 26700, loss 0.11011330783367157\n",
      "iteration 26800, loss 0.05025043338537216\n",
      "iteration 26900, loss 0.10730700939893723\n",
      "TEST accuracy at end of epoch 68: 0.8839\n",
      "TRAIN accuracy at end of epoch 68: 0.97\n",
      "                          epoch 69\n",
      "iteration 27000, loss 0.07476472109556198\n",
      "iteration 27100, loss 0.0462811253964901\n",
      "iteration 27200, loss 0.11669585853815079\n",
      "iteration 27300, loss 0.08703360706567764\n",
      "TEST accuracy at end of epoch 69: 0.8731\n",
      "TRAIN accuracy at end of epoch 69: 0.97\n",
      "                          epoch 70\n",
      "iteration 27400, loss 0.05115965008735657\n",
      "iteration 27500, loss 0.034721724689006805\n",
      "iteration 27600, loss 0.09163675457239151\n",
      "iteration 27700, loss 0.05261591076850891\n",
      "TEST accuracy at end of epoch 70: 0.8881\n",
      "TRAIN accuracy at end of epoch 70: 0.98\n",
      "                          epoch 71\n",
      "iteration 27800, loss 0.07748066633939743\n",
      "iteration 27900, loss 0.08973371982574463\n",
      "iteration 28000, loss 0.11145768314599991\n",
      "iteration 28100, loss 0.06862876564264297\n",
      "TEST accuracy at end of epoch 71: 0.8913\n",
      "TRAIN accuracy at end of epoch 71: 0.99\n",
      "                          epoch 72\n",
      "iteration 28200, loss 0.08806069195270538\n",
      "iteration 28300, loss 0.07644485682249069\n",
      "iteration 28400, loss 0.057910047471523285\n",
      "iteration 28500, loss 0.05186297371983528\n",
      "TEST accuracy at end of epoch 72: 0.8908\n",
      "TRAIN accuracy at end of epoch 72: 0.97\n",
      "                          epoch 73\n",
      "iteration 28600, loss 0.07277625799179077\n",
      "iteration 28700, loss 0.07436168193817139\n",
      "iteration 28800, loss 0.12737461924552917\n",
      "iteration 28900, loss 0.02211660146713257\n",
      "TEST accuracy at end of epoch 73: 0.8889\n",
      "TRAIN accuracy at end of epoch 73: 0.95\n",
      "                          epoch 74\n",
      "iteration 29000, loss 0.030386779457330704\n",
      "iteration 29100, loss 0.09672990441322327\n",
      "iteration 29200, loss 0.02993970364332199\n",
      "iteration 29300, loss 0.08552971482276917\n",
      "TEST accuracy at end of epoch 74: 0.8939\n",
      "TRAIN accuracy at end of epoch 74: 0.99\n",
      "                          epoch 75\n",
      "iteration 29400, loss 0.08211655914783478\n",
      "iteration 29500, loss 0.06530981510877609\n",
      "iteration 29600, loss 0.028710758313536644\n",
      "iteration 29700, loss 0.03696758300065994\n",
      "TEST accuracy at end of epoch 75: 0.893\n",
      "TRAIN accuracy at end of epoch 75: 0.97\n",
      "                          epoch 76\n",
      "iteration 29800, loss 0.06158462539315224\n",
      "iteration 29900, loss 0.027092643082141876\n",
      "iteration 30000, loss 0.021204836666584015\n",
      "iteration 30100, loss 0.057752758264541626\n",
      "TEST accuracy at end of epoch 76: 0.8981\n",
      "TRAIN accuracy at end of epoch 76: 1.0\n",
      "                          epoch 77\n",
      "iteration 30200, loss 0.06714241951704025\n",
      "iteration 30300, loss 0.031674716621637344\n",
      "iteration 30400, loss 0.10037185251712799\n",
      "TEST accuracy at end of epoch 77: 0.8951\n",
      "TRAIN accuracy at end of epoch 77: 0.99\n",
      "                          epoch 78\n",
      "iteration 30500, loss 0.06427586823701859\n",
      "iteration 30600, loss 0.00966764148324728\n",
      "iteration 30700, loss 0.05005514621734619\n",
      "iteration 30800, loss 0.03229496255517006\n",
      "TEST accuracy at end of epoch 78: 0.8976\n",
      "TRAIN accuracy at end of epoch 78: 0.98\n",
      "                          epoch 79\n",
      "iteration 30900, loss 0.01261029951274395\n",
      "iteration 31000, loss 0.013333218172192574\n",
      "iteration 31100, loss 0.06810560822486877\n",
      "iteration 31200, loss 0.18400445580482483\n",
      "TEST accuracy at end of epoch 79: 0.8914\n",
      "TRAIN accuracy at end of epoch 79: 0.98\n",
      "                          epoch 80\n",
      "iteration 31300, loss 0.0497988760471344\n",
      "iteration 31400, loss 0.07027444243431091\n",
      "iteration 31500, loss 0.030026160180568695\n",
      "iteration 31600, loss 0.018455244600772858\n",
      "TEST accuracy at end of epoch 80: 0.8955\n",
      "TRAIN accuracy at end of epoch 80: 1.0\n",
      "                          epoch 81\n",
      "iteration 31700, loss 0.027436215430498123\n",
      "iteration 31800, loss 0.056517310440540314\n",
      "iteration 31900, loss 0.03974524512887001\n",
      "iteration 32000, loss 0.06605508923530579\n",
      "TEST accuracy at end of epoch 81: 0.8948\n",
      "TRAIN accuracy at end of epoch 81: 0.99\n",
      "                          epoch 82\n",
      "iteration 32100, loss 0.052325982600450516\n",
      "iteration 32200, loss 0.03084268420934677\n",
      "iteration 32300, loss 0.043960802257061005\n",
      "iteration 32400, loss 0.01909327507019043\n",
      "TEST accuracy at end of epoch 82: 0.8953\n",
      "TRAIN accuracy at end of epoch 82: 0.97\n",
      "                          epoch 83\n",
      "iteration 32500, loss 0.02376363053917885\n",
      "iteration 32600, loss 0.02066230960190296\n",
      "iteration 32700, loss 0.09889648854732513\n",
      "iteration 32800, loss 0.023594500496983528\n",
      "TEST accuracy at end of epoch 83: 0.8983\n",
      "TRAIN accuracy at end of epoch 83: 0.99\n",
      "                          epoch 84\n",
      "iteration 32900, loss 0.04905058816075325\n",
      "iteration 33000, loss 0.08558665215969086\n",
      "iteration 33100, loss 0.019952699542045593\n",
      "iteration 33200, loss 0.019681088626384735\n",
      "TEST accuracy at end of epoch 84: 0.8898\n",
      "TRAIN accuracy at end of epoch 84: 1.0\n",
      "                          epoch 85\n",
      "iteration 33300, loss 0.08641830831766129\n",
      "iteration 33400, loss 0.022801512852311134\n",
      "iteration 33500, loss 0.05966167151927948\n",
      "iteration 33600, loss 0.045150041580200195\n",
      "TEST accuracy at end of epoch 85: 0.8889\n",
      "TRAIN accuracy at end of epoch 85: 0.99\n",
      "                          epoch 86\n",
      "iteration 33700, loss 0.07783938944339752\n",
      "iteration 33800, loss 0.07433061301708221\n",
      "iteration 33900, loss 0.033001869916915894\n",
      "iteration 34000, loss 0.047865867614746094\n",
      "TEST accuracy at end of epoch 86: 0.9002\n",
      "TRAIN accuracy at end of epoch 86: 0.99\n",
      "                          epoch 87\n",
      "iteration 34100, loss 0.1255098432302475\n",
      "iteration 34200, loss 0.04743540659546852\n",
      "iteration 34300, loss 0.0393499881029129\n",
      "iteration 34400, loss 0.02908775582909584\n",
      "TEST accuracy at end of epoch 87: 0.8907\n",
      "TRAIN accuracy at end of epoch 87: 0.97\n",
      "                          epoch 88\n",
      "iteration 34500, loss 0.06451748311519623\n",
      "iteration 34600, loss 0.039281778037548065\n",
      "iteration 34700, loss 0.06305468827486038\n",
      "TEST accuracy at end of epoch 88: 0.9044\n",
      "TRAIN accuracy at end of epoch 88: 0.99\n",
      "                          epoch 89\n",
      "iteration 34800, loss 0.008921124041080475\n",
      "iteration 34900, loss 0.03682690113782883\n",
      "iteration 35000, loss 0.04899780824780464\n",
      "iteration 35100, loss 0.04778219014406204\n",
      "TEST accuracy at end of epoch 89: 0.8996\n",
      "TRAIN accuracy at end of epoch 89: 0.98\n",
      "                          epoch 90\n",
      "iteration 35200, loss 0.04584060609340668\n",
      "iteration 35300, loss 0.08475780487060547\n",
      "iteration 35400, loss 0.007337227463722229\n",
      "iteration 35500, loss 0.08801579475402832\n",
      "TEST accuracy at end of epoch 90: 0.895\n",
      "TRAIN accuracy at end of epoch 90: 0.97\n",
      "                          epoch 91\n",
      "iteration 35600, loss 0.01679368130862713\n",
      "iteration 35700, loss 0.012846704572439194\n",
      "iteration 35800, loss 0.05762114375829697\n",
      "iteration 35900, loss 0.045347053557634354\n",
      "TEST accuracy at end of epoch 91: 0.8828\n",
      "TRAIN accuracy at end of epoch 91: 0.95\n",
      "                          epoch 92\n",
      "iteration 36000, loss 0.01832869090139866\n",
      "iteration 36100, loss 0.016064675524830818\n",
      "iteration 36200, loss 0.0424126535654068\n",
      "iteration 36300, loss 0.017187967896461487\n",
      "TEST accuracy at end of epoch 92: 0.9013\n",
      "TRAIN accuracy at end of epoch 92: 1.0\n",
      "                          epoch 93\n",
      "iteration 36400, loss 0.017703764140605927\n",
      "iteration 36500, loss 0.021180815994739532\n",
      "iteration 36600, loss 0.028412630781531334\n",
      "iteration 36700, loss 0.048507772386074066\n",
      "TEST accuracy at end of epoch 93: 0.9033\n",
      "TRAIN accuracy at end of epoch 93: 0.99\n",
      "                          epoch 94\n",
      "iteration 36800, loss 0.010861609131097794\n",
      "iteration 36900, loss 0.08413957059383392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 37000, loss 0.006335936952382326\n",
      "iteration 37100, loss 0.020954739302396774\n",
      "TEST accuracy at end of epoch 94: 0.8951\n",
      "TRAIN accuracy at end of epoch 94: 0.98\n",
      "                          epoch 95\n",
      "iteration 37200, loss 0.011652352288365364\n",
      "iteration 37300, loss 0.08159583806991577\n",
      "iteration 37400, loss 0.04627733305096626\n",
      "iteration 37500, loss 0.08277072012424469\n",
      "TEST accuracy at end of epoch 95: 0.8971\n",
      "TRAIN accuracy at end of epoch 95: 1.0\n",
      "                          epoch 96\n",
      "iteration 37600, loss 0.09366637468338013\n",
      "iteration 37700, loss 0.03246065229177475\n",
      "iteration 37800, loss 0.03910012170672417\n",
      "iteration 37900, loss 0.056603334844112396\n",
      "TEST accuracy at end of epoch 96: 0.8888\n",
      "TRAIN accuracy at end of epoch 96: 0.98\n",
      "                          epoch 97\n",
      "iteration 38000, loss 0.036202289164066315\n",
      "iteration 38100, loss 0.009359408169984818\n",
      "iteration 38200, loss 0.12441647797822952\n",
      "iteration 38300, loss 0.033383291214704514\n",
      "TEST accuracy at end of epoch 97: 0.9026\n",
      "TRAIN accuracy at end of epoch 97: 1.0\n",
      "                          epoch 98\n",
      "iteration 38400, loss 0.029630620032548904\n",
      "iteration 38500, loss 0.05376853793859482\n",
      "iteration 38600, loss 0.053338225930929184\n",
      "iteration 38700, loss 0.034699760377407074\n",
      "TEST accuracy at end of epoch 98: 0.8916\n",
      "TRAIN accuracy at end of epoch 98: 0.97\n",
      "                          epoch 99\n",
      "iteration 38800, loss 0.03251464664936066\n",
      "iteration 38900, loss 0.025798091664910316\n",
      "iteration 39000, loss 0.022635649889707565\n",
      "iteration 39100, loss 0.07253153622150421\n",
      "TEST accuracy at end of epoch 99: 0.8994\n",
      "TRAIN accuracy at end of epoch 99: 0.98\n",
      "                          epoch 100\n",
      "iteration 39200, loss 0.03771553561091423\n",
      "iteration 39300, loss 0.0034592580050230026\n",
      "iteration 39400, loss 0.014996212907135487\n",
      "TEST accuracy at end of epoch 100: 0.9044\n",
      "TRAIN accuracy at end of epoch 100: 0.99\n",
      "                          epoch 101\n",
      "iteration 39500, loss 0.04759099334478378\n",
      "iteration 39600, loss 0.011211825534701347\n",
      "iteration 39700, loss 0.037429798394441605\n",
      "iteration 39800, loss 0.052038975059986115\n",
      "TEST accuracy at end of epoch 101: 0.9006\n",
      "TRAIN accuracy at end of epoch 101: 0.99\n",
      "                          epoch 102\n",
      "iteration 39900, loss 0.017310412600636482\n",
      "iteration 40000, loss 0.026270683854818344\n",
      "iteration 40100, loss 0.0625925213098526\n",
      "iteration 40200, loss 0.017374996095895767\n",
      "TEST accuracy at end of epoch 102: 0.9053\n",
      "TRAIN accuracy at end of epoch 102: 1.0\n",
      "                          epoch 103\n",
      "iteration 40300, loss 0.051256656646728516\n",
      "iteration 40400, loss 0.016767041757702827\n",
      "iteration 40500, loss 0.10686363279819489\n",
      "iteration 40600, loss 0.008086592890322208\n",
      "TEST accuracy at end of epoch 103: 0.8958\n",
      "TRAIN accuracy at end of epoch 103: 0.98\n",
      "                          epoch 104\n",
      "iteration 40700, loss 0.02762748673558235\n",
      "iteration 40800, loss 0.023923039436340332\n",
      "iteration 40900, loss 0.007306382525712252\n",
      "iteration 41000, loss 0.015303028747439384\n",
      "TEST accuracy at end of epoch 104: 0.8919\n",
      "TRAIN accuracy at end of epoch 104: 0.98\n",
      "                          epoch 105\n",
      "iteration 41100, loss 0.022665541619062424\n",
      "iteration 41200, loss 0.01865369640290737\n",
      "iteration 41300, loss 0.03163883090019226\n",
      "iteration 41400, loss 0.016782701015472412\n",
      "TEST accuracy at end of epoch 105: 0.9021\n",
      "TRAIN accuracy at end of epoch 105: 0.99\n",
      "                          epoch 106\n",
      "iteration 41500, loss 0.02131826989352703\n",
      "iteration 41600, loss 0.01350327581167221\n",
      "iteration 41700, loss 0.009225491434335709\n",
      "iteration 41800, loss 0.025531142950057983\n",
      "TEST accuracy at end of epoch 106: 0.9063\n",
      "TRAIN accuracy at end of epoch 106: 1.0\n",
      "                          epoch 107\n",
      "iteration 41900, loss 0.0767686665058136\n",
      "iteration 42000, loss 0.006686769425868988\n",
      "iteration 42100, loss 0.019210539758205414\n",
      "iteration 42200, loss 0.0019860679749399424\n",
      "TEST accuracy at end of epoch 107: 0.9027\n",
      "TRAIN accuracy at end of epoch 107: 0.99\n",
      "                          epoch 108\n",
      "iteration 42300, loss 0.01836315356194973\n",
      "iteration 42400, loss 0.014394894242286682\n",
      "iteration 42500, loss 0.013241834007203579\n",
      "iteration 42600, loss 0.07361365854740143\n",
      "TEST accuracy at end of epoch 108: 0.8963\n",
      "TRAIN accuracy at end of epoch 108: 1.0\n",
      "                          epoch 109\n",
      "iteration 42700, loss 0.03826567530632019\n",
      "iteration 42800, loss 0.06017690151929855\n",
      "iteration 42900, loss 0.03174181655049324\n",
      "iteration 43000, loss 0.04242008551955223\n",
      "TEST accuracy at end of epoch 109: 0.9053\n",
      "TRAIN accuracy at end of epoch 109: 0.99\n",
      "                          epoch 110\n",
      "iteration 43100, loss 0.015979811549186707\n",
      "iteration 43200, loss 0.019756892696022987\n",
      "iteration 43300, loss 0.0025229244492948055\n",
      "iteration 43400, loss 0.03609270602464676\n",
      "TEST accuracy at end of epoch 110: 0.904\n",
      "TRAIN accuracy at end of epoch 110: 0.98\n",
      "                          epoch 111\n",
      "iteration 43500, loss 0.01741659641265869\n",
      "iteration 43600, loss 0.010299024172127247\n",
      "iteration 43700, loss 0.026130570098757744\n",
      "TEST accuracy at end of epoch 111: 0.905\n",
      "TRAIN accuracy at end of epoch 111: 1.0\n",
      "                          epoch 112\n",
      "iteration 43800, loss 0.0038942345418035984\n",
      "iteration 43900, loss 0.02230989933013916\n",
      "iteration 44000, loss 0.06314661353826523\n",
      "iteration 44100, loss 0.055638328194618225\n",
      "TEST accuracy at end of epoch 112: 0.9064\n",
      "TRAIN accuracy at end of epoch 112: 0.98\n",
      "                          epoch 113\n",
      "iteration 44200, loss 0.0012775980867445469\n",
      "iteration 44300, loss 0.034650012850761414\n",
      "iteration 44400, loss 0.004806569777429104\n",
      "iteration 44500, loss 0.0032366979867219925\n",
      "TEST accuracy at end of epoch 113: 0.9076\n",
      "TRAIN accuracy at end of epoch 113: 1.0\n",
      "                          epoch 114\n",
      "iteration 44600, loss 0.023680666461586952\n",
      "iteration 44700, loss 0.03193486109375954\n",
      "iteration 44800, loss 0.02081412822008133\n",
      "iteration 44900, loss 0.003735404461622238\n",
      "TEST accuracy at end of epoch 114: 0.9021\n",
      "TRAIN accuracy at end of epoch 114: 1.0\n",
      "                          epoch 115\n",
      "iteration 45000, loss 0.019776109606027603\n",
      "iteration 45100, loss 0.02443753369152546\n",
      "iteration 45200, loss 0.013256335631012917\n",
      "iteration 45300, loss 0.029866572469472885\n",
      "TEST accuracy at end of epoch 115: 0.9061\n",
      "TRAIN accuracy at end of epoch 115: 1.0\n",
      "                          epoch 116\n",
      "iteration 45400, loss 0.03656232729554176\n",
      "iteration 45500, loss 0.016283124685287476\n",
      "iteration 45600, loss 0.009027035906910896\n",
      "iteration 45700, loss 0.02936691977083683\n",
      "TEST accuracy at end of epoch 116: 0.9038\n",
      "TRAIN accuracy at end of epoch 116: 0.99\n",
      "                          epoch 117\n",
      "iteration 45800, loss 0.022414376959204674\n",
      "iteration 45900, loss 0.007504066918045282\n",
      "iteration 46000, loss 0.010231560096144676\n",
      "iteration 46100, loss 0.002407235559076071\n",
      "TEST accuracy at end of epoch 117: 0.9119\n",
      "TRAIN accuracy at end of epoch 117: 1.0\n",
      "                          epoch 118\n",
      "iteration 46200, loss 0.0035420143976807594\n",
      "iteration 46300, loss 0.054334498941898346\n",
      "iteration 46400, loss 0.013033530674874783\n",
      "iteration 46500, loss 0.008285433985292912\n",
      "TEST accuracy at end of epoch 118: 0.9081\n",
      "TRAIN accuracy at end of epoch 118: 1.0\n",
      "                          epoch 119\n",
      "iteration 46600, loss 0.03094112128019333\n",
      "iteration 46700, loss 0.014556759968400002\n",
      "iteration 46800, loss 0.01937197893857956\n",
      "iteration 46900, loss 0.041255198419094086\n",
      "TEST accuracy at end of epoch 119: 0.9067\n",
      "TRAIN accuracy at end of epoch 119: 0.99\n",
      "                          epoch 120\n",
      "iteration 47000, loss 0.02130012772977352\n",
      "iteration 47100, loss 0.030043961480259895\n",
      "iteration 47200, loss 0.012200483120977879\n",
      "iteration 47300, loss 0.015746619552373886\n",
      "TEST accuracy at end of epoch 120: 0.911\n",
      "TRAIN accuracy at end of epoch 120: 1.0\n",
      "                          epoch 121\n",
      "iteration 47400, loss 0.01210678182542324\n",
      "iteration 47500, loss 0.03521067276597023\n",
      "iteration 47600, loss 0.05364565923810005\n",
      "iteration 47700, loss 0.02183057740330696\n",
      "TEST accuracy at end of epoch 121: 0.9079\n",
      "TRAIN accuracy at end of epoch 121: 0.98\n",
      "                          epoch 122\n",
      "iteration 47800, loss 0.00843333825469017\n",
      "iteration 47900, loss 0.009128084406256676\n",
      "iteration 48000, loss 0.05694109573960304\n",
      "TEST accuracy at end of epoch 122: 0.9128\n",
      "TRAIN accuracy at end of epoch 122: 1.0\n",
      "                          epoch 123\n",
      "iteration 48100, loss 0.034196048974990845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 48200, loss 0.022136123850941658\n",
      "iteration 48300, loss 0.003599633928388357\n",
      "iteration 48400, loss 0.02737482450902462\n",
      "TEST accuracy at end of epoch 123: 0.909\n",
      "TRAIN accuracy at end of epoch 123: 1.0\n",
      "                          epoch 124\n",
      "iteration 48500, loss 0.002722404431551695\n",
      "iteration 48600, loss 0.00337978289462626\n",
      "iteration 48700, loss 0.012104157358407974\n",
      "iteration 48800, loss 0.01447199285030365\n",
      "TEST accuracy at end of epoch 124: 0.9106\n",
      "TRAIN accuracy at end of epoch 124: 1.0\n",
      "                          epoch 125\n",
      "iteration 48900, loss 0.01278725080192089\n",
      "iteration 49000, loss 0.00922355242073536\n",
      "iteration 49100, loss 0.008554551750421524\n",
      "iteration 49200, loss 0.0375342033803463\n",
      "TEST accuracy at end of epoch 125: 0.9091\n",
      "TRAIN accuracy at end of epoch 125: 1.0\n",
      "                          epoch 126\n",
      "iteration 49300, loss 0.017033923417329788\n",
      "iteration 49400, loss 0.00900314375758171\n",
      "iteration 49500, loss 0.03546582907438278\n",
      "iteration 49600, loss 0.008299638517200947\n",
      "TEST accuracy at end of epoch 126: 0.9113\n",
      "TRAIN accuracy at end of epoch 126: 1.0\n",
      "                          epoch 127\n",
      "iteration 49700, loss 0.001712961820885539\n",
      "iteration 49800, loss 0.005423468071967363\n",
      "iteration 49900, loss 0.007232253439724445\n",
      "iteration 50000, loss 0.009345460683107376\n",
      "TEST accuracy at end of epoch 127: 0.9074\n",
      "TRAIN accuracy at end of epoch 127: 1.0\n",
      "                          epoch 128\n",
      "iteration 50100, loss 0.023569656535983086\n",
      "iteration 50200, loss 0.001098819775506854\n",
      "iteration 50300, loss 0.028903495520353317\n",
      "iteration 50400, loss 0.003934606444090605\n",
      "TEST accuracy at end of epoch 128: 0.911\n",
      "TRAIN accuracy at end of epoch 128: 1.0\n",
      "                          epoch 129\n",
      "iteration 50500, loss 0.0017113653011620045\n",
      "iteration 50600, loss 0.004719443619251251\n",
      "iteration 50700, loss 0.028414085507392883\n",
      "iteration 50800, loss 0.008004747331142426\n",
      "TEST accuracy at end of epoch 129: 0.9122\n",
      "TRAIN accuracy at end of epoch 129: 1.0\n",
      "                          epoch 130\n",
      "iteration 50900, loss 0.015557734295725822\n",
      "iteration 51000, loss 0.004590030759572983\n",
      "iteration 51100, loss 0.002010738244280219\n",
      "iteration 51200, loss 0.006267691031098366\n",
      "TEST accuracy at end of epoch 130: 0.911\n",
      "TRAIN accuracy at end of epoch 130: 1.0\n",
      "                          epoch 131\n",
      "iteration 51300, loss 0.04178749769926071\n",
      "iteration 51400, loss 0.03789259120821953\n",
      "iteration 51500, loss 0.0007750459481030703\n",
      "iteration 51600, loss 0.012998076155781746\n",
      "TEST accuracy at end of epoch 131: 0.9132\n",
      "TRAIN accuracy at end of epoch 131: 1.0\n",
      "                          epoch 132\n",
      "iteration 51700, loss 0.0019718175753951073\n",
      "iteration 51800, loss 0.002094007097184658\n",
      "iteration 51900, loss 0.0006414065137505531\n",
      "iteration 52000, loss 0.002209659665822983\n",
      "TEST accuracy at end of epoch 132: 0.9121\n",
      "TRAIN accuracy at end of epoch 132: 1.0\n",
      "                          epoch 133\n",
      "iteration 52100, loss 0.046703122556209564\n",
      "iteration 52200, loss 0.009115923196077347\n",
      "iteration 52300, loss 0.018759116530418396\n",
      "TEST accuracy at end of epoch 133: 0.9099\n",
      "TRAIN accuracy at end of epoch 133: 1.0\n",
      "                          epoch 134\n",
      "iteration 52400, loss 0.003101312555372715\n",
      "iteration 52500, loss 0.008473392575979233\n",
      "iteration 52600, loss 0.027587953954935074\n",
      "iteration 52700, loss 0.0017215313855558634\n",
      "TEST accuracy at end of epoch 134: 0.9113\n",
      "TRAIN accuracy at end of epoch 134: 1.0\n",
      "                          epoch 135\n",
      "iteration 52800, loss 0.0025261312257498503\n",
      "iteration 52900, loss 0.003079584101215005\n",
      "iteration 53000, loss 0.002086897613480687\n",
      "iteration 53100, loss 0.01998765394091606\n",
      "TEST accuracy at end of epoch 135: 0.91\n",
      "TRAIN accuracy at end of epoch 135: 1.0\n",
      "                          epoch 136\n",
      "iteration 53200, loss 0.013327553868293762\n",
      "iteration 53300, loss 0.00532738771289587\n",
      "iteration 53400, loss 0.0013606613501906395\n",
      "iteration 53500, loss 0.008590888231992722\n",
      "TEST accuracy at end of epoch 136: 0.9101\n",
      "TRAIN accuracy at end of epoch 136: 1.0\n",
      "                          epoch 137\n",
      "iteration 53600, loss 0.004451039247214794\n",
      "iteration 53700, loss 0.0037538795731961727\n",
      "iteration 53800, loss 0.0028695205692201853\n",
      "iteration 53900, loss 0.001535863266326487\n",
      "TEST accuracy at end of epoch 137: 0.9113\n",
      "TRAIN accuracy at end of epoch 137: 1.0\n",
      "                          epoch 138\n",
      "iteration 54000, loss 0.08895651996135712\n",
      "iteration 54100, loss 0.007722256705164909\n",
      "iteration 54200, loss 0.005617384798824787\n",
      "iteration 54300, loss 0.0026234108954668045\n",
      "TEST accuracy at end of epoch 138: 0.9111\n",
      "TRAIN accuracy at end of epoch 138: 1.0\n",
      "                          epoch 139\n",
      "iteration 54400, loss 0.0016442310297861695\n",
      "iteration 54500, loss 0.005907122045755386\n",
      "iteration 54600, loss 0.002254435094073415\n",
      "iteration 54700, loss 0.009067254140973091\n",
      "TEST accuracy at end of epoch 139: 0.909\n",
      "TRAIN accuracy at end of epoch 139: 1.0\n",
      "                          epoch 140\n",
      "iteration 54800, loss 0.002170365769416094\n",
      "iteration 54900, loss 0.010631721466779709\n",
      "iteration 55000, loss 0.010020775720477104\n",
      "iteration 55100, loss 0.008338208310306072\n",
      "TEST accuracy at end of epoch 140: 0.9119\n",
      "TRAIN accuracy at end of epoch 140: 1.0\n",
      "                          epoch 141\n",
      "iteration 55200, loss 0.0019798842258751392\n",
      "iteration 55300, loss 0.0035008806735277176\n",
      "iteration 55400, loss 0.029751673340797424\n",
      "iteration 55500, loss 0.0006457857089117169\n",
      "TEST accuracy at end of epoch 141: 0.909\n",
      "TRAIN accuracy at end of epoch 141: 1.0\n",
      "                          epoch 142\n",
      "iteration 55600, loss 0.002119162119925022\n",
      "iteration 55700, loss 0.0009846690809354186\n",
      "iteration 55800, loss 0.023593202233314514\n",
      "iteration 55900, loss 0.026316728442907333\n",
      "TEST accuracy at end of epoch 142: 0.9122\n",
      "TRAIN accuracy at end of epoch 142: 1.0\n",
      "                          epoch 143\n",
      "iteration 56000, loss 0.0009229441056959331\n",
      "iteration 56100, loss 0.003520803991705179\n",
      "iteration 56200, loss 0.039408013224601746\n",
      "iteration 56300, loss 0.011665770784020424\n",
      "TEST accuracy at end of epoch 143: 0.9119\n",
      "TRAIN accuracy at end of epoch 143: 1.0\n",
      "                          epoch 144\n",
      "iteration 56400, loss 0.0023793140426278114\n",
      "iteration 56500, loss 0.0021524077747017145\n",
      "iteration 56600, loss 0.007385921664535999\n",
      "TEST accuracy at end of epoch 144: 0.9122\n",
      "TRAIN accuracy at end of epoch 144: 1.0\n",
      "                          epoch 145\n",
      "iteration 56700, loss 0.005008689593523741\n",
      "iteration 56800, loss 0.009667257778346539\n",
      "iteration 56900, loss 0.0019450391409918666\n",
      "iteration 57000, loss 0.007767650298774242\n",
      "TEST accuracy at end of epoch 145: 0.912\n",
      "TRAIN accuracy at end of epoch 145: 1.0\n",
      "                          epoch 146\n",
      "iteration 57100, loss 0.0028435224667191505\n",
      "iteration 57200, loss 0.00628309091553092\n",
      "iteration 57300, loss 0.0004427586100064218\n",
      "iteration 57400, loss 0.00900913868099451\n",
      "TEST accuracy at end of epoch 146: 0.9113\n",
      "TRAIN accuracy at end of epoch 146: 1.0\n",
      "                          epoch 147\n",
      "iteration 57500, loss 0.005778755526989698\n",
      "iteration 57600, loss 0.005183897912502289\n",
      "iteration 57700, loss 0.005566089414060116\n",
      "iteration 57800, loss 0.0037307506427168846\n",
      "TEST accuracy at end of epoch 147: 0.9112\n",
      "TRAIN accuracy at end of epoch 147: 1.0\n",
      "                          epoch 148\n",
      "iteration 57900, loss 0.0072231050580739975\n",
      "iteration 58000, loss 0.004736563190817833\n",
      "iteration 58100, loss 0.004336485639214516\n",
      "iteration 58200, loss 0.007980989292263985\n",
      "TEST accuracy at end of epoch 148: 0.9126\n",
      "TRAIN accuracy at end of epoch 148: 1.0\n",
      "                          epoch 149\n",
      "iteration 58300, loss 0.0028491769917309284\n",
      "iteration 58400, loss 0.026406200602650642\n",
      "iteration 58500, loss 0.014571295119822025\n",
      "iteration 58600, loss 0.020788123831152916\n",
      "TEST accuracy at end of epoch 149: 0.9126\n",
      "TRAIN accuracy at end of epoch 149: 1.0\n",
      "                          epoch 150\n",
      "iteration 58700, loss 0.01054510846734047\n",
      "iteration 58800, loss 0.028180928900837898\n",
      "iteration 58900, loss 0.013932162895798683\n",
      "iteration 59000, loss 0.0016112211160361767\n",
      "TEST accuracy at end of epoch 150: 0.9126\n",
      "TRAIN accuracy at end of epoch 150: 1.0\n",
      "                          epoch 151\n",
      "iteration 59100, loss 0.007570505142211914\n",
      "iteration 59200, loss 0.0045166658237576485\n",
      "iteration 59300, loss 0.002621202729642391\n",
      "iteration 59400, loss 0.00043706202995963395\n",
      "TEST accuracy at end of epoch 151: 0.9132\n",
      "TRAIN accuracy at end of epoch 151: 1.0\n",
      "                          epoch 152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 59500, loss 0.002664881758391857\n",
      "iteration 59600, loss 0.009907970204949379\n",
      "iteration 59700, loss 0.002156447619199753\n",
      "iteration 59800, loss 0.003460875479504466\n",
      "TEST accuracy at end of epoch 152: 0.9113\n",
      "TRAIN accuracy at end of epoch 152: 1.0\n",
      "                          epoch 153\n",
      "iteration 59900, loss 0.000982505502179265\n",
      "iteration 60000, loss 0.0014087226008996367\n",
      "iteration 60100, loss 0.002863312605768442\n",
      "iteration 60200, loss 0.0012375152437016368\n",
      "TEST accuracy at end of epoch 153: 0.9123\n",
      "TRAIN accuracy at end of epoch 153: 1.0\n",
      "                          epoch 154\n",
      "iteration 60300, loss 0.03135671839118004\n",
      "iteration 60400, loss 0.0021706041879951954\n",
      "iteration 60500, loss 0.012142681516706944\n",
      "iteration 60600, loss 0.005360931623727083\n",
      "TEST accuracy at end of epoch 154: 0.9125\n",
      "TRAIN accuracy at end of epoch 154: 1.0\n",
      "                          epoch 155\n",
      "iteration 60700, loss 0.003315098350867629\n",
      "iteration 60800, loss 0.014830770902335644\n",
      "iteration 60900, loss 0.009417609311640263\n",
      "TEST accuracy at end of epoch 155: 0.9124\n",
      "TRAIN accuracy at end of epoch 155: 1.0\n",
      "                          epoch 156\n",
      "iteration 61000, loss 0.002679135650396347\n",
      "iteration 61100, loss 0.0180820282548666\n",
      "iteration 61200, loss 0.01634429581463337\n",
      "iteration 61300, loss 0.0011148875346407294\n",
      "TEST accuracy at end of epoch 156: 0.9123\n",
      "TRAIN accuracy at end of epoch 156: 1.0\n",
      "                          epoch 157\n",
      "iteration 61400, loss 0.000806786643806845\n",
      "iteration 61500, loss 0.0006315576029010117\n",
      "iteration 61600, loss 0.002887688111513853\n",
      "iteration 61700, loss 0.0005830676527693868\n",
      "TEST accuracy at end of epoch 157: 0.914\n",
      "TRAIN accuracy at end of epoch 157: 1.0\n",
      "                          epoch 158\n",
      "iteration 61800, loss 0.0019619285594671965\n",
      "iteration 61900, loss 0.003604989033192396\n",
      "iteration 62000, loss 0.0061131613329052925\n",
      "iteration 62100, loss 0.0008471837500110269\n",
      "TEST accuracy at end of epoch 158: 0.9132\n",
      "TRAIN accuracy at end of epoch 158: 1.0\n",
      "                          epoch 159\n",
      "iteration 62200, loss 0.024664411321282387\n",
      "iteration 62300, loss 0.0021097788121551275\n",
      "iteration 62400, loss 0.0004355700802989304\n",
      "iteration 62500, loss 0.019705763086676598\n",
      "TEST accuracy at end of epoch 159: 0.9139\n",
      "TRAIN accuracy at end of epoch 159: 1.0\n",
      "                          epoch 160\n",
      "iteration 62600, loss 0.003984440583735704\n",
      "iteration 62700, loss 0.0012574915308505297\n",
      "iteration 62800, loss 0.001491429517045617\n",
      "iteration 62900, loss 0.002808080054819584\n",
      "TEST accuracy at end of epoch 160: 0.9133\n",
      "TRAIN accuracy at end of epoch 160: 1.0\n",
      "                          epoch 161\n",
      "iteration 63000, loss 0.00033850991167128086\n",
      "iteration 63100, loss 0.009906407445669174\n",
      "iteration 63200, loss 0.006918489001691341\n",
      "iteration 63300, loss 0.0023268398363143206\n",
      "TEST accuracy at end of epoch 161: 0.9134\n",
      "TRAIN accuracy at end of epoch 161: 1.0\n",
      "                          epoch 162\n",
      "iteration 63400, loss 0.010701799765229225\n",
      "iteration 63500, loss 0.0007971168379299343\n",
      "iteration 63600, loss 0.011173414997756481\n",
      "iteration 63700, loss 0.006530189421027899\n",
      "TEST accuracy at end of epoch 162: 0.9122\n",
      "TRAIN accuracy at end of epoch 162: 1.0\n",
      "                          epoch 163\n",
      "iteration 63800, loss 0.0016475138254463673\n",
      "iteration 63900, loss 0.0077607412822544575\n",
      "iteration 64000, loss 0.0018325375858694315\n",
      "iteration 64100, loss 0.014928214251995087\n",
      "TEST accuracy at end of epoch 163: 0.9132\n",
      "TRAIN accuracy at end of epoch 163: 1.0\n",
      "                          epoch 164\n",
      "iteration 64200, loss 0.0006812938954681158\n",
      "iteration 64300, loss 0.00401917053386569\n",
      "iteration 64400, loss 0.0005156827392056584\n",
      "iteration 64500, loss 0.001165150199085474\n",
      "TEST accuracy at end of epoch 164: 0.9136\n",
      "TRAIN accuracy at end of epoch 164: 1.0\n",
      "                          epoch 165\n",
      "iteration 64600, loss 0.030943656340241432\n",
      "iteration 64700, loss 0.012806171551346779\n",
      "iteration 64800, loss 0.001068117213435471\n",
      "iteration 64900, loss 0.01255799736827612\n",
      "TEST accuracy at end of epoch 165: 0.9137\n",
      "TRAIN accuracy at end of epoch 165: 1.0\n",
      "                          epoch 166\n",
      "iteration 65000, loss 0.014341052621603012\n",
      "iteration 65100, loss 0.0047765146009624004\n",
      "iteration 65200, loss 0.004518953152000904\n",
      "TEST accuracy at end of epoch 166: 0.9152\n",
      "TRAIN accuracy at end of epoch 166: 1.0\n",
      "                          epoch 167\n",
      "iteration 65300, loss 0.000675337272696197\n",
      "iteration 65400, loss 0.0048379553481936455\n",
      "iteration 65500, loss 0.0007515063043683767\n",
      "iteration 65600, loss 0.0006614953745156527\n",
      "TEST accuracy at end of epoch 167: 0.9145\n",
      "TRAIN accuracy at end of epoch 167: 1.0\n",
      "                          epoch 168\n",
      "iteration 65700, loss 0.0005580839933827519\n",
      "iteration 65800, loss 0.0028277139645069838\n",
      "iteration 65900, loss 0.0017096318770200014\n",
      "iteration 66000, loss 0.021644843742251396\n",
      "TEST accuracy at end of epoch 168: 0.9149\n",
      "TRAIN accuracy at end of epoch 168: 1.0\n",
      "                          epoch 169\n",
      "iteration 66100, loss 0.00901607982814312\n",
      "iteration 66200, loss 0.002403115388005972\n",
      "iteration 66300, loss 0.0020529143512248993\n",
      "iteration 66400, loss 0.00013713451335206628\n",
      "TEST accuracy at end of epoch 169: 0.914\n",
      "TRAIN accuracy at end of epoch 169: 1.0\n",
      "                          epoch 170\n",
      "iteration 66500, loss 0.0008567497134208679\n",
      "iteration 66600, loss 0.0021432125940918922\n",
      "iteration 66700, loss 0.003499815007671714\n",
      "iteration 66800, loss 0.003437667153775692\n",
      "TEST accuracy at end of epoch 170: 0.9132\n",
      "TRAIN accuracy at end of epoch 170: 1.0\n",
      "                          epoch 171\n",
      "iteration 66900, loss 0.0019456397276371717\n",
      "iteration 67000, loss 0.0007337065762840211\n",
      "iteration 67100, loss 0.009925870224833488\n",
      "iteration 67200, loss 0.00016012645210139453\n",
      "TEST accuracy at end of epoch 171: 0.9137\n",
      "TRAIN accuracy at end of epoch 171: 1.0\n",
      "                          epoch 172\n",
      "iteration 67300, loss 0.014469015412032604\n",
      "iteration 67400, loss 0.0011000351514667273\n",
      "iteration 67500, loss 0.006054792087525129\n",
      "iteration 67600, loss 0.0027511806692928076\n",
      "TEST accuracy at end of epoch 172: 0.9135\n",
      "TRAIN accuracy at end of epoch 172: 1.0\n",
      "                          epoch 173\n",
      "iteration 67700, loss 0.002452945802360773\n",
      "iteration 67800, loss 0.0026385767851024866\n",
      "iteration 67900, loss 0.0006901646265760064\n",
      "iteration 68000, loss 0.0015169286634773016\n",
      "TEST accuracy at end of epoch 173: 0.9142\n",
      "TRAIN accuracy at end of epoch 173: 1.0\n",
      "                          epoch 174\n",
      "iteration 68100, loss 0.0010799396550282836\n",
      "iteration 68200, loss 0.0010225127916783094\n",
      "iteration 68300, loss 0.008026985451579094\n",
      "iteration 68400, loss 0.018019897863268852\n",
      "TEST accuracy at end of epoch 174: 0.9134\n",
      "TRAIN accuracy at end of epoch 174: 1.0\n",
      "                          epoch 175\n",
      "iteration 68500, loss 0.007598559837788343\n",
      "iteration 68600, loss 0.0010274674277752638\n",
      "iteration 68700, loss 0.0020192654337733984\n",
      "iteration 68800, loss 0.0008759032934904099\n",
      "TEST accuracy at end of epoch 175: 0.9138\n",
      "TRAIN accuracy at end of epoch 175: 1.0\n",
      "                          epoch 176\n",
      "iteration 68900, loss 0.0014776501338928938\n",
      "iteration 69000, loss 0.003949888050556183\n",
      "iteration 69100, loss 0.0013961559161543846\n",
      "iteration 69200, loss 0.0029445141553878784\n",
      "TEST accuracy at end of epoch 176: 0.9127\n",
      "TRAIN accuracy at end of epoch 176: 1.0\n",
      "                          epoch 177\n",
      "iteration 69300, loss 0.0009980339091271162\n",
      "iteration 69400, loss 0.00018697651103138924\n",
      "iteration 69500, loss 0.00034033373231068254\n",
      "TEST accuracy at end of epoch 177: 0.9137\n",
      "TRAIN accuracy at end of epoch 177: 1.0\n",
      "                          epoch 178\n",
      "iteration 69600, loss 0.008632531389594078\n",
      "iteration 69700, loss 0.0036309666465967894\n",
      "iteration 69800, loss 0.004193229600787163\n",
      "iteration 69900, loss 0.0019024754874408245\n",
      "TEST accuracy at end of epoch 178: 0.9142\n",
      "TRAIN accuracy at end of epoch 178: 1.0\n",
      "                          epoch 179\n",
      "iteration 70000, loss 0.0027432518545538187\n",
      "iteration 70100, loss 0.0012678132625296712\n",
      "iteration 70200, loss 0.0009364438010379672\n",
      "iteration 70300, loss 0.0017361748032271862\n",
      "TEST accuracy at end of epoch 179: 0.914\n",
      "TRAIN accuracy at end of epoch 179: 1.0\n",
      "                          epoch 180\n",
      "iteration 70400, loss 3.0240211344789714e-05\n",
      "iteration 70500, loss 0.0011605441104620695\n",
      "iteration 70600, loss 0.0004966466804035008\n",
      "iteration 70700, loss 0.000186880148248747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST accuracy at end of epoch 180: 0.9146\n",
      "TRAIN accuracy at end of epoch 180: 1.0\n",
      "                          epoch 181\n",
      "iteration 70800, loss 5.522068386198953e-05\n",
      "iteration 70900, loss 0.0020152481738477945\n",
      "iteration 71000, loss 0.0007990323356352746\n",
      "iteration 71100, loss 0.0009881085716187954\n",
      "TEST accuracy at end of epoch 181: 0.9147\n",
      "TRAIN accuracy at end of epoch 181: 1.0\n",
      "                          epoch 182\n",
      "iteration 71200, loss 0.005280962213873863\n",
      "iteration 71300, loss 0.0034308813046664\n",
      "iteration 71400, loss 0.00294517888687551\n",
      "iteration 71500, loss 0.004368482157588005\n",
      "TEST accuracy at end of epoch 182: 0.914\n",
      "TRAIN accuracy at end of epoch 182: 1.0\n",
      "                          epoch 183\n",
      "iteration 71600, loss 0.002664112253114581\n",
      "iteration 71700, loss 0.000589313916862011\n",
      "iteration 71800, loss 0.0066322931088507175\n",
      "iteration 71900, loss 0.0011153463274240494\n",
      "TEST accuracy at end of epoch 183: 0.9137\n",
      "TRAIN accuracy at end of epoch 183: 0.99\n",
      "                          epoch 184\n",
      "iteration 72000, loss 0.012561171315610409\n",
      "iteration 72100, loss 0.0014066870789974928\n",
      "iteration 72200, loss 0.004212172701954842\n",
      "iteration 72300, loss 0.001015951856970787\n",
      "TEST accuracy at end of epoch 184: 0.9151\n",
      "TRAIN accuracy at end of epoch 184: 1.0\n",
      "                          epoch 185\n",
      "iteration 72400, loss 0.001668618991971016\n",
      "iteration 72500, loss 0.03278600797057152\n",
      "iteration 72600, loss 0.0002837285283021629\n",
      "iteration 72700, loss 0.0008255504071712494\n",
      "TEST accuracy at end of epoch 185: 0.9144\n",
      "TRAIN accuracy at end of epoch 185: 1.0\n",
      "                          epoch 186\n",
      "iteration 72800, loss 0.0019786113407462835\n",
      "iteration 72900, loss 0.002788910875096917\n",
      "iteration 73000, loss 0.0035838917829096317\n",
      "iteration 73100, loss 0.00031731859780848026\n",
      "TEST accuracy at end of epoch 186: 0.9143\n",
      "TRAIN accuracy at end of epoch 186: 1.0\n",
      "                          epoch 187\n",
      "iteration 73200, loss 0.00010741319420048967\n",
      "iteration 73300, loss 0.0008791165892034769\n",
      "iteration 73400, loss 0.010774070397019386\n",
      "iteration 73500, loss 0.0006961670005694032\n",
      "TEST accuracy at end of epoch 187: 0.9145\n",
      "TRAIN accuracy at end of epoch 187: 1.0\n",
      "                          epoch 188\n",
      "iteration 73600, loss 0.0016020422335714102\n",
      "iteration 73700, loss 0.003835486713796854\n",
      "iteration 73800, loss 0.0055779702961444855\n",
      "TEST accuracy at end of epoch 188: 0.9142\n",
      "TRAIN accuracy at end of epoch 188: 1.0\n",
      "                          epoch 189\n",
      "iteration 73900, loss 0.00402571726590395\n",
      "iteration 74000, loss 0.007937305606901646\n",
      "iteration 74100, loss 0.002645376604050398\n",
      "iteration 74200, loss 0.0020006352569907904\n",
      "TEST accuracy at end of epoch 189: 0.9146\n",
      "TRAIN accuracy at end of epoch 189: 1.0\n",
      "                          epoch 190\n",
      "iteration 74300, loss 0.0003227692795917392\n",
      "iteration 74400, loss 0.011359692551195621\n",
      "iteration 74500, loss 0.0007693596999160945\n",
      "iteration 74600, loss 0.001464990433305502\n",
      "TEST accuracy at end of epoch 190: 0.9148\n",
      "TRAIN accuracy at end of epoch 190: 1.0\n",
      "                          epoch 191\n",
      "iteration 74700, loss 0.001595149515196681\n",
      "iteration 74800, loss 0.0027811694890260696\n",
      "iteration 74900, loss 0.00962098129093647\n",
      "iteration 75000, loss 0.0013949447311460972\n",
      "TEST accuracy at end of epoch 191: 0.9147\n",
      "TRAIN accuracy at end of epoch 191: 1.0\n",
      "                          epoch 192\n",
      "iteration 75100, loss 0.0005716161685995758\n",
      "iteration 75200, loss 0.0013940251665189862\n",
      "iteration 75300, loss 0.0013676001690328121\n",
      "iteration 75400, loss 0.0004110463778488338\n",
      "TEST accuracy at end of epoch 192: 0.9145\n",
      "TRAIN accuracy at end of epoch 192: 1.0\n",
      "                          epoch 193\n",
      "iteration 75500, loss 0.0005119960987940431\n",
      "iteration 75600, loss 0.0003150156117044389\n",
      "iteration 75700, loss 0.0017192272935062647\n",
      "iteration 75800, loss 0.005821668542921543\n",
      "TEST accuracy at end of epoch 193: 0.9147\n",
      "TRAIN accuracy at end of epoch 193: 1.0\n",
      "                          epoch 194\n",
      "iteration 75900, loss 0.006554365158081055\n",
      "iteration 76000, loss 0.04622410237789154\n",
      "iteration 76100, loss 0.002152874832972884\n",
      "iteration 76200, loss 0.0007910841377452016\n",
      "TEST accuracy at end of epoch 194: 0.9144\n",
      "TRAIN accuracy at end of epoch 194: 1.0\n",
      "                          epoch 195\n",
      "iteration 76300, loss 0.002772121224552393\n",
      "iteration 76400, loss 0.004227300174534321\n",
      "iteration 76500, loss 0.004238533321768045\n",
      "iteration 76600, loss 0.00018116984574589878\n",
      "TEST accuracy at end of epoch 195: 0.9144\n",
      "TRAIN accuracy at end of epoch 195: 1.0\n",
      "                          epoch 196\n",
      "iteration 76700, loss 0.0007132859318517148\n",
      "iteration 76800, loss 0.0006410634377971292\n",
      "iteration 76900, loss 0.0005469924071803689\n",
      "iteration 77000, loss 0.003208539681509137\n",
      "TEST accuracy at end of epoch 196: 0.9146\n",
      "TRAIN accuracy at end of epoch 196: 1.0\n",
      "                          epoch 197\n",
      "iteration 77100, loss 0.0025557989720255136\n",
      "iteration 77200, loss 0.00807182677090168\n",
      "iteration 77300, loss 0.011073403991758823\n",
      "iteration 77400, loss 0.001370724756270647\n",
      "TEST accuracy at end of epoch 197: 0.9145\n",
      "TRAIN accuracy at end of epoch 197: 1.0\n",
      "                          epoch 198\n",
      "iteration 77500, loss 0.0012536593712866306\n",
      "iteration 77600, loss 0.00027064033201895654\n",
      "iteration 77700, loss 0.0002576859260443598\n",
      "iteration 77800, loss 0.00031595423934049904\n",
      "TEST accuracy at end of epoch 198: 0.9144\n",
      "TRAIN accuracy at end of epoch 198: 1.0\n",
      "                          epoch 199\n",
      "iteration 77900, loss 0.003298854688182473\n",
      "iteration 78000, loss 0.012424135580658913\n",
      "iteration 78100, loss 0.0011574840173125267\n",
      "iteration 78200, loss 0.0021273756865411997\n",
      "TEST accuracy at end of epoch 199: 0.9144\n",
      "TRAIN accuracy at end of epoch 199: 1.0\n",
      "╰┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╯\n",
      "╭┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╮\n",
      "                         n: 3, res: True\n",
      "size of reduce_mean: (?, 1, 1, 64)\n",
      "size of squeeze: (?, 64)\n",
      "training for 78125 iterations\n",
      "                          epoch 0\n",
      "iteration 100, loss 1.6613879203796387\n",
      "iteration 200, loss 1.3161498308181763\n",
      "iteration 300, loss 1.1072957515716553\n",
      "TEST accuracy at end of epoch 0: 0.182\n",
      "TRAIN accuracy at end of epoch 0: 0.22\n",
      "                          epoch 1\n",
      "iteration 400, loss 1.102392554283142\n",
      "iteration 500, loss 1.110951542854309\n",
      "iteration 600, loss 0.9885979294776917\n",
      "iteration 700, loss 0.8580679297447205\n",
      "TEST accuracy at end of epoch 1: 0.5904\n",
      "TRAIN accuracy at end of epoch 1: 0.54\n",
      "                          epoch 2\n",
      "iteration 800, loss 0.6895461082458496\n",
      "iteration 900, loss 0.8055877685546875\n",
      "iteration 1000, loss 0.7414970397949219\n",
      "iteration 1100, loss 0.8725310564041138\n",
      "TEST accuracy at end of epoch 2: 0.6751\n",
      "TRAIN accuracy at end of epoch 2: 0.65\n",
      "                          epoch 3\n",
      "iteration 1200, loss 0.5510725378990173\n",
      "iteration 1300, loss 0.7264318466186523\n",
      "iteration 1400, loss 0.7695969343185425\n",
      "iteration 1500, loss 0.6139127016067505\n",
      "TEST accuracy at end of epoch 3: 0.5933\n",
      "TRAIN accuracy at end of epoch 3: 0.57\n",
      "                          epoch 4\n",
      "iteration 1600, loss 0.7101335525512695\n",
      "iteration 1700, loss 0.5903331637382507\n",
      "iteration 1800, loss 0.5845355987548828\n",
      "iteration 1900, loss 0.4631119966506958\n",
      "TEST accuracy at end of epoch 4: 0.7364\n",
      "TRAIN accuracy at end of epoch 4: 0.72\n",
      "                          epoch 5\n",
      "iteration 2000, loss 0.667218804359436\n",
      "iteration 2100, loss 0.4941974878311157\n",
      "iteration 2200, loss 0.689569890499115\n",
      "iteration 2300, loss 0.4486411213874817\n",
      "TEST accuracy at end of epoch 5: 0.7068\n",
      "TRAIN accuracy at end of epoch 5: 0.68\n",
      "                          epoch 6\n",
      "iteration 2400, loss 0.41286587715148926\n",
      "iteration 2500, loss 0.45087504386901855\n",
      "iteration 2600, loss 0.47146135568618774\n",
      "iteration 2700, loss 0.49758657813072205\n",
      "TEST accuracy at end of epoch 6: 0.7949\n",
      "TRAIN accuracy at end of epoch 6: 0.84\n",
      "                          epoch 7\n",
      "iteration 2800, loss 0.5201115608215332\n",
      "iteration 2900, loss 0.30214923620224\n",
      "iteration 3000, loss 0.38667553663253784\n",
      "iteration 3100, loss 0.5225386619567871\n",
      "TEST accuracy at end of epoch 7: 0.796\n",
      "TRAIN accuracy at end of epoch 7: 0.8\n",
      "                          epoch 8\n",
      "iteration 3200, loss 0.3931484818458557\n",
      "iteration 3300, loss 0.4671250879764557\n",
      "iteration 3400, loss 0.46363359689712524\n",
      "iteration 3500, loss 0.5522421598434448\n",
      "TEST accuracy at end of epoch 8: 0.7307\n",
      "TRAIN accuracy at end of epoch 8: 0.82\n",
      "                          epoch 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3600, loss 0.5321890115737915\n",
      "iteration 3700, loss 0.4098251163959503\n",
      "iteration 3800, loss 0.4434494376182556\n",
      "iteration 3900, loss 0.47997477650642395\n",
      "TEST accuracy at end of epoch 9: 0.8049\n",
      "TRAIN accuracy at end of epoch 9: 0.83\n",
      "                          epoch 10\n",
      "iteration 4000, loss 0.3418161869049072\n",
      "iteration 4100, loss 0.533062219619751\n",
      "iteration 4200, loss 0.2880774140357971\n",
      "iteration 4300, loss 0.3633609712123871\n",
      "TEST accuracy at end of epoch 10: 0.7992\n",
      "TRAIN accuracy at end of epoch 10: 0.84\n",
      "                          epoch 11\n",
      "iteration 4400, loss 0.3405272364616394\n",
      "iteration 4500, loss 0.33085203170776367\n",
      "iteration 4600, loss 0.49026837944984436\n",
      "TEST accuracy at end of epoch 11: 0.7545\n",
      "TRAIN accuracy at end of epoch 11: 0.73\n",
      "                          epoch 12\n",
      "iteration 4700, loss 0.4627982974052429\n",
      "iteration 4800, loss 0.3545210361480713\n",
      "iteration 4900, loss 0.31835609674453735\n",
      "iteration 5000, loss 0.27946364879608154\n",
      "TEST accuracy at end of epoch 12: 0.8077\n",
      "TRAIN accuracy at end of epoch 12: 0.83\n",
      "                          epoch 13\n",
      "iteration 5100, loss 0.3875119090080261\n",
      "iteration 5200, loss 0.36136043071746826\n",
      "iteration 5300, loss 0.2796933054924011\n",
      "iteration 5400, loss 0.36034125089645386\n",
      "TEST accuracy at end of epoch 13: 0.826\n",
      "TRAIN accuracy at end of epoch 13: 0.86\n",
      "                          epoch 14\n",
      "iteration 5500, loss 0.3288595676422119\n",
      "iteration 5600, loss 0.45266300439834595\n",
      "iteration 5700, loss 0.4173511266708374\n",
      "iteration 5800, loss 0.44322794675827026\n",
      "TEST accuracy at end of epoch 14: 0.8382\n",
      "TRAIN accuracy at end of epoch 14: 0.88\n",
      "                          epoch 15\n",
      "iteration 5900, loss 0.29520121216773987\n",
      "iteration 6000, loss 0.3195529580116272\n",
      "iteration 6100, loss 0.34656980633735657\n",
      "iteration 6200, loss 0.3565528392791748\n",
      "TEST accuracy at end of epoch 15: 0.8224\n",
      "TRAIN accuracy at end of epoch 15: 0.77\n",
      "                          epoch 16\n",
      "iteration 6300, loss 0.26242849230766296\n",
      "iteration 6400, loss 0.33201783895492554\n",
      "iteration 6500, loss 0.25574684143066406\n",
      "iteration 6600, loss 0.34000885486602783\n",
      "TEST accuracy at end of epoch 16: 0.8327\n",
      "TRAIN accuracy at end of epoch 16: 0.86\n",
      "                          epoch 17\n",
      "iteration 6700, loss 0.211478590965271\n",
      "iteration 6800, loss 0.3530641794204712\n",
      "iteration 6900, loss 0.335374116897583\n",
      "iteration 7000, loss 0.38653701543807983\n",
      "TEST accuracy at end of epoch 17: 0.8324\n",
      "TRAIN accuracy at end of epoch 17: 0.88\n",
      "                          epoch 18\n",
      "iteration 7100, loss 0.29320186376571655\n",
      "iteration 7200, loss 0.23203754425048828\n",
      "iteration 7300, loss 0.26472780108451843\n",
      "iteration 7400, loss 0.22967621684074402\n",
      "TEST accuracy at end of epoch 18: 0.8466\n",
      "TRAIN accuracy at end of epoch 18: 0.85\n",
      "                          epoch 19\n",
      "iteration 7500, loss 0.2667273283004761\n",
      "iteration 7600, loss 0.29597175121307373\n",
      "iteration 7700, loss 0.21008002758026123\n",
      "iteration 7800, loss 0.4975326657295227\n",
      "TEST accuracy at end of epoch 19: 0.817\n",
      "TRAIN accuracy at end of epoch 19: 0.85\n",
      "                          epoch 20\n",
      "iteration 7900, loss 0.27465665340423584\n",
      "iteration 8000, loss 0.21977098286151886\n",
      "iteration 8100, loss 0.4176034927368164\n",
      "iteration 8200, loss 0.23084595799446106\n",
      "TEST accuracy at end of epoch 20: 0.8265\n",
      "TRAIN accuracy at end of epoch 20: 0.8\n",
      "                          epoch 21\n",
      "iteration 8300, loss 0.15556922554969788\n",
      "iteration 8400, loss 0.21590551733970642\n",
      "iteration 8500, loss 0.3544960021972656\n",
      "iteration 8600, loss 0.2311399132013321\n",
      "TEST accuracy at end of epoch 21: 0.8331\n",
      "TRAIN accuracy at end of epoch 21: 0.87\n",
      "                          epoch 22\n",
      "iteration 8700, loss 0.25552141666412354\n",
      "iteration 8800, loss 0.30051353573799133\n",
      "iteration 8900, loss 0.32947173714637756\n",
      "TEST accuracy at end of epoch 22: 0.8412\n",
      "TRAIN accuracy at end of epoch 22: 0.88\n",
      "                          epoch 23\n",
      "iteration 9000, loss 0.34196627140045166\n",
      "iteration 9100, loss 0.2928827404975891\n",
      "iteration 9200, loss 0.19578537344932556\n",
      "iteration 9300, loss 0.24977678060531616\n",
      "TEST accuracy at end of epoch 23: 0.8365\n",
      "TRAIN accuracy at end of epoch 23: 0.88\n",
      "                          epoch 24\n",
      "iteration 9400, loss 0.231610968708992\n",
      "iteration 9500, loss 0.28218650817871094\n",
      "iteration 9600, loss 0.24991938471794128\n",
      "iteration 9700, loss 0.2543604373931885\n",
      "TEST accuracy at end of epoch 24: 0.855\n",
      "TRAIN accuracy at end of epoch 24: 0.86\n",
      "                          epoch 25\n",
      "iteration 9800, loss 0.22298341989517212\n",
      "iteration 9900, loss 0.31901586055755615\n",
      "iteration 10000, loss 0.247013658285141\n",
      "iteration 10100, loss 0.21504460275173187\n",
      "TEST accuracy at end of epoch 25: 0.8376\n",
      "TRAIN accuracy at end of epoch 25: 0.92\n",
      "                          epoch 26\n",
      "iteration 10200, loss 0.25233128666877747\n",
      "iteration 10300, loss 0.29956454038619995\n",
      "iteration 10400, loss 0.30756038427352905\n",
      "iteration 10500, loss 0.2620685398578644\n",
      "TEST accuracy at end of epoch 26: 0.8544\n",
      "TRAIN accuracy at end of epoch 26: 0.92\n",
      "                          epoch 27\n",
      "iteration 10600, loss 0.15208779275417328\n",
      "iteration 10700, loss 0.20987524092197418\n",
      "iteration 10800, loss 0.16919711232185364\n",
      "iteration 10900, loss 0.3030312657356262\n",
      "TEST accuracy at end of epoch 27: 0.8614\n",
      "TRAIN accuracy at end of epoch 27: 0.93\n",
      "                          epoch 28\n",
      "iteration 11000, loss 0.311123788356781\n",
      "iteration 11100, loss 0.3278881013393402\n",
      "iteration 11200, loss 0.22521278262138367\n",
      "iteration 11300, loss 0.24249185621738434\n",
      "TEST accuracy at end of epoch 28: 0.8438\n",
      "TRAIN accuracy at end of epoch 28: 0.89\n",
      "                          epoch 29\n",
      "iteration 11400, loss 0.20969168841838837\n",
      "iteration 11500, loss 0.15096089243888855\n",
      "iteration 11600, loss 0.3022741675376892\n",
      "iteration 11700, loss 0.16090407967567444\n",
      "TEST accuracy at end of epoch 29: 0.8633\n",
      "TRAIN accuracy at end of epoch 29: 0.89\n",
      "                          epoch 30\n",
      "iteration 11800, loss 0.2197568714618683\n",
      "iteration 11900, loss 0.11800868809223175\n",
      "iteration 12000, loss 0.2467358112335205\n",
      "iteration 12100, loss 0.2636606693267822\n",
      "TEST accuracy at end of epoch 30: 0.8595\n",
      "TRAIN accuracy at end of epoch 30: 0.91\n",
      "                          epoch 31\n",
      "iteration 12200, loss 0.2328723669052124\n",
      "iteration 12300, loss 0.21209508180618286\n",
      "iteration 12400, loss 0.1889374852180481\n",
      "iteration 12500, loss 0.22378534078598022\n",
      "TEST accuracy at end of epoch 31: 0.8453\n",
      "TRAIN accuracy at end of epoch 31: 0.91\n",
      "                          epoch 32\n",
      "iteration 12600, loss 0.2704324722290039\n",
      "iteration 12700, loss 0.19116435945034027\n",
      "iteration 12800, loss 0.31633028388023376\n",
      "iteration 12900, loss 0.23935356736183167\n",
      "TEST accuracy at end of epoch 32: 0.8642\n",
      "TRAIN accuracy at end of epoch 32: 0.95\n",
      "                          epoch 33\n",
      "iteration 13000, loss 0.30991131067276\n",
      "iteration 13100, loss 0.20355992019176483\n",
      "iteration 13200, loss 0.19020918011665344\n",
      "TEST accuracy at end of epoch 33: 0.863\n",
      "TRAIN accuracy at end of epoch 33: 0.9\n",
      "                          epoch 34\n",
      "iteration 13300, loss 0.14136770367622375\n",
      "iteration 13400, loss 0.16510552167892456\n",
      "iteration 13500, loss 0.17703500390052795\n",
      "iteration 13600, loss 0.24491176009178162\n",
      "TEST accuracy at end of epoch 34: 0.8546\n",
      "TRAIN accuracy at end of epoch 34: 0.89\n",
      "                          epoch 35\n",
      "iteration 13700, loss 0.1368502825498581\n",
      "iteration 13800, loss 0.11548644304275513\n",
      "iteration 13900, loss 0.15965057909488678\n",
      "iteration 14000, loss 0.1353638470172882\n",
      "TEST accuracy at end of epoch 35: 0.8615\n",
      "TRAIN accuracy at end of epoch 35: 0.94\n",
      "                          epoch 36\n",
      "iteration 14100, loss 0.21095865964889526\n",
      "iteration 14200, loss 0.16508306562900543\n",
      "iteration 14300, loss 0.1426246166229248\n",
      "iteration 14400, loss 0.2741850018501282\n",
      "TEST accuracy at end of epoch 36: 0.8662\n",
      "TRAIN accuracy at end of epoch 36: 0.95\n",
      "                          epoch 37\n",
      "iteration 14500, loss 0.16308292746543884\n",
      "iteration 14600, loss 0.19061550498008728\n",
      "iteration 14700, loss 0.1871960163116455\n",
      "iteration 14800, loss 0.26805153489112854\n",
      "TEST accuracy at end of epoch 37: 0.8577\n",
      "TRAIN accuracy at end of epoch 37: 0.96\n",
      "                          epoch 38\n",
      "iteration 14900, loss 0.17740416526794434\n",
      "iteration 15000, loss 0.09838851541280746\n",
      "iteration 15100, loss 0.20675817131996155\n",
      "iteration 15200, loss 0.222042053937912\n",
      "TEST accuracy at end of epoch 38: 0.8857\n",
      "TRAIN accuracy at end of epoch 38: 0.93\n",
      "                          epoch 39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 15300, loss 0.09756594896316528\n",
      "iteration 15400, loss 0.19141383469104767\n",
      "iteration 15500, loss 0.1432596743106842\n",
      "iteration 15600, loss 0.17190726101398468\n",
      "TEST accuracy at end of epoch 39: 0.8579\n",
      "TRAIN accuracy at end of epoch 39: 0.9\n",
      "                          epoch 40\n",
      "iteration 15700, loss 0.16310539841651917\n",
      "iteration 15800, loss 0.1281227171421051\n",
      "iteration 15900, loss 0.0952834039926529\n",
      "iteration 16000, loss 0.1333457976579666\n",
      "TEST accuracy at end of epoch 40: 0.8711\n",
      "TRAIN accuracy at end of epoch 40: 0.88\n",
      "                          epoch 41\n",
      "iteration 16100, loss 0.18387673795223236\n",
      "iteration 16200, loss 0.20104649662971497\n",
      "iteration 16300, loss 0.10564428567886353\n",
      "iteration 16400, loss 0.18207646906375885\n",
      "TEST accuracy at end of epoch 41: 0.861\n",
      "TRAIN accuracy at end of epoch 41: 0.87\n",
      "                          epoch 42\n",
      "iteration 16500, loss 0.13981974124908447\n",
      "iteration 16600, loss 0.17595341801643372\n",
      "iteration 16700, loss 0.19514644145965576\n",
      "iteration 16800, loss 0.15955078601837158\n",
      "TEST accuracy at end of epoch 42: 0.8695\n",
      "TRAIN accuracy at end of epoch 42: 0.93\n",
      "                          epoch 43\n",
      "iteration 16900, loss 0.12657317519187927\n",
      "iteration 17000, loss 0.10752419382333755\n",
      "iteration 17100, loss 0.17588630318641663\n",
      "iteration 17200, loss 0.19859930872917175\n",
      "TEST accuracy at end of epoch 43: 0.8806\n",
      "TRAIN accuracy at end of epoch 43: 0.97\n",
      "                          epoch 44\n",
      "iteration 17300, loss 0.16197705268859863\n",
      "iteration 17400, loss 0.10249735414981842\n",
      "iteration 17500, loss 0.13474424183368683\n",
      "TEST accuracy at end of epoch 44: 0.874\n",
      "TRAIN accuracy at end of epoch 44: 0.99\n",
      "                          epoch 45\n",
      "iteration 17600, loss 0.13510510325431824\n",
      "iteration 17700, loss 0.11195103824138641\n",
      "iteration 17800, loss 0.2713969945907593\n",
      "iteration 17900, loss 0.14373856782913208\n",
      "TEST accuracy at end of epoch 45: 0.8624\n",
      "TRAIN accuracy at end of epoch 45: 0.93\n",
      "                          epoch 46\n",
      "iteration 18000, loss 0.11421770602464676\n",
      "iteration 18100, loss 0.14105965197086334\n",
      "iteration 18200, loss 0.08805467188358307\n",
      "iteration 18300, loss 0.12977203726768494\n",
      "TEST accuracy at end of epoch 46: 0.8711\n",
      "TRAIN accuracy at end of epoch 46: 0.96\n",
      "                          epoch 47\n",
      "iteration 18400, loss 0.16252054274082184\n",
      "iteration 18500, loss 0.14203311502933502\n",
      "iteration 18600, loss 0.15417298674583435\n",
      "iteration 18700, loss 0.1776251345872879\n",
      "TEST accuracy at end of epoch 47: 0.8827\n",
      "TRAIN accuracy at end of epoch 47: 0.94\n",
      "                          epoch 48\n",
      "iteration 18800, loss 0.11201488226652145\n",
      "iteration 18900, loss 0.10769602656364441\n",
      "iteration 19000, loss 0.08286283910274506\n",
      "iteration 19100, loss 0.1705632507801056\n",
      "TEST accuracy at end of epoch 48: 0.8693\n",
      "TRAIN accuracy at end of epoch 48: 0.94\n",
      "                          epoch 49\n",
      "iteration 19200, loss 0.04150420427322388\n",
      "iteration 19300, loss 0.10345706343650818\n",
      "iteration 19400, loss 0.1063309758901596\n",
      "iteration 19500, loss 0.2546761631965637\n",
      "TEST accuracy at end of epoch 49: 0.8662\n",
      "TRAIN accuracy at end of epoch 49: 0.94\n",
      "                          epoch 50\n",
      "iteration 19600, loss 0.2011108100414276\n",
      "iteration 19700, loss 0.15037477016448975\n",
      "iteration 19800, loss 0.13466694951057434\n",
      "iteration 19900, loss 0.1150357648730278\n",
      "TEST accuracy at end of epoch 50: 0.8715\n",
      "TRAIN accuracy at end of epoch 50: 0.91\n",
      "                          epoch 51\n",
      "iteration 20000, loss 0.12135900557041168\n",
      "iteration 20100, loss 0.11352129280567169\n",
      "iteration 20200, loss 0.1271759271621704\n",
      "iteration 20300, loss 0.09323593229055405\n",
      "TEST accuracy at end of epoch 51: 0.8621\n",
      "TRAIN accuracy at end of epoch 51: 0.94\n",
      "                          epoch 52\n",
      "iteration 20400, loss 0.07586293667554855\n",
      "iteration 20500, loss 0.21218661963939667\n",
      "iteration 20600, loss 0.15690451860427856\n",
      "iteration 20700, loss 0.19465294480323792\n",
      "TEST accuracy at end of epoch 52: 0.8793\n",
      "TRAIN accuracy at end of epoch 52: 0.95\n",
      "                          epoch 53\n",
      "iteration 20800, loss 0.13645270466804504\n",
      "iteration 20900, loss 0.1666097342967987\n",
      "iteration 21000, loss 0.09857970476150513\n",
      "iteration 21100, loss 0.12654650211334229\n",
      "TEST accuracy at end of epoch 53: 0.8737\n",
      "TRAIN accuracy at end of epoch 53: 0.92\n",
      "                          epoch 54\n",
      "iteration 21200, loss 0.11484025418758392\n",
      "iteration 21300, loss 0.1983717381954193\n",
      "iteration 21400, loss 0.15597191452980042\n",
      "iteration 21500, loss 0.10215888917446136\n",
      "TEST accuracy at end of epoch 54: 0.8597\n",
      "TRAIN accuracy at end of epoch 54: 0.92\n",
      "                          epoch 55\n",
      "iteration 21600, loss 0.09279082715511322\n",
      "iteration 21700, loss 0.11380977183580399\n",
      "iteration 21800, loss 0.1330735832452774\n",
      "TEST accuracy at end of epoch 55: 0.8652\n",
      "TRAIN accuracy at end of epoch 55: 0.96\n",
      "                          epoch 56\n",
      "iteration 21900, loss 0.13668890297412872\n",
      "iteration 22000, loss 0.12855041027069092\n",
      "iteration 22100, loss 0.12008713185787201\n",
      "iteration 22200, loss 0.14327171444892883\n",
      "TEST accuracy at end of epoch 56: 0.8716\n",
      "TRAIN accuracy at end of epoch 56: 0.93\n",
      "                          epoch 57\n",
      "iteration 22300, loss 0.11017154157161713\n",
      "iteration 22400, loss 0.24615618586540222\n",
      "iteration 22500, loss 0.1570865958929062\n",
      "iteration 22600, loss 0.151193305850029\n",
      "TEST accuracy at end of epoch 57: 0.8751\n",
      "TRAIN accuracy at end of epoch 57: 0.97\n",
      "                          epoch 58\n",
      "iteration 22700, loss 0.0871579498052597\n",
      "iteration 22800, loss 0.07468877732753754\n",
      "iteration 22900, loss 0.20184394717216492\n",
      "iteration 23000, loss 0.08057167381048203\n",
      "TEST accuracy at end of epoch 58: 0.8875\n",
      "TRAIN accuracy at end of epoch 58: 0.95\n",
      "                          epoch 59\n",
      "iteration 23100, loss 0.18089140951633453\n",
      "iteration 23200, loss 0.07776226848363876\n",
      "iteration 23300, loss 0.10897351801395416\n",
      "iteration 23400, loss 0.11438221484422684\n",
      "TEST accuracy at end of epoch 59: 0.8604\n",
      "TRAIN accuracy at end of epoch 59: 0.97\n",
      "                          epoch 60\n",
      "iteration 23500, loss 0.07919729501008987\n",
      "iteration 23600, loss 0.08646055310964584\n",
      "iteration 23700, loss 0.1689455807209015\n",
      "iteration 23800, loss 0.11168136447668076\n",
      "TEST accuracy at end of epoch 60: 0.8599\n",
      "TRAIN accuracy at end of epoch 60: 0.99\n",
      "                          epoch 61\n",
      "iteration 23900, loss 0.11974715441465378\n",
      "iteration 24000, loss 0.09417548030614853\n",
      "iteration 24100, loss 0.1022043377161026\n",
      "iteration 24200, loss 0.11434360593557358\n",
      "TEST accuracy at end of epoch 61: 0.8726\n",
      "TRAIN accuracy at end of epoch 61: 0.99\n",
      "                          epoch 62\n",
      "iteration 24300, loss 0.10306042432785034\n",
      "iteration 24400, loss 0.06500431895256042\n",
      "iteration 24500, loss 0.09002101421356201\n",
      "iteration 24600, loss 0.23820467293262482\n",
      "TEST accuracy at end of epoch 62: 0.8666\n",
      "TRAIN accuracy at end of epoch 62: 0.91\n",
      "                          epoch 63\n",
      "iteration 24700, loss 0.11112278699874878\n",
      "iteration 24800, loss 0.13746139407157898\n",
      "iteration 24900, loss 0.10866190493106842\n",
      "iteration 25000, loss 0.06991583108901978\n",
      "TEST accuracy at end of epoch 63: 0.8687\n",
      "TRAIN accuracy at end of epoch 63: 0.96\n",
      "                          epoch 64\n",
      "iteration 25100, loss 0.09928032755851746\n",
      "iteration 25200, loss 0.03840049356222153\n",
      "iteration 25300, loss 0.16943183541297913\n",
      "iteration 25400, loss 0.0956791490316391\n",
      "TEST accuracy at end of epoch 64: 0.8664\n",
      "TRAIN accuracy at end of epoch 64: 0.91\n",
      "                          epoch 65\n",
      "iteration 25500, loss 0.08826994895935059\n",
      "iteration 25600, loss 0.10151131451129913\n",
      "iteration 25700, loss 0.06342717260122299\n",
      "iteration 25800, loss 0.1365572214126587\n",
      "TEST accuracy at end of epoch 65: 0.8909\n",
      "TRAIN accuracy at end of epoch 65: 0.97\n",
      "                          epoch 66\n",
      "iteration 25900, loss 0.04175366461277008\n",
      "iteration 26000, loss 0.07453598082065582\n",
      "iteration 26100, loss 0.09317406266927719\n",
      "TEST accuracy at end of epoch 66: 0.8943\n",
      "TRAIN accuracy at end of epoch 66: 0.96\n",
      "                          epoch 67\n",
      "iteration 26200, loss 0.05339229106903076\n",
      "iteration 26300, loss 0.11382344365119934\n",
      "iteration 26400, loss 0.05967530608177185\n",
      "iteration 26500, loss 0.1138123944401741\n",
      "TEST accuracy at end of epoch 67: 0.8842\n",
      "TRAIN accuracy at end of epoch 67: 0.97\n",
      "                          epoch 68\n",
      "iteration 26600, loss 0.05532952770590782\n",
      "iteration 26700, loss 0.08336061239242554\n",
      "iteration 26800, loss 0.1490548700094223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 26900, loss 0.129150390625\n",
      "TEST accuracy at end of epoch 68: 0.8729\n",
      "TRAIN accuracy at end of epoch 68: 0.96\n",
      "                          epoch 69\n",
      "iteration 27000, loss 0.0899999737739563\n",
      "iteration 27100, loss 0.09967383742332458\n",
      "iteration 27200, loss 0.07677562534809113\n",
      "iteration 27300, loss 0.21000753343105316\n",
      "TEST accuracy at end of epoch 69: 0.8825\n",
      "TRAIN accuracy at end of epoch 69: 0.96\n",
      "                          epoch 70\n",
      "iteration 27400, loss 0.09802600741386414\n",
      "iteration 27500, loss 0.03903006762266159\n",
      "iteration 27600, loss 0.09504292160272598\n",
      "iteration 27700, loss 0.13760395348072052\n",
      "TEST accuracy at end of epoch 70: 0.8901\n",
      "TRAIN accuracy at end of epoch 70: 0.97\n",
      "                          epoch 71\n",
      "iteration 27800, loss 0.08330608904361725\n",
      "iteration 27900, loss 0.08542651683092117\n",
      "iteration 28000, loss 0.0795864686369896\n",
      "iteration 28100, loss 0.09746496379375458\n",
      "TEST accuracy at end of epoch 71: 0.8812\n",
      "TRAIN accuracy at end of epoch 71: 0.97\n",
      "                          epoch 72\n",
      "iteration 28200, loss 0.146819606423378\n",
      "iteration 28300, loss 0.08101505041122437\n",
      "iteration 28400, loss 0.04839673638343811\n",
      "iteration 28500, loss 0.0695769190788269\n",
      "TEST accuracy at end of epoch 72: 0.8905\n",
      "TRAIN accuracy at end of epoch 72: 0.99\n",
      "                          epoch 73\n",
      "iteration 28600, loss 0.14842599630355835\n",
      "iteration 28700, loss 0.14421570301055908\n",
      "iteration 28800, loss 0.065593421459198\n",
      "iteration 28900, loss 0.06332158297300339\n",
      "TEST accuracy at end of epoch 73: 0.8852\n",
      "TRAIN accuracy at end of epoch 73: 0.99\n",
      "                          epoch 74\n",
      "iteration 29000, loss 0.08054151386022568\n",
      "iteration 29100, loss 0.0679876059293747\n",
      "iteration 29200, loss 0.063715860247612\n",
      "iteration 29300, loss 0.13924098014831543\n",
      "TEST accuracy at end of epoch 74: 0.872\n",
      "TRAIN accuracy at end of epoch 74: 0.93\n",
      "                          epoch 75\n",
      "iteration 29400, loss 0.14348198473453522\n",
      "iteration 29500, loss 0.08418277651071548\n",
      "iteration 29600, loss 0.07305608689785004\n",
      "iteration 29700, loss 0.12122486531734467\n",
      "TEST accuracy at end of epoch 75: 0.8827\n",
      "TRAIN accuracy at end of epoch 75: 0.98\n",
      "                          epoch 76\n",
      "iteration 29800, loss 0.0738745629787445\n",
      "iteration 29900, loss 0.047327954322099686\n",
      "iteration 30000, loss 0.0903945043683052\n",
      "iteration 30100, loss 0.1007806807756424\n",
      "TEST accuracy at end of epoch 76: 0.8846\n",
      "TRAIN accuracy at end of epoch 76: 0.95\n",
      "                          epoch 77\n",
      "iteration 30200, loss 0.06601579487323761\n",
      "iteration 30300, loss 0.10209822654724121\n",
      "iteration 30400, loss 0.03783973306417465\n",
      "TEST accuracy at end of epoch 77: 0.8717\n",
      "TRAIN accuracy at end of epoch 77: 0.97\n",
      "                          epoch 78\n",
      "iteration 30500, loss 0.10325194150209427\n",
      "iteration 30600, loss 0.03891889005899429\n",
      "iteration 30700, loss 0.0621052160859108\n",
      "iteration 30800, loss 0.0581372007727623\n",
      "TEST accuracy at end of epoch 78: 0.8722\n",
      "TRAIN accuracy at end of epoch 78: 0.97\n",
      "                          epoch 79\n",
      "iteration 30900, loss 0.07678462564945221\n",
      "iteration 31000, loss 0.07003052532672882\n",
      "iteration 31100, loss 0.02721703238785267\n",
      "iteration 31200, loss 0.11994390934705734\n",
      "TEST accuracy at end of epoch 79: 0.8859\n",
      "TRAIN accuracy at end of epoch 79: 0.99\n",
      "                          epoch 80\n",
      "iteration 31300, loss 0.06793829798698425\n",
      "iteration 31400, loss 0.10814046859741211\n",
      "iteration 31500, loss 0.2496209442615509\n",
      "iteration 31600, loss 0.05509591102600098\n",
      "TEST accuracy at end of epoch 80: 0.887\n",
      "TRAIN accuracy at end of epoch 80: 0.99\n",
      "                          epoch 81\n",
      "iteration 31700, loss 0.0806720107793808\n",
      "iteration 31800, loss 0.13721388578414917\n",
      "iteration 31900, loss 0.14861926436424255\n",
      "iteration 32000, loss 0.04130755364894867\n",
      "TEST accuracy at end of epoch 81: 0.8905\n",
      "TRAIN accuracy at end of epoch 81: 0.98\n",
      "                          epoch 82\n",
      "iteration 32100, loss 0.029155224561691284\n",
      "iteration 32200, loss 0.05919383466243744\n",
      "iteration 32300, loss 0.06896763294935226\n",
      "iteration 32400, loss 0.03650309145450592\n",
      "TEST accuracy at end of epoch 82: 0.8869\n",
      "TRAIN accuracy at end of epoch 82: 0.97\n",
      "                          epoch 83\n",
      "iteration 32500, loss 0.07632671296596527\n",
      "iteration 32600, loss 0.046141114085912704\n",
      "iteration 32700, loss 0.008355454541742802\n",
      "iteration 32800, loss 0.0468120202422142\n",
      "TEST accuracy at end of epoch 83: 0.8944\n",
      "TRAIN accuracy at end of epoch 83: 0.98\n",
      "                          epoch 84\n",
      "iteration 32900, loss 0.09997839480638504\n",
      "iteration 33000, loss 0.08945693075656891\n",
      "iteration 33100, loss 0.016010962426662445\n",
      "iteration 33200, loss 0.10816112160682678\n",
      "TEST accuracy at end of epoch 84: 0.8861\n",
      "TRAIN accuracy at end of epoch 84: 0.98\n",
      "                          epoch 85\n",
      "iteration 33300, loss 0.07643076777458191\n",
      "iteration 33400, loss 0.07687609642744064\n",
      "iteration 33500, loss 0.07840199023485184\n",
      "iteration 33600, loss 0.08763228356838226\n",
      "TEST accuracy at end of epoch 85: 0.8948\n",
      "TRAIN accuracy at end of epoch 85: 0.99\n",
      "                          epoch 86\n",
      "iteration 33700, loss 0.027663959190249443\n",
      "iteration 33800, loss 0.030531203374266624\n",
      "iteration 33900, loss 0.08347983658313751\n",
      "iteration 34000, loss 0.06919540464878082\n",
      "TEST accuracy at end of epoch 86: 0.8912\n",
      "TRAIN accuracy at end of epoch 86: 0.97\n",
      "                          epoch 87\n",
      "iteration 34100, loss 0.0811840072274208\n",
      "iteration 34200, loss 0.05304740369319916\n",
      "iteration 34300, loss 0.11836293339729309\n",
      "iteration 34400, loss 0.17755253612995148\n",
      "TEST accuracy at end of epoch 87: 0.8874\n",
      "TRAIN accuracy at end of epoch 87: 0.98\n",
      "                          epoch 88\n",
      "iteration 34500, loss 0.05769650265574455\n",
      "iteration 34600, loss 0.03256002441048622\n",
      "iteration 34700, loss 0.027115050703287125\n",
      "TEST accuracy at end of epoch 88: 0.8849\n",
      "TRAIN accuracy at end of epoch 88: 0.95\n",
      "                          epoch 89\n",
      "iteration 34800, loss 0.044232286512851715\n",
      "iteration 34900, loss 0.09043522924184799\n",
      "iteration 35000, loss 0.07775335758924484\n",
      "iteration 35100, loss 0.06305278837680817\n",
      "TEST accuracy at end of epoch 89: 0.901\n",
      "TRAIN accuracy at end of epoch 89: 1.0\n",
      "                          epoch 90\n",
      "iteration 35200, loss 0.11271598190069199\n",
      "iteration 35300, loss 0.06975265592336655\n",
      "iteration 35400, loss 0.027164068073034286\n",
      "iteration 35500, loss 0.08022714406251907\n",
      "TEST accuracy at end of epoch 90: 0.8828\n",
      "TRAIN accuracy at end of epoch 90: 0.96\n",
      "                          epoch 91\n",
      "iteration 35600, loss 0.06497962027788162\n",
      "iteration 35700, loss 0.06342679262161255\n",
      "iteration 35800, loss 0.0625859797000885\n",
      "iteration 35900, loss 0.08228336274623871\n",
      "TEST accuracy at end of epoch 91: 0.8971\n",
      "TRAIN accuracy at end of epoch 91: 0.98\n",
      "                          epoch 92\n",
      "iteration 36000, loss 0.04657769575715065\n",
      "iteration 36100, loss 0.022587571293115616\n",
      "iteration 36200, loss 0.12029983103275299\n",
      "iteration 36300, loss 0.01013559103012085\n",
      "TEST accuracy at end of epoch 92: 0.8887\n",
      "TRAIN accuracy at end of epoch 92: 0.98\n",
      "                          epoch 93\n",
      "iteration 36400, loss 0.08083392679691315\n",
      "iteration 36500, loss 0.07042635232210159\n",
      "iteration 36600, loss 0.05250938981771469\n",
      "iteration 36700, loss 0.09391820430755615\n",
      "TEST accuracy at end of epoch 93: 0.8908\n",
      "TRAIN accuracy at end of epoch 93: 0.98\n",
      "                          epoch 94\n",
      "iteration 36800, loss 0.06102988123893738\n",
      "iteration 36900, loss 0.038619525730609894\n",
      "iteration 37000, loss 0.06652519851922989\n",
      "iteration 37100, loss 0.03831647336483002\n",
      "TEST accuracy at end of epoch 94: 0.8798\n",
      "TRAIN accuracy at end of epoch 94: 0.95\n",
      "                          epoch 95\n",
      "iteration 37200, loss 0.03371860831975937\n",
      "iteration 37300, loss 0.03484892100095749\n",
      "iteration 37400, loss 0.01999506726861\n",
      "iteration 37500, loss 0.0569184310734272\n",
      "TEST accuracy at end of epoch 95: 0.8908\n",
      "TRAIN accuracy at end of epoch 95: 1.0\n",
      "                          epoch 96\n",
      "iteration 37600, loss 0.038073763251304626\n",
      "iteration 37700, loss 0.04134131968021393\n",
      "iteration 37800, loss 0.036937788128852844\n",
      "iteration 37900, loss 0.027019556611776352\n",
      "TEST accuracy at end of epoch 96: 0.8934\n",
      "TRAIN accuracy at end of epoch 96: 0.96\n",
      "                          epoch 97\n",
      "iteration 38000, loss 0.0828574150800705\n",
      "iteration 38100, loss 0.1003866046667099\n",
      "iteration 38200, loss 0.05670846626162529\n",
      "iteration 38300, loss 0.056134819984436035\n",
      "TEST accuracy at end of epoch 97: 0.8736\n",
      "TRAIN accuracy at end of epoch 97: 0.96\n",
      "                          epoch 98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 38400, loss 0.034003183245658875\n",
      "iteration 38500, loss 0.03904782980680466\n",
      "iteration 38600, loss 0.033158764243125916\n",
      "iteration 38700, loss 0.08717436343431473\n",
      "TEST accuracy at end of epoch 98: 0.8906\n",
      "TRAIN accuracy at end of epoch 98: 0.98\n",
      "                          epoch 99\n",
      "iteration 38800, loss 0.043639328330755234\n",
      "iteration 38900, loss 0.035061873495578766\n",
      "iteration 39000, loss 0.020043347030878067\n",
      "iteration 39100, loss 0.05096746236085892\n",
      "TEST accuracy at end of epoch 99: 0.8989\n",
      "TRAIN accuracy at end of epoch 99: 1.0\n",
      "                          epoch 100\n",
      "iteration 39200, loss 0.046513982117176056\n",
      "iteration 39300, loss 0.08206456899642944\n",
      "iteration 39400, loss 0.07091452926397324\n",
      "TEST accuracy at end of epoch 100: 0.8948\n",
      "TRAIN accuracy at end of epoch 100: 1.0\n",
      "                          epoch 101\n",
      "iteration 39500, loss 0.022845424711704254\n",
      "iteration 39600, loss 0.07861514389514923\n",
      "iteration 39700, loss 0.04326840117573738\n",
      "iteration 39800, loss 0.046732425689697266\n",
      "TEST accuracy at end of epoch 101: 0.8929\n",
      "TRAIN accuracy at end of epoch 101: 0.99\n",
      "                          epoch 102\n",
      "iteration 39900, loss 0.00820108875632286\n",
      "iteration 40000, loss 0.03708873689174652\n",
      "iteration 40100, loss 0.019293393939733505\n",
      "iteration 40200, loss 0.07782545685768127\n",
      "TEST accuracy at end of epoch 102: 0.8986\n",
      "TRAIN accuracy at end of epoch 102: 1.0\n",
      "                          epoch 103\n",
      "iteration 40300, loss 0.08515097200870514\n",
      "iteration 40400, loss 0.01741890422999859\n",
      "iteration 40500, loss 0.04071963578462601\n",
      "iteration 40600, loss 0.020555712282657623\n",
      "TEST accuracy at end of epoch 103: 0.8991\n",
      "TRAIN accuracy at end of epoch 103: 0.97\n",
      "                          epoch 104\n",
      "iteration 40700, loss 0.05805923417210579\n",
      "iteration 40800, loss 0.03480967879295349\n",
      "iteration 40900, loss 0.03947892412543297\n",
      "iteration 41000, loss 0.04477175325155258\n",
      "TEST accuracy at end of epoch 104: 0.8869\n",
      "TRAIN accuracy at end of epoch 104: 0.98\n",
      "                          epoch 105\n",
      "iteration 41100, loss 0.08935791254043579\n",
      "iteration 41200, loss 0.018711593002080917\n",
      "iteration 41300, loss 0.15982654690742493\n",
      "iteration 41400, loss 0.04989788681268692\n",
      "TEST accuracy at end of epoch 105: 0.8932\n",
      "TRAIN accuracy at end of epoch 105: 0.99\n",
      "                          epoch 106\n",
      "iteration 41500, loss 0.013053450733423233\n",
      "iteration 41600, loss 0.10403493046760559\n",
      "iteration 41700, loss 0.010847367346286774\n",
      "iteration 41800, loss 0.038607753813266754\n",
      "TEST accuracy at end of epoch 106: 0.9042\n",
      "TRAIN accuracy at end of epoch 106: 0.99\n",
      "                          epoch 107\n",
      "iteration 41900, loss 0.027465710416436195\n",
      "iteration 42000, loss 0.0889330580830574\n",
      "iteration 42100, loss 0.07763062417507172\n",
      "iteration 42200, loss 0.01475487556308508\n",
      "TEST accuracy at end of epoch 107: 0.8985\n",
      "TRAIN accuracy at end of epoch 107: 0.99\n",
      "                          epoch 108\n",
      "iteration 42300, loss 0.0566113144159317\n",
      "iteration 42400, loss 0.057267092168331146\n",
      "iteration 42500, loss 0.014109411276876926\n",
      "iteration 42600, loss 0.01251215673983097\n",
      "TEST accuracy at end of epoch 108: 0.8969\n",
      "TRAIN accuracy at end of epoch 108: 0.99\n",
      "                          epoch 109\n",
      "iteration 42700, loss 0.10139439254999161\n",
      "iteration 42800, loss 0.04456270486116409\n",
      "iteration 42900, loss 0.04508375748991966\n",
      "iteration 43000, loss 0.027158737182617188\n",
      "TEST accuracy at end of epoch 109: 0.8978\n",
      "TRAIN accuracy at end of epoch 109: 0.99\n",
      "                          epoch 110\n",
      "iteration 43100, loss 0.02545713260769844\n",
      "iteration 43200, loss 0.03545922040939331\n",
      "iteration 43300, loss 0.042903121560811996\n",
      "iteration 43400, loss 0.028906062245368958\n",
      "TEST accuracy at end of epoch 110: 0.8917\n",
      "TRAIN accuracy at end of epoch 110: 0.97\n",
      "                          epoch 111\n",
      "iteration 43500, loss 0.027784641832113266\n",
      "iteration 43600, loss 0.035529449582099915\n",
      "iteration 43700, loss 0.09006870537996292\n",
      "TEST accuracy at end of epoch 111: 0.8912\n",
      "TRAIN accuracy at end of epoch 111: 0.97\n",
      "                          epoch 112\n",
      "iteration 43800, loss 0.011508512310683727\n",
      "iteration 43900, loss 0.015102198347449303\n",
      "iteration 44000, loss 0.016007624566555023\n",
      "iteration 44100, loss 0.034674957394599915\n",
      "TEST accuracy at end of epoch 112: 0.8976\n",
      "TRAIN accuracy at end of epoch 112: 0.99\n",
      "                          epoch 113\n",
      "iteration 44200, loss 0.027558136731386185\n",
      "iteration 44300, loss 0.013633191585540771\n",
      "iteration 44400, loss 0.04692348092794418\n",
      "iteration 44500, loss 0.006654317956417799\n",
      "TEST accuracy at end of epoch 113: 0.8996\n",
      "TRAIN accuracy at end of epoch 113: 0.99\n",
      "                          epoch 114\n",
      "iteration 44600, loss 0.01765819452702999\n",
      "iteration 44700, loss 0.03885975107550621\n",
      "iteration 44800, loss 0.03117741085588932\n",
      "iteration 44900, loss 0.06151428818702698\n",
      "TEST accuracy at end of epoch 114: 0.8963\n",
      "TRAIN accuracy at end of epoch 114: 1.0\n",
      "                          epoch 115\n",
      "iteration 45000, loss 0.015809904783964157\n",
      "iteration 45100, loss 0.016305692493915558\n",
      "iteration 45200, loss 0.07658223807811737\n",
      "iteration 45300, loss 0.03851494938135147\n",
      "TEST accuracy at end of epoch 115: 0.9002\n",
      "TRAIN accuracy at end of epoch 115: 1.0\n",
      "                          epoch 116\n",
      "iteration 45400, loss 0.021267129108309746\n",
      "iteration 45500, loss 0.05428445339202881\n",
      "iteration 45600, loss 0.015109434723854065\n",
      "iteration 45700, loss 0.019996244460344315\n",
      "TEST accuracy at end of epoch 116: 0.9031\n",
      "TRAIN accuracy at end of epoch 116: 0.99\n",
      "                          epoch 117\n",
      "iteration 45800, loss 0.018953267484903336\n",
      "iteration 45900, loss 0.02752474509179592\n",
      "iteration 46000, loss 0.04831743240356445\n",
      "iteration 46100, loss 0.03865759074687958\n",
      "TEST accuracy at end of epoch 117: 0.9056\n",
      "TRAIN accuracy at end of epoch 117: 1.0\n",
      "                          epoch 118\n",
      "iteration 46200, loss 0.019990071654319763\n",
      "iteration 46300, loss 0.024294622242450714\n",
      "iteration 46400, loss 0.0038098464719951153\n",
      "iteration 46500, loss 0.09350235760211945\n",
      "TEST accuracy at end of epoch 118: 0.9032\n",
      "TRAIN accuracy at end of epoch 118: 0.99\n",
      "                          epoch 119\n",
      "iteration 46600, loss 0.05248665064573288\n",
      "iteration 46700, loss 0.05147675797343254\n",
      "iteration 46800, loss 0.04158405214548111\n",
      "iteration 46900, loss 0.019181031733751297\n",
      "TEST accuracy at end of epoch 119: 0.9017\n",
      "TRAIN accuracy at end of epoch 119: 1.0\n",
      "                          epoch 120\n",
      "iteration 47000, loss 0.031153153628110886\n",
      "iteration 47100, loss 0.023823795840144157\n",
      "iteration 47200, loss 0.04210134223103523\n",
      "iteration 47300, loss 0.042821794748306274\n",
      "TEST accuracy at end of epoch 120: 0.9008\n",
      "TRAIN accuracy at end of epoch 120: 1.0\n",
      "                          epoch 121\n",
      "iteration 47400, loss 0.05595933273434639\n",
      "iteration 47500, loss 0.07142139971256256\n",
      "iteration 47600, loss 0.028720999136567116\n",
      "iteration 47700, loss 0.0049867816269397736\n",
      "TEST accuracy at end of epoch 121: 0.8977\n",
      "TRAIN accuracy at end of epoch 121: 0.98\n",
      "                          epoch 122\n",
      "iteration 47800, loss 0.043083321303129196\n",
      "iteration 47900, loss 0.007309981156140566\n",
      "iteration 48000, loss 0.0741974338889122\n",
      "TEST accuracy at end of epoch 122: 0.8969\n",
      "TRAIN accuracy at end of epoch 122: 1.0\n",
      "                          epoch 123\n",
      "iteration 48100, loss 0.04504463076591492\n",
      "iteration 48200, loss 0.025754455476999283\n",
      "iteration 48300, loss 0.12146558612585068\n",
      "iteration 48400, loss 0.027078639715909958\n",
      "TEST accuracy at end of epoch 123: 0.8936\n",
      "TRAIN accuracy at end of epoch 123: 0.99\n",
      "                          epoch 124\n",
      "iteration 48500, loss 0.08533425629138947\n",
      "iteration 48600, loss 0.035816896706819534\n",
      "iteration 48700, loss 0.009628585539758205\n",
      "iteration 48800, loss 0.0883570984005928\n",
      "TEST accuracy at end of epoch 124: 0.9031\n",
      "TRAIN accuracy at end of epoch 124: 1.0\n",
      "                          epoch 125\n",
      "iteration 48900, loss 0.03792411834001541\n",
      "iteration 49000, loss 0.0076852282509207726\n",
      "iteration 49100, loss 0.05360126122832298\n",
      "iteration 49200, loss 0.056291770190000534\n",
      "TEST accuracy at end of epoch 125: 0.9016\n",
      "TRAIN accuracy at end of epoch 125: 0.99\n",
      "                          epoch 126\n",
      "iteration 49300, loss 0.022463569417595863\n",
      "iteration 49400, loss 0.018333973363041878\n",
      "iteration 49500, loss 0.02029109001159668\n",
      "iteration 49600, loss 0.0026835231110453606\n",
      "TEST accuracy at end of epoch 126: 0.9048\n",
      "TRAIN accuracy at end of epoch 126: 0.98\n",
      "                          epoch 127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 49700, loss 0.0344739593565464\n",
      "iteration 49800, loss 0.033719439059495926\n",
      "iteration 49900, loss 0.011914231814444065\n",
      "iteration 50000, loss 0.015459311194717884\n",
      "TEST accuracy at end of epoch 127: 0.9036\n",
      "TRAIN accuracy at end of epoch 127: 1.0\n",
      "                          epoch 128\n",
      "iteration 50100, loss 0.020348861813545227\n",
      "iteration 50200, loss 0.02052624523639679\n",
      "iteration 50300, loss 0.03298553079366684\n",
      "iteration 50400, loss 0.013276014477014542\n",
      "TEST accuracy at end of epoch 128: 0.9022\n",
      "TRAIN accuracy at end of epoch 128: 0.99\n",
      "                          epoch 129\n",
      "iteration 50500, loss 0.08444961160421371\n",
      "iteration 50600, loss 0.00454888679087162\n",
      "iteration 50700, loss 0.008344855159521103\n",
      "iteration 50800, loss 0.014368366450071335\n",
      "TEST accuracy at end of epoch 129: 0.9031\n",
      "TRAIN accuracy at end of epoch 129: 1.0\n",
      "                          epoch 130\n",
      "iteration 50900, loss 0.02567857876420021\n",
      "iteration 51000, loss 0.045232877135276794\n",
      "iteration 51100, loss 0.05290087312459946\n",
      "iteration 51200, loss 0.03304808586835861\n",
      "TEST accuracy at end of epoch 130: 0.9023\n",
      "TRAIN accuracy at end of epoch 130: 1.0\n",
      "                          epoch 131\n",
      "iteration 51300, loss 0.0058404188603162766\n",
      "iteration 51400, loss 0.019384723156690598\n",
      "iteration 51500, loss 0.010825366713106632\n",
      "iteration 51600, loss 0.0658949688076973\n",
      "TEST accuracy at end of epoch 131: 0.9014\n",
      "TRAIN accuracy at end of epoch 131: 0.98\n",
      "                          epoch 132\n",
      "iteration 51700, loss 0.0073882839642465115\n",
      "iteration 51800, loss 0.017592893913388252\n",
      "iteration 51900, loss 0.004179678857326508\n",
      "iteration 52000, loss 0.016071679070591927\n",
      "TEST accuracy at end of epoch 132: 0.9048\n",
      "TRAIN accuracy at end of epoch 132: 1.0\n",
      "                          epoch 133\n",
      "iteration 52100, loss 0.011572269722819328\n",
      "iteration 52200, loss 0.027439367026090622\n",
      "iteration 52300, loss 0.0063614388927817345\n",
      "TEST accuracy at end of epoch 133: 0.8975\n",
      "TRAIN accuracy at end of epoch 133: 1.0\n",
      "                          epoch 134\n",
      "iteration 52400, loss 0.01084817759692669\n",
      "iteration 52500, loss 0.0061429208144545555\n",
      "iteration 52600, loss 0.002342586638405919\n",
      "iteration 52700, loss 0.017731407657265663\n",
      "TEST accuracy at end of epoch 134: 0.9063\n",
      "TRAIN accuracy at end of epoch 134: 1.0\n",
      "                          epoch 135\n",
      "iteration 52800, loss 0.01571016013622284\n",
      "iteration 52900, loss 0.07998983561992645\n",
      "iteration 53000, loss 0.015964411199092865\n",
      "iteration 53100, loss 0.028189662843942642\n",
      "TEST accuracy at end of epoch 135: 0.9059\n",
      "TRAIN accuracy at end of epoch 135: 1.0\n",
      "                          epoch 136\n",
      "iteration 53200, loss 0.014368034899234772\n",
      "iteration 53300, loss 0.02331417240202427\n",
      "iteration 53400, loss 0.01508733257651329\n",
      "iteration 53500, loss 0.013794862665235996\n",
      "TEST accuracy at end of epoch 136: 0.9052\n",
      "TRAIN accuracy at end of epoch 136: 1.0\n",
      "                          epoch 137\n",
      "iteration 53600, loss 0.06506571918725967\n",
      "iteration 53700, loss 0.009988700039684772\n",
      "iteration 53800, loss 0.08021984994411469\n",
      "iteration 53900, loss 0.012535854242742062\n",
      "TEST accuracy at end of epoch 137: 0.9046\n",
      "TRAIN accuracy at end of epoch 137: 1.0\n",
      "                          epoch 138\n",
      "iteration 54000, loss 0.007828295230865479\n",
      "iteration 54100, loss 0.006327047012746334\n",
      "iteration 54200, loss 0.015633229166269302\n",
      "iteration 54300, loss 0.004053670912981033\n",
      "TEST accuracy at end of epoch 138: 0.9068\n",
      "TRAIN accuracy at end of epoch 138: 1.0\n",
      "                          epoch 139\n",
      "iteration 54400, loss 0.004287022165954113\n",
      "iteration 54500, loss 0.05165436863899231\n",
      "iteration 54600, loss 0.052160680294036865\n",
      "iteration 54700, loss 0.017364908009767532\n",
      "TEST accuracy at end of epoch 139: 0.9023\n",
      "TRAIN accuracy at end of epoch 139: 1.0\n",
      "                          epoch 140\n",
      "iteration 54800, loss 0.019996991381049156\n",
      "iteration 54900, loss 0.002272421959787607\n",
      "iteration 55000, loss 0.009565474465489388\n",
      "iteration 55100, loss 0.011331847868859768\n",
      "TEST accuracy at end of epoch 140: 0.9081\n",
      "TRAIN accuracy at end of epoch 140: 1.0\n",
      "                          epoch 141\n",
      "iteration 55200, loss 0.005104494746774435\n",
      "iteration 55300, loss 0.012553677894175053\n",
      "iteration 55400, loss 0.02968444861471653\n",
      "iteration 55500, loss 0.05142988637089729\n",
      "TEST accuracy at end of epoch 141: 0.9033\n",
      "TRAIN accuracy at end of epoch 141: 1.0\n",
      "                          epoch 142\n",
      "iteration 55600, loss 0.01334184780716896\n",
      "iteration 55700, loss 0.0054481131955981255\n",
      "iteration 55800, loss 0.006075958721339703\n",
      "iteration 55900, loss 0.0026982221752405167\n",
      "TEST accuracy at end of epoch 142: 0.907\n",
      "TRAIN accuracy at end of epoch 142: 1.0\n",
      "                          epoch 143\n",
      "iteration 56000, loss 0.034024063497781754\n",
      "iteration 56100, loss 0.025965917855501175\n",
      "iteration 56200, loss 0.06397271156311035\n",
      "iteration 56300, loss 0.005075238645076752\n",
      "TEST accuracy at end of epoch 143: 0.9062\n",
      "TRAIN accuracy at end of epoch 143: 1.0\n",
      "                          epoch 144\n",
      "iteration 56400, loss 0.013292517513036728\n",
      "iteration 56500, loss 0.028290409594774246\n",
      "iteration 56600, loss 0.006693289615213871\n",
      "TEST accuracy at end of epoch 144: 0.9073\n",
      "TRAIN accuracy at end of epoch 144: 1.0\n",
      "                          epoch 145\n",
      "iteration 56700, loss 0.02012605406343937\n",
      "iteration 56800, loss 0.007183408364653587\n",
      "iteration 56900, loss 0.013931436464190483\n",
      "iteration 57000, loss 0.0017242252361029387\n",
      "TEST accuracy at end of epoch 145: 0.9066\n",
      "TRAIN accuracy at end of epoch 145: 1.0\n",
      "                          epoch 146\n",
      "iteration 57100, loss 0.044780321419239044\n",
      "iteration 57200, loss 0.035699550062417984\n",
      "iteration 57300, loss 0.009895479306578636\n",
      "iteration 57400, loss 0.022914644330739975\n",
      "TEST accuracy at end of epoch 146: 0.908\n",
      "TRAIN accuracy at end of epoch 146: 1.0\n",
      "                          epoch 147\n",
      "iteration 57500, loss 0.005711434409022331\n",
      "iteration 57600, loss 0.012606734409928322\n",
      "iteration 57700, loss 0.006089221686124802\n",
      "iteration 57800, loss 0.006735634990036488\n",
      "TEST accuracy at end of epoch 147: 0.9081\n",
      "TRAIN accuracy at end of epoch 147: 1.0\n",
      "                          epoch 148\n",
      "iteration 57900, loss 0.007242954336106777\n",
      "iteration 58000, loss 0.02401411160826683\n",
      "iteration 58100, loss 0.03224530071020126\n",
      "iteration 58200, loss 0.010627983137965202\n",
      "TEST accuracy at end of epoch 148: 0.9061\n",
      "TRAIN accuracy at end of epoch 148: 1.0\n",
      "                          epoch 149\n",
      "iteration 58300, loss 0.012049742974340916\n",
      "iteration 58400, loss 0.03725489228963852\n",
      "iteration 58500, loss 0.005362881347537041\n",
      "iteration 58600, loss 0.006479477975517511\n",
      "TEST accuracy at end of epoch 149: 0.9089\n",
      "TRAIN accuracy at end of epoch 149: 1.0\n",
      "                          epoch 150\n",
      "iteration 58700, loss 0.01617060787975788\n",
      "iteration 58800, loss 0.006277671083807945\n",
      "iteration 58900, loss 0.012617355212569237\n",
      "iteration 59000, loss 0.008259089663624763\n",
      "TEST accuracy at end of epoch 150: 0.9079\n",
      "TRAIN accuracy at end of epoch 150: 1.0\n",
      "                          epoch 151\n",
      "iteration 59100, loss 0.004342085216194391\n",
      "iteration 59200, loss 0.01516036968678236\n",
      "iteration 59300, loss 0.0025983559899032116\n",
      "iteration 59400, loss 0.014176549389958382\n",
      "TEST accuracy at end of epoch 151: 0.9056\n",
      "TRAIN accuracy at end of epoch 151: 1.0\n",
      "                          epoch 152\n",
      "iteration 59500, loss 0.01058926247060299\n",
      "iteration 59600, loss 0.001975302817299962\n",
      "iteration 59700, loss 0.02166837267577648\n",
      "iteration 59800, loss 0.005714168772101402\n",
      "TEST accuracy at end of epoch 152: 0.9075\n",
      "TRAIN accuracy at end of epoch 152: 0.99\n",
      "                          epoch 153\n",
      "iteration 59900, loss 0.0027239781338721514\n",
      "iteration 60000, loss 0.038487691432237625\n",
      "iteration 60100, loss 0.007446177303791046\n",
      "iteration 60200, loss 0.0020526715088635683\n",
      "TEST accuracy at end of epoch 153: 0.9075\n",
      "TRAIN accuracy at end of epoch 153: 1.0\n",
      "                          epoch 154\n",
      "iteration 60300, loss 0.002567082876339555\n",
      "iteration 60400, loss 0.007418661378324032\n",
      "iteration 60500, loss 0.006402261089533567\n",
      "iteration 60600, loss 0.007779882289469242\n",
      "TEST accuracy at end of epoch 154: 0.9082\n",
      "TRAIN accuracy at end of epoch 154: 1.0\n",
      "                          epoch 155\n",
      "iteration 60700, loss 0.007488311268389225\n",
      "iteration 60800, loss 0.006544650532305241\n",
      "iteration 60900, loss 0.017558032646775246\n",
      "TEST accuracy at end of epoch 155: 0.9072\n",
      "TRAIN accuracy at end of epoch 155: 1.0\n",
      "                          epoch 156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 61000, loss 0.014555595815181732\n",
      "iteration 61100, loss 0.026045382022857666\n",
      "iteration 61200, loss 0.0048396289348602295\n",
      "iteration 61300, loss 0.006013980135321617\n",
      "TEST accuracy at end of epoch 156: 0.9061\n",
      "TRAIN accuracy at end of epoch 156: 1.0\n",
      "                          epoch 157\n",
      "iteration 61400, loss 0.006944745313376188\n",
      "iteration 61500, loss 0.0026539089158177376\n",
      "iteration 61600, loss 0.025546330958604813\n",
      "iteration 61700, loss 0.00310659222304821\n",
      "TEST accuracy at end of epoch 157: 0.9085\n",
      "TRAIN accuracy at end of epoch 157: 1.0\n",
      "                          epoch 158\n",
      "iteration 61800, loss 0.005536391865462065\n",
      "iteration 61900, loss 0.003282152581959963\n",
      "iteration 62000, loss 0.010669190436601639\n",
      "iteration 62100, loss 0.02283639647066593\n",
      "TEST accuracy at end of epoch 158: 0.9065\n",
      "TRAIN accuracy at end of epoch 158: 1.0\n",
      "                          epoch 159\n",
      "iteration 62200, loss 0.0021528524812310934\n",
      "iteration 62300, loss 0.016457606106996536\n",
      "iteration 62400, loss 0.012210560962557793\n",
      "iteration 62500, loss 0.004545542877167463\n",
      "TEST accuracy at end of epoch 159: 0.9083\n",
      "TRAIN accuracy at end of epoch 159: 1.0\n",
      "                          epoch 160\n",
      "iteration 62600, loss 0.004576199222356081\n",
      "iteration 62700, loss 0.012129828333854675\n",
      "iteration 62800, loss 0.001787447021342814\n",
      "iteration 62900, loss 0.02526550367474556\n",
      "TEST accuracy at end of epoch 160: 0.9097\n",
      "TRAIN accuracy at end of epoch 160: 0.98\n",
      "                          epoch 161\n",
      "iteration 63000, loss 0.012169869616627693\n",
      "iteration 63100, loss 0.009193860925734043\n",
      "iteration 63200, loss 0.009007416665554047\n",
      "iteration 63300, loss 0.007177297025918961\n",
      "TEST accuracy at end of epoch 161: 0.9093\n",
      "TRAIN accuracy at end of epoch 161: 1.0\n",
      "                          epoch 162\n",
      "iteration 63400, loss 0.00421805027872324\n",
      "iteration 63500, loss 0.0060379114001989365\n",
      "iteration 63600, loss 0.03080420382320881\n",
      "iteration 63700, loss 0.006111796945333481\n",
      "TEST accuracy at end of epoch 162: 0.9098\n",
      "TRAIN accuracy at end of epoch 162: 1.0\n",
      "                          epoch 163\n",
      "iteration 63800, loss 0.0007605490391142666\n",
      "iteration 63900, loss 0.018160449340939522\n",
      "iteration 64000, loss 0.011554047465324402\n",
      "iteration 64100, loss 0.013341613113880157\n",
      "TEST accuracy at end of epoch 163: 0.9084\n",
      "TRAIN accuracy at end of epoch 163: 1.0\n",
      "                          epoch 164\n",
      "iteration 64200, loss 0.0031082460191100836\n",
      "iteration 64300, loss 0.016975603997707367\n",
      "iteration 64400, loss 0.017544809728860855\n",
      "iteration 64500, loss 0.004715429153293371\n",
      "TEST accuracy at end of epoch 164: 0.9095\n",
      "TRAIN accuracy at end of epoch 164: 1.0\n",
      "                          epoch 165\n",
      "iteration 64600, loss 0.0057142022997140884\n",
      "iteration 64700, loss 0.007910685613751411\n",
      "iteration 64800, loss 0.0023470502346754074\n",
      "iteration 64900, loss 0.012273154221475124\n",
      "TEST accuracy at end of epoch 165: 0.9097\n",
      "TRAIN accuracy at end of epoch 165: 1.0\n",
      "                          epoch 166\n",
      "iteration 65000, loss 0.0028154226019978523\n",
      "iteration 65100, loss 0.014365135692059994\n",
      "iteration 65200, loss 0.010382140055298805\n",
      "TEST accuracy at end of epoch 166: 0.9099\n",
      "TRAIN accuracy at end of epoch 166: 1.0\n",
      "                          epoch 167\n",
      "iteration 65300, loss 0.0023224735632538795\n",
      "iteration 65400, loss 0.02109282836318016\n",
      "iteration 65500, loss 0.006029550451785326\n",
      "iteration 65600, loss 0.015748273581266403\n",
      "TEST accuracy at end of epoch 167: 0.9107\n",
      "TRAIN accuracy at end of epoch 167: 1.0\n",
      "                          epoch 168\n",
      "iteration 65700, loss 0.015095939859747887\n",
      "iteration 65800, loss 0.022700032219290733\n",
      "iteration 65900, loss 0.005851273890584707\n",
      "iteration 66000, loss 0.003973600920289755\n",
      "TEST accuracy at end of epoch 168: 0.9097\n",
      "TRAIN accuracy at end of epoch 168: 1.0\n",
      "                          epoch 169\n",
      "iteration 66100, loss 0.029255645349621773\n",
      "iteration 66200, loss 0.01479589194059372\n",
      "iteration 66300, loss 0.008596018888056278\n",
      "iteration 66400, loss 0.004163730889558792\n",
      "TEST accuracy at end of epoch 169: 0.91\n",
      "TRAIN accuracy at end of epoch 169: 1.0\n",
      "                          epoch 170\n",
      "iteration 66500, loss 0.01351295318454504\n",
      "iteration 66600, loss 0.0033191863913089037\n",
      "iteration 66700, loss 0.0033752990420907736\n",
      "iteration 66800, loss 0.011977928690612316\n",
      "TEST accuracy at end of epoch 170: 0.9088\n",
      "TRAIN accuracy at end of epoch 170: 1.0\n",
      "                          epoch 171\n",
      "iteration 66900, loss 0.009891185909509659\n",
      "iteration 67000, loss 0.0048524076119065285\n",
      "iteration 67100, loss 0.0011805599788203835\n",
      "iteration 67200, loss 0.007887010462582111\n",
      "TEST accuracy at end of epoch 171: 0.9093\n",
      "TRAIN accuracy at end of epoch 171: 1.0\n",
      "                          epoch 172\n",
      "iteration 67300, loss 0.005569476168602705\n",
      "iteration 67400, loss 0.028544560074806213\n",
      "iteration 67500, loss 0.0037992019206285477\n",
      "iteration 67600, loss 0.00538068450987339\n",
      "TEST accuracy at end of epoch 172: 0.9104\n",
      "TRAIN accuracy at end of epoch 172: 1.0\n",
      "                          epoch 173\n",
      "iteration 67700, loss 0.02446654811501503\n",
      "iteration 67800, loss 0.002756028203293681\n",
      "iteration 67900, loss 0.006274971179664135\n",
      "iteration 68000, loss 0.0036194403655827045\n",
      "TEST accuracy at end of epoch 173: 0.91\n",
      "TRAIN accuracy at end of epoch 173: 1.0\n",
      "                          epoch 174\n",
      "iteration 68100, loss 0.021155115216970444\n",
      "iteration 68200, loss 0.001832938869483769\n",
      "iteration 68300, loss 0.015365827828645706\n",
      "iteration 68400, loss 0.014841361902654171\n",
      "TEST accuracy at end of epoch 174: 0.9104\n",
      "TRAIN accuracy at end of epoch 174: 1.0\n",
      "                          epoch 175\n",
      "iteration 68500, loss 0.017126746475696564\n",
      "iteration 68600, loss 0.0052881017327308655\n",
      "iteration 68700, loss 0.001147128059528768\n",
      "iteration 68800, loss 0.002561983186751604\n",
      "TEST accuracy at end of epoch 175: 0.9098\n",
      "TRAIN accuracy at end of epoch 175: 0.99\n",
      "                          epoch 176\n",
      "iteration 68900, loss 0.0030053704977035522\n",
      "iteration 69000, loss 0.00990005861967802\n",
      "iteration 69100, loss 0.0030833573546260595\n",
      "iteration 69200, loss 0.018859706819057465\n",
      "TEST accuracy at end of epoch 176: 0.9108\n",
      "TRAIN accuracy at end of epoch 176: 1.0\n",
      "                          epoch 177\n",
      "iteration 69300, loss 0.005739700514823198\n",
      "iteration 69400, loss 0.004383266903460026\n",
      "iteration 69500, loss 0.012946957722306252\n",
      "TEST accuracy at end of epoch 177: 0.9097\n",
      "TRAIN accuracy at end of epoch 177: 1.0\n",
      "                          epoch 178\n",
      "iteration 69600, loss 0.021163057535886765\n",
      "iteration 69700, loss 0.0020349561236798763\n",
      "iteration 69800, loss 0.009708103723824024\n",
      "iteration 69900, loss 0.0034538009203970432\n",
      "TEST accuracy at end of epoch 178: 0.9101\n",
      "TRAIN accuracy at end of epoch 178: 1.0\n",
      "                          epoch 179\n",
      "iteration 70000, loss 0.006596096325665712\n",
      "iteration 70100, loss 0.008553970605134964\n",
      "iteration 70200, loss 0.0005199548904784024\n",
      "iteration 70300, loss 0.008709903806447983\n",
      "TEST accuracy at end of epoch 179: 0.9105\n",
      "TRAIN accuracy at end of epoch 179: 1.0\n",
      "                          epoch 180\n",
      "iteration 70400, loss 0.001997490646317601\n",
      "iteration 70500, loss 0.009100813418626785\n",
      "iteration 70600, loss 0.05482722446322441\n",
      "iteration 70700, loss 0.0015705679543316364\n",
      "TEST accuracy at end of epoch 180: 0.9104\n",
      "TRAIN accuracy at end of epoch 180: 1.0\n",
      "                          epoch 181\n",
      "iteration 70800, loss 0.0023462986573576927\n",
      "iteration 70900, loss 0.019541004672646523\n",
      "iteration 71000, loss 0.01701662316918373\n",
      "iteration 71100, loss 0.017320217564702034\n",
      "TEST accuracy at end of epoch 181: 0.9103\n",
      "TRAIN accuracy at end of epoch 181: 1.0\n",
      "                          epoch 182\n",
      "iteration 71200, loss 0.0045907036401331425\n",
      "iteration 71300, loss 0.0015274742618203163\n",
      "iteration 71400, loss 0.002779086120426655\n",
      "iteration 71500, loss 0.0007606008439324796\n",
      "TEST accuracy at end of epoch 182: 0.9103\n",
      "TRAIN accuracy at end of epoch 182: 1.0\n",
      "                          epoch 183\n",
      "iteration 71600, loss 0.022436100989580154\n",
      "iteration 71700, loss 0.007863646373152733\n",
      "iteration 71800, loss 0.0020670252852141857\n",
      "iteration 71900, loss 0.0038191056810319424\n",
      "TEST accuracy at end of epoch 183: 0.9107\n",
      "TRAIN accuracy at end of epoch 183: 1.0\n",
      "                          epoch 184\n",
      "iteration 72000, loss 0.008002756163477898\n",
      "iteration 72100, loss 0.0016256571980193257\n",
      "iteration 72200, loss 0.03160270303487778\n",
      "iteration 72300, loss 0.004598773550242186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST accuracy at end of epoch 184: 0.9108\n",
      "TRAIN accuracy at end of epoch 184: 1.0\n",
      "                          epoch 185\n",
      "iteration 72400, loss 0.0041822162456810474\n",
      "iteration 72500, loss 0.005800226703286171\n",
      "iteration 72600, loss 0.00826636515557766\n",
      "iteration 72700, loss 0.007197065744549036\n",
      "TEST accuracy at end of epoch 185: 0.9112\n",
      "TRAIN accuracy at end of epoch 185: 1.0\n",
      "                          epoch 186\n",
      "iteration 72800, loss 0.015705013647675514\n",
      "iteration 72900, loss 0.006591221317648888\n",
      "iteration 73000, loss 0.002521544462069869\n",
      "iteration 73100, loss 0.016002926975488663\n",
      "TEST accuracy at end of epoch 186: 0.9104\n",
      "TRAIN accuracy at end of epoch 186: 1.0\n",
      "                          epoch 187\n",
      "iteration 73200, loss 0.002434020396322012\n",
      "iteration 73300, loss 0.007564620580524206\n",
      "iteration 73400, loss 0.02593880519270897\n",
      "iteration 73500, loss 0.005636009853333235\n",
      "TEST accuracy at end of epoch 187: 0.9106\n",
      "TRAIN accuracy at end of epoch 187: 0.99\n",
      "                          epoch 188\n",
      "iteration 73600, loss 0.04727509245276451\n",
      "iteration 73700, loss 0.015201464295387268\n",
      "iteration 73800, loss 0.004492540843784809\n",
      "TEST accuracy at end of epoch 188: 0.9113\n",
      "TRAIN accuracy at end of epoch 188: 1.0\n",
      "                          epoch 189\n",
      "iteration 73900, loss 0.02115653082728386\n",
      "iteration 74000, loss 0.010069506242871284\n",
      "iteration 74100, loss 0.0026035322807729244\n",
      "iteration 74200, loss 0.006794244050979614\n",
      "TEST accuracy at end of epoch 189: 0.9105\n",
      "TRAIN accuracy at end of epoch 189: 1.0\n",
      "                          epoch 190\n",
      "iteration 74300, loss 0.002280743792653084\n",
      "iteration 74400, loss 0.03347653150558472\n",
      "iteration 74500, loss 0.01721082255244255\n",
      "iteration 74600, loss 0.0015527522191405296\n",
      "TEST accuracy at end of epoch 190: 0.9113\n",
      "TRAIN accuracy at end of epoch 190: 1.0\n",
      "                          epoch 191\n",
      "iteration 74700, loss 0.03456704691052437\n",
      "iteration 74800, loss 0.0031566573306918144\n",
      "iteration 74900, loss 0.01215367205440998\n",
      "iteration 75000, loss 0.0015813745558261871\n",
      "TEST accuracy at end of epoch 191: 0.9115\n",
      "TRAIN accuracy at end of epoch 191: 1.0\n",
      "                          epoch 192\n",
      "iteration 75100, loss 0.0013954833848401904\n",
      "iteration 75200, loss 0.002554347040131688\n",
      "iteration 75300, loss 0.001625776058062911\n",
      "iteration 75400, loss 0.019813938066363335\n",
      "TEST accuracy at end of epoch 192: 0.9111\n",
      "TRAIN accuracy at end of epoch 192: 1.0\n",
      "                          epoch 193\n",
      "iteration 75500, loss 0.012709185481071472\n",
      "iteration 75600, loss 0.0010168449953198433\n",
      "iteration 75700, loss 0.001339094014838338\n",
      "iteration 75800, loss 0.015576230362057686\n",
      "TEST accuracy at end of epoch 193: 0.9114\n",
      "TRAIN accuracy at end of epoch 193: 1.0\n",
      "                          epoch 194\n",
      "iteration 75900, loss 0.006144292652606964\n",
      "iteration 76000, loss 0.0035714111290872097\n",
      "iteration 76100, loss 0.015592467039823532\n",
      "iteration 76200, loss 0.0033631008118391037\n",
      "TEST accuracy at end of epoch 194: 0.9116\n",
      "TRAIN accuracy at end of epoch 194: 1.0\n",
      "                          epoch 195\n",
      "iteration 76300, loss 0.0013781710295006633\n",
      "iteration 76400, loss 0.0034629744477570057\n",
      "iteration 76500, loss 0.014222982339560986\n",
      "iteration 76600, loss 0.0010097032645717263\n",
      "TEST accuracy at end of epoch 195: 0.9108\n",
      "TRAIN accuracy at end of epoch 195: 1.0\n",
      "                          epoch 196\n",
      "iteration 76700, loss 0.004047459922730923\n",
      "iteration 76800, loss 0.010848050005733967\n",
      "iteration 76900, loss 0.009585446678102016\n",
      "iteration 77000, loss 0.00483830738812685\n",
      "TEST accuracy at end of epoch 196: 0.9111\n",
      "TRAIN accuracy at end of epoch 196: 1.0\n",
      "                          epoch 197\n",
      "iteration 77100, loss 0.011794163845479488\n",
      "iteration 77200, loss 0.029546473175287247\n",
      "iteration 77300, loss 0.00798437837511301\n",
      "iteration 77400, loss 0.01706756092607975\n",
      "TEST accuracy at end of epoch 197: 0.911\n",
      "TRAIN accuracy at end of epoch 197: 1.0\n",
      "                          epoch 198\n",
      "iteration 77500, loss 0.006053692661225796\n",
      "iteration 77600, loss 0.05571049451828003\n",
      "iteration 77700, loss 0.0025842066388577223\n",
      "iteration 77800, loss 0.013048287481069565\n",
      "TEST accuracy at end of epoch 198: 0.9112\n",
      "TRAIN accuracy at end of epoch 198: 1.0\n",
      "                          epoch 199\n",
      "iteration 77900, loss 0.0028681051917374134\n",
      "iteration 78000, loss 0.0037571785505861044\n",
      "iteration 78100, loss 0.01997571997344494\n",
      "iteration 78200, loss 0.020371783524751663\n",
      "TEST accuracy at end of epoch 199: 0.9108\n",
      "TRAIN accuracy at end of epoch 199: 1.0\n",
      "╰┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╯\n",
      "╭┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╮\n",
      "                         n: 9, res: False\n",
      "size of reduce_mean: (?, 1, 1, 64)\n",
      "size of squeeze: (?, 64)\n",
      "training for 78125 iterations\n",
      "                          epoch 0\n",
      "iteration 100, loss 2.202404260635376\n",
      "iteration 200, loss 2.067155361175537\n",
      "iteration 300, loss 2.016538143157959\n",
      "TEST accuracy at end of epoch 0: 0.1568\n",
      "TRAIN accuracy at end of epoch 0: 0.16\n",
      "                          epoch 1\n",
      "iteration 400, loss 2.0396933555603027\n",
      "iteration 500, loss 1.8087430000305176\n",
      "iteration 600, loss 2.0058135986328125\n",
      "iteration 700, loss 1.96464204788208\n",
      "TEST accuracy at end of epoch 1: 0.2865\n",
      "TRAIN accuracy at end of epoch 1: 0.31\n",
      "                          epoch 2\n",
      "iteration 800, loss 1.8913840055465698\n",
      "iteration 900, loss 1.8919810056686401\n",
      "iteration 1000, loss 1.7340010404586792\n",
      "iteration 1100, loss 1.6649823188781738\n",
      "TEST accuracy at end of epoch 2: 0.1406\n",
      "TRAIN accuracy at end of epoch 2: 0.11\n",
      "                          epoch 3\n",
      "iteration 1200, loss 1.684319257736206\n",
      "iteration 1300, loss 1.8772494792938232\n",
      "iteration 1400, loss 1.6508841514587402\n",
      "iteration 1500, loss 1.7197530269622803\n",
      "TEST accuracy at end of epoch 3: 0.3022\n",
      "TRAIN accuracy at end of epoch 3: 0.29\n",
      "                          epoch 4\n",
      "iteration 1600, loss 1.5813190937042236\n",
      "iteration 1700, loss 1.5000771284103394\n",
      "iteration 1800, loss 1.6488769054412842\n",
      "iteration 1900, loss 1.5924580097198486\n",
      "TEST accuracy at end of epoch 4: 0.3585\n",
      "TRAIN accuracy at end of epoch 4: 0.34\n",
      "                          epoch 5\n",
      "iteration 2000, loss 1.682570457458496\n",
      "iteration 2100, loss 1.4500999450683594\n",
      "iteration 2200, loss 1.61409592628479\n",
      "iteration 2300, loss 1.5361664295196533\n",
      "TEST accuracy at end of epoch 5: 0.3587\n",
      "TRAIN accuracy at end of epoch 5: 0.29\n",
      "                          epoch 6\n",
      "iteration 2400, loss 1.6061079502105713\n",
      "iteration 2500, loss 1.6184921264648438\n",
      "iteration 2600, loss 1.4967434406280518\n",
      "iteration 2700, loss 1.5601601600646973\n",
      "TEST accuracy at end of epoch 6: 0.4025\n",
      "TRAIN accuracy at end of epoch 6: 0.38\n",
      "                          epoch 7\n",
      "iteration 2800, loss 1.4672268629074097\n",
      "iteration 2900, loss 1.618037223815918\n",
      "iteration 3000, loss 1.5866400003433228\n",
      "iteration 3100, loss 1.5917624235153198\n",
      "TEST accuracy at end of epoch 7: 0.3707\n",
      "TRAIN accuracy at end of epoch 7: 0.35\n",
      "                          epoch 8\n",
      "iteration 3200, loss 1.4179073572158813\n",
      "iteration 3300, loss 1.4147660732269287\n",
      "iteration 3400, loss 1.5501091480255127\n",
      "iteration 3500, loss 1.4970498085021973\n",
      "TEST accuracy at end of epoch 8: 0.3985\n",
      "TRAIN accuracy at end of epoch 8: 0.45\n",
      "                          epoch 9\n",
      "iteration 3600, loss 1.656224012374878\n",
      "iteration 3700, loss 1.3921968936920166\n",
      "iteration 3800, loss 1.252502202987671\n",
      "iteration 3900, loss 1.4025218486785889\n",
      "TEST accuracy at end of epoch 9: 0.4877\n",
      "TRAIN accuracy at end of epoch 9: 0.59\n",
      "                          epoch 10\n",
      "iteration 4000, loss 1.4104647636413574\n",
      "iteration 4100, loss 1.3167283535003662\n",
      "iteration 4200, loss 1.4033727645874023\n",
      "iteration 4300, loss 1.3843512535095215\n",
      "TEST accuracy at end of epoch 10: 0.4815\n",
      "TRAIN accuracy at end of epoch 10: 0.49\n",
      "                          epoch 11\n",
      "iteration 4400, loss 1.4032800197601318\n",
      "iteration 4500, loss 1.1605368852615356\n",
      "iteration 4600, loss 1.333082914352417\n",
      "TEST accuracy at end of epoch 11: 0.5077\n",
      "TRAIN accuracy at end of epoch 11: 0.49\n",
      "                          epoch 12\n",
      "iteration 4700, loss 1.2739038467407227\n",
      "iteration 4800, loss 1.3255035877227783\n",
      "iteration 4900, loss 1.3803255558013916\n",
      "iteration 5000, loss 1.1707367897033691\n",
      "TEST accuracy at end of epoch 12: 0.3983\n",
      "TRAIN accuracy at end of epoch 12: 0.36\n",
      "                          epoch 13\n",
      "iteration 5100, loss 1.3493828773498535\n",
      "iteration 5200, loss 1.4268614053726196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5300, loss 1.3193168640136719\n",
      "iteration 5400, loss 1.3094630241394043\n",
      "TEST accuracy at end of epoch 13: 0.4684\n",
      "TRAIN accuracy at end of epoch 13: 0.42\n",
      "                          epoch 14\n",
      "iteration 5500, loss 1.1758414506912231\n",
      "iteration 5600, loss 1.0748934745788574\n",
      "iteration 5700, loss 1.1601712703704834\n",
      "iteration 5800, loss 1.3553951978683472\n",
      "TEST accuracy at end of epoch 14: 0.5575\n",
      "TRAIN accuracy at end of epoch 14: 0.59\n",
      "                          epoch 15\n",
      "iteration 5900, loss 1.024653673171997\n",
      "iteration 6000, loss 1.348629355430603\n",
      "iteration 6100, loss 1.2172579765319824\n",
      "iteration 6200, loss 1.1507117748260498\n",
      "TEST accuracy at end of epoch 15: 0.5255\n",
      "TRAIN accuracy at end of epoch 15: 0.55\n",
      "                          epoch 16\n",
      "iteration 6300, loss 0.9611028432846069\n",
      "iteration 6400, loss 1.1762807369232178\n",
      "iteration 6500, loss 1.2583999633789062\n",
      "iteration 6600, loss 1.052595853805542\n",
      "TEST accuracy at end of epoch 16: 0.4901\n",
      "TRAIN accuracy at end of epoch 16: 0.48\n",
      "                          epoch 17\n",
      "iteration 6700, loss 1.1990002393722534\n",
      "iteration 6800, loss 1.0298466682434082\n",
      "iteration 6900, loss 1.0829254388809204\n",
      "iteration 7000, loss 1.0562372207641602\n",
      "TEST accuracy at end of epoch 17: 0.5324\n",
      "TRAIN accuracy at end of epoch 17: 0.62\n",
      "                          epoch 18\n",
      "iteration 7100, loss 0.9036277532577515\n",
      "iteration 7200, loss 0.9224660396575928\n",
      "iteration 7300, loss 0.9954913854598999\n",
      "iteration 7400, loss 0.9271624088287354\n",
      "TEST accuracy at end of epoch 18: 0.6217\n",
      "TRAIN accuracy at end of epoch 18: 0.6\n",
      "                          epoch 19\n",
      "iteration 7500, loss 0.8776317834854126\n",
      "iteration 7600, loss 0.8603105545043945\n",
      "iteration 7700, loss 1.0323431491851807\n",
      "iteration 7800, loss 0.9540316462516785\n",
      "TEST accuracy at end of epoch 19: 0.5644\n",
      "TRAIN accuracy at end of epoch 19: 0.61\n",
      "                          epoch 20\n",
      "iteration 7900, loss 0.9488934278488159\n",
      "iteration 8000, loss 0.867499828338623\n",
      "iteration 8100, loss 0.9887517690658569\n",
      "iteration 8200, loss 0.9922332763671875\n",
      "TEST accuracy at end of epoch 20: 0.509\n",
      "TRAIN accuracy at end of epoch 20: 0.51\n",
      "                          epoch 21\n",
      "iteration 8300, loss 1.0530085563659668\n",
      "iteration 8400, loss 0.8527387380599976\n",
      "iteration 8500, loss 0.8035446405410767\n",
      "iteration 8600, loss 0.8332046270370483\n",
      "TEST accuracy at end of epoch 21: 0.5723\n",
      "TRAIN accuracy at end of epoch 21: 0.54\n",
      "                          epoch 22\n",
      "iteration 8700, loss 0.8098969459533691\n",
      "iteration 8800, loss 0.854041576385498\n",
      "iteration 8900, loss 0.8856698870658875\n",
      "TEST accuracy at end of epoch 22: 0.5802\n",
      "TRAIN accuracy at end of epoch 22: 0.52\n",
      "                          epoch 23\n",
      "iteration 9000, loss 0.7071260213851929\n",
      "iteration 9100, loss 0.9003981947898865\n",
      "iteration 9200, loss 0.9557915925979614\n",
      "iteration 9300, loss 0.9261136054992676\n",
      "TEST accuracy at end of epoch 23: 0.7093\n",
      "TRAIN accuracy at end of epoch 23: 0.77\n",
      "                          epoch 24\n",
      "iteration 9400, loss 0.743351936340332\n",
      "iteration 9500, loss 0.9905718564987183\n",
      "iteration 9600, loss 0.7900193929672241\n",
      "iteration 9700, loss 0.8789358139038086\n",
      "TEST accuracy at end of epoch 24: 0.6784\n",
      "TRAIN accuracy at end of epoch 24: 0.66\n",
      "                          epoch 25\n",
      "iteration 9800, loss 0.9405806064605713\n",
      "iteration 9900, loss 0.6611142754554749\n",
      "iteration 10000, loss 0.7484920024871826\n",
      "iteration 10100, loss 0.7431032061576843\n",
      "TEST accuracy at end of epoch 25: 0.6956\n",
      "TRAIN accuracy at end of epoch 25: 0.74\n",
      "                          epoch 26\n",
      "iteration 10200, loss 0.6686012744903564\n",
      "iteration 10300, loss 0.8101873397827148\n",
      "iteration 10400, loss 0.7829842567443848\n",
      "iteration 10500, loss 0.7189178466796875\n",
      "TEST accuracy at end of epoch 26: 0.5083\n",
      "TRAIN accuracy at end of epoch 26: 0.58\n",
      "                          epoch 27\n",
      "iteration 10600, loss 0.6860207319259644\n",
      "iteration 10700, loss 0.5920692682266235\n",
      "iteration 10800, loss 0.5887189507484436\n",
      "iteration 10900, loss 0.7280717492103577\n",
      "TEST accuracy at end of epoch 27: 0.7077\n",
      "TRAIN accuracy at end of epoch 27: 0.75\n",
      "                          epoch 28\n",
      "iteration 11000, loss 0.7054917812347412\n",
      "iteration 11100, loss 0.623638927936554\n",
      "iteration 11200, loss 0.7391060590744019\n",
      "iteration 11300, loss 0.6937367916107178\n",
      "TEST accuracy at end of epoch 28: 0.6687\n",
      "TRAIN accuracy at end of epoch 28: 0.69\n",
      "                          epoch 29\n",
      "iteration 11400, loss 0.7094895839691162\n",
      "iteration 11500, loss 0.7822043895721436\n",
      "iteration 11600, loss 0.6353145837783813\n",
      "iteration 11700, loss 0.685572624206543\n",
      "TEST accuracy at end of epoch 29: 0.7033\n",
      "TRAIN accuracy at end of epoch 29: 0.66\n",
      "                          epoch 30\n",
      "iteration 11800, loss 0.6843183040618896\n",
      "iteration 11900, loss 0.5977175831794739\n",
      "iteration 12000, loss 0.6187528371810913\n",
      "iteration 12100, loss 0.5640827417373657\n",
      "TEST accuracy at end of epoch 30: 0.697\n",
      "TRAIN accuracy at end of epoch 30: 0.7\n",
      "                          epoch 31\n",
      "iteration 12200, loss 0.7669090032577515\n",
      "iteration 12300, loss 0.5624986886978149\n",
      "iteration 12400, loss 0.6647058725357056\n",
      "iteration 12500, loss 0.701270580291748\n",
      "TEST accuracy at end of epoch 31: 0.6565\n",
      "TRAIN accuracy at end of epoch 31: 0.62\n",
      "                          epoch 32\n",
      "iteration 12600, loss 0.5031620264053345\n",
      "iteration 12700, loss 0.7366385459899902\n",
      "iteration 12800, loss 0.6500786542892456\n",
      "iteration 12900, loss 0.6333836913108826\n",
      "TEST accuracy at end of epoch 32: 0.6401\n",
      "TRAIN accuracy at end of epoch 32: 0.7\n",
      "                          epoch 33\n",
      "iteration 13000, loss 0.5956428647041321\n",
      "iteration 13100, loss 0.7764968872070312\n",
      "iteration 13200, loss 0.632908821105957\n",
      "TEST accuracy at end of epoch 33: 0.6361\n",
      "TRAIN accuracy at end of epoch 33: 0.6\n",
      "                          epoch 34\n",
      "iteration 13300, loss 0.746519923210144\n",
      "iteration 13400, loss 0.560914158821106\n",
      "iteration 13500, loss 0.6199265122413635\n",
      "iteration 13600, loss 0.6257002353668213\n",
      "TEST accuracy at end of epoch 34: 0.6885\n",
      "TRAIN accuracy at end of epoch 34: 0.79\n",
      "                          epoch 35\n",
      "iteration 13700, loss 0.6805135607719421\n",
      "iteration 13800, loss 0.5929034352302551\n",
      "iteration 13900, loss 0.5685129165649414\n",
      "iteration 14000, loss 0.6512174606323242\n",
      "TEST accuracy at end of epoch 35: 0.556\n",
      "TRAIN accuracy at end of epoch 35: 0.53\n",
      "                          epoch 36\n",
      "iteration 14100, loss 0.6568232774734497\n",
      "iteration 14200, loss 0.5759478807449341\n",
      "iteration 14300, loss 0.5217817425727844\n",
      "iteration 14400, loss 0.5389767289161682\n",
      "TEST accuracy at end of epoch 36: 0.7692\n",
      "TRAIN accuracy at end of epoch 36: 0.87\n",
      "                          epoch 37\n",
      "iteration 14500, loss 0.627284586429596\n",
      "iteration 14600, loss 0.6658452749252319\n",
      "iteration 14700, loss 0.6340962648391724\n",
      "iteration 14800, loss 0.46695607900619507\n",
      "TEST accuracy at end of epoch 37: 0.7522\n",
      "TRAIN accuracy at end of epoch 37: 0.75\n",
      "                          epoch 38\n",
      "iteration 14900, loss 0.6749085783958435\n",
      "iteration 15000, loss 0.5760335326194763\n",
      "iteration 15100, loss 0.550534725189209\n",
      "iteration 15200, loss 0.5915157794952393\n",
      "TEST accuracy at end of epoch 38: 0.7685\n",
      "TRAIN accuracy at end of epoch 38: 0.84\n",
      "                          epoch 39\n",
      "iteration 15300, loss 0.5666899681091309\n",
      "iteration 15400, loss 0.5220775604248047\n",
      "iteration 15500, loss 0.4639335870742798\n",
      "iteration 15600, loss 0.513636589050293\n",
      "TEST accuracy at end of epoch 39: 0.7568\n",
      "TRAIN accuracy at end of epoch 39: 0.75\n",
      "                          epoch 40\n",
      "iteration 15700, loss 0.6466476917266846\n",
      "iteration 15800, loss 0.562174379825592\n",
      "iteration 15900, loss 0.6384522914886475\n",
      "iteration 16000, loss 0.47793304920196533\n",
      "TEST accuracy at end of epoch 40: 0.6896\n",
      "TRAIN accuracy at end of epoch 40: 0.7\n",
      "                          epoch 41\n",
      "iteration 16100, loss 0.4639070928096771\n",
      "iteration 16200, loss 0.6666526794433594\n",
      "iteration 16300, loss 0.41563427448272705\n",
      "iteration 16400, loss 0.5053013563156128\n",
      "TEST accuracy at end of epoch 41: 0.7717\n",
      "TRAIN accuracy at end of epoch 41: 0.85\n",
      "                          epoch 42\n",
      "iteration 16500, loss 0.462339848279953\n",
      "iteration 16600, loss 0.5454467535018921\n",
      "iteration 16700, loss 0.5871983766555786\n",
      "iteration 16800, loss 0.5102510452270508\n",
      "TEST accuracy at end of epoch 42: 0.784\n",
      "TRAIN accuracy at end of epoch 42: 0.84\n",
      "                          epoch 43\n",
      "iteration 16900, loss 0.38762366771698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 17000, loss 0.5284136533737183\n",
      "iteration 17100, loss 0.453752338886261\n",
      "iteration 17200, loss 0.5152056813240051\n",
      "TEST accuracy at end of epoch 43: 0.7337\n",
      "TRAIN accuracy at end of epoch 43: 0.8\n",
      "                          epoch 44\n",
      "iteration 17300, loss 0.38945865631103516\n",
      "iteration 17400, loss 0.46046268939971924\n",
      "iteration 17500, loss 0.47463053464889526\n",
      "TEST accuracy at end of epoch 44: 0.7488\n",
      "TRAIN accuracy at end of epoch 44: 0.83\n",
      "                          epoch 45\n",
      "iteration 17600, loss 0.4221075177192688\n",
      "iteration 17700, loss 0.4990871250629425\n",
      "iteration 17800, loss 0.5868237018585205\n",
      "iteration 17900, loss 0.6418123245239258\n",
      "TEST accuracy at end of epoch 45: 0.7391\n",
      "TRAIN accuracy at end of epoch 45: 0.84\n",
      "                          epoch 46\n",
      "iteration 18000, loss 0.47021812200546265\n",
      "iteration 18100, loss 0.5563802719116211\n",
      "iteration 18200, loss 0.5232171416282654\n",
      "iteration 18300, loss 0.47392281889915466\n",
      "TEST accuracy at end of epoch 46: 0.7613\n",
      "TRAIN accuracy at end of epoch 46: 0.82\n",
      "                          epoch 47\n",
      "iteration 18400, loss 0.5085577964782715\n",
      "iteration 18500, loss 0.4248787462711334\n",
      "iteration 18600, loss 0.37429389357566833\n",
      "iteration 18700, loss 0.5336587429046631\n",
      "TEST accuracy at end of epoch 47: 0.8023\n",
      "TRAIN accuracy at end of epoch 47: 0.79\n",
      "                          epoch 48\n",
      "iteration 18800, loss 0.4744841158390045\n",
      "iteration 18900, loss 0.7291625738143921\n",
      "iteration 19000, loss 0.531214714050293\n",
      "iteration 19100, loss 0.44137710332870483\n",
      "TEST accuracy at end of epoch 48: 0.779\n",
      "TRAIN accuracy at end of epoch 48: 0.84\n",
      "                          epoch 49\n",
      "iteration 19200, loss 0.5112906694412231\n",
      "iteration 19300, loss 0.3956853151321411\n",
      "iteration 19400, loss 0.3981949985027313\n",
      "iteration 19500, loss 0.36064669489860535\n",
      "TEST accuracy at end of epoch 49: 0.6854\n",
      "TRAIN accuracy at end of epoch 49: 0.69\n",
      "                          epoch 50\n",
      "iteration 19600, loss 0.3628697395324707\n",
      "iteration 19700, loss 0.3795550763607025\n",
      "iteration 19800, loss 0.5707708597183228\n",
      "iteration 19900, loss 0.5371440052986145\n",
      "TEST accuracy at end of epoch 50: 0.705\n",
      "TRAIN accuracy at end of epoch 50: 0.73\n",
      "                          epoch 51\n",
      "iteration 20000, loss 0.3966653347015381\n",
      "iteration 20100, loss 0.5780096054077148\n",
      "iteration 20200, loss 0.4906512200832367\n",
      "iteration 20300, loss 0.4090908169746399\n",
      "TEST accuracy at end of epoch 51: 0.78\n",
      "TRAIN accuracy at end of epoch 51: 0.83\n",
      "                          epoch 52\n",
      "iteration 20400, loss 0.5229218006134033\n",
      "iteration 20500, loss 0.5743656754493713\n",
      "iteration 20600, loss 0.5091117024421692\n",
      "iteration 20700, loss 0.45828068256378174\n",
      "TEST accuracy at end of epoch 52: 0.7788\n",
      "TRAIN accuracy at end of epoch 52: 0.87\n",
      "                          epoch 53\n",
      "iteration 20800, loss 0.5028544068336487\n",
      "iteration 20900, loss 0.4289199113845825\n",
      "iteration 21000, loss 0.44634756445884705\n",
      "iteration 21100, loss 0.5571594834327698\n",
      "TEST accuracy at end of epoch 53: 0.783\n",
      "TRAIN accuracy at end of epoch 53: 0.73\n",
      "                          epoch 54\n",
      "iteration 21200, loss 0.4602198004722595\n",
      "iteration 21300, loss 0.37449097633361816\n",
      "iteration 21400, loss 0.46402084827423096\n",
      "iteration 21500, loss 0.4038502871990204\n",
      "TEST accuracy at end of epoch 54: 0.8093\n",
      "TRAIN accuracy at end of epoch 54: 0.88\n",
      "                          epoch 55\n",
      "iteration 21600, loss 0.39901095628738403\n",
      "iteration 21700, loss 0.3650602102279663\n",
      "iteration 21800, loss 0.41123688220977783\n",
      "TEST accuracy at end of epoch 55: 0.7774\n",
      "TRAIN accuracy at end of epoch 55: 0.85\n",
      "                          epoch 56\n",
      "iteration 21900, loss 0.3866320848464966\n",
      "iteration 22000, loss 0.44417765736579895\n",
      "iteration 22100, loss 0.4669885039329529\n",
      "iteration 22200, loss 0.32377204298973083\n",
      "TEST accuracy at end of epoch 56: 0.7844\n",
      "TRAIN accuracy at end of epoch 56: 0.75\n",
      "                          epoch 57\n",
      "iteration 22300, loss 0.4405236840248108\n",
      "iteration 22400, loss 0.32891011238098145\n",
      "iteration 22500, loss 0.44449421763420105\n",
      "iteration 22600, loss 0.43914884328842163\n",
      "TEST accuracy at end of epoch 57: 0.817\n",
      "TRAIN accuracy at end of epoch 57: 0.92\n",
      "                          epoch 58\n",
      "iteration 22700, loss 0.46331191062927246\n",
      "iteration 22800, loss 0.39454445242881775\n",
      "iteration 22900, loss 0.3151302933692932\n",
      "iteration 23000, loss 0.34462735056877136\n",
      "TEST accuracy at end of epoch 58: 0.7808\n",
      "TRAIN accuracy at end of epoch 58: 0.79\n",
      "                          epoch 59\n",
      "iteration 23100, loss 0.48954105377197266\n",
      "iteration 23200, loss 0.41631370782852173\n",
      "iteration 23300, loss 0.36921417713165283\n",
      "iteration 23400, loss 0.48340827226638794\n",
      "TEST accuracy at end of epoch 59: 0.8071\n",
      "TRAIN accuracy at end of epoch 59: 0.84\n",
      "                          epoch 60\n",
      "iteration 23500, loss 0.3746389150619507\n",
      "iteration 23600, loss 0.4528157711029053\n",
      "iteration 23700, loss 0.40020591020584106\n",
      "iteration 23800, loss 0.3820611238479614\n",
      "TEST accuracy at end of epoch 60: 0.8134\n",
      "TRAIN accuracy at end of epoch 60: 0.94\n",
      "                          epoch 61\n",
      "iteration 23900, loss 0.4786625802516937\n",
      "iteration 24000, loss 0.33746713399887085\n",
      "iteration 24100, loss 0.2802695333957672\n",
      "iteration 24200, loss 0.3783559203147888\n",
      "TEST accuracy at end of epoch 61: 0.8086\n",
      "TRAIN accuracy at end of epoch 61: 0.88\n",
      "                          epoch 62\n",
      "iteration 24300, loss 0.2962667942047119\n",
      "iteration 24400, loss 0.2654260993003845\n",
      "iteration 24500, loss 0.35348445177078247\n",
      "iteration 24600, loss 0.4666872024536133\n",
      "TEST accuracy at end of epoch 62: 0.7623\n",
      "TRAIN accuracy at end of epoch 62: 0.76\n",
      "                          epoch 63\n",
      "iteration 24700, loss 0.36035752296447754\n",
      "iteration 24800, loss 0.30230528116226196\n",
      "iteration 24900, loss 0.48019641637802124\n",
      "iteration 25000, loss 0.36795103549957275\n",
      "TEST accuracy at end of epoch 63: 0.7881\n",
      "TRAIN accuracy at end of epoch 63: 0.83\n",
      "                          epoch 64\n",
      "iteration 25100, loss 0.31691545248031616\n",
      "iteration 25200, loss 0.4332558810710907\n",
      "iteration 25300, loss 0.31681519746780396\n",
      "iteration 25400, loss 0.3600162863731384\n",
      "TEST accuracy at end of epoch 64: 0.8173\n",
      "TRAIN accuracy at end of epoch 64: 0.87\n",
      "                          epoch 65\n",
      "iteration 25500, loss 0.38770580291748047\n",
      "iteration 25600, loss 0.4181234836578369\n",
      "iteration 25700, loss 0.42039138078689575\n",
      "iteration 25800, loss 0.278651624917984\n",
      "TEST accuracy at end of epoch 65: 0.7997\n",
      "TRAIN accuracy at end of epoch 65: 0.87\n",
      "                          epoch 66\n",
      "iteration 25900, loss 0.2827030420303345\n",
      "iteration 26000, loss 0.3745521306991577\n",
      "iteration 26100, loss 0.31601667404174805\n",
      "TEST accuracy at end of epoch 66: 0.8264\n",
      "TRAIN accuracy at end of epoch 66: 0.8\n",
      "                          epoch 67\n",
      "iteration 26200, loss 0.3238755464553833\n",
      "iteration 26300, loss 0.36765605211257935\n",
      "iteration 26400, loss 0.2560739517211914\n",
      "iteration 26500, loss 0.31641334295272827\n",
      "TEST accuracy at end of epoch 67: 0.7689\n",
      "TRAIN accuracy at end of epoch 67: 0.82\n",
      "                          epoch 68\n",
      "iteration 26600, loss 0.27065110206604004\n",
      "iteration 26700, loss 0.392824649810791\n",
      "iteration 26800, loss 0.2884226441383362\n",
      "iteration 26900, loss 0.4293742775917053\n",
      "TEST accuracy at end of epoch 68: 0.8238\n",
      "TRAIN accuracy at end of epoch 68: 0.88\n",
      "                          epoch 69\n",
      "iteration 27000, loss 0.28078898787498474\n",
      "iteration 27100, loss 0.28049612045288086\n",
      "iteration 27200, loss 0.3437075912952423\n",
      "iteration 27300, loss 0.3110848665237427\n",
      "TEST accuracy at end of epoch 69: 0.8076\n",
      "TRAIN accuracy at end of epoch 69: 0.9\n",
      "                          epoch 70\n",
      "iteration 27400, loss 0.3895750939846039\n",
      "iteration 27500, loss 0.28260529041290283\n",
      "iteration 27600, loss 0.3090519905090332\n",
      "iteration 27700, loss 0.43130648136138916\n",
      "TEST accuracy at end of epoch 70: 0.7903\n",
      "TRAIN accuracy at end of epoch 70: 0.83\n",
      "                          epoch 71\n",
      "iteration 27800, loss 0.19112345576286316\n",
      "iteration 27900, loss 0.36312049627304077\n",
      "iteration 28000, loss 0.35096654295921326\n",
      "iteration 28100, loss 0.23573067784309387\n",
      "TEST accuracy at end of epoch 71: 0.7179\n",
      "TRAIN accuracy at end of epoch 71: 0.79\n",
      "                          epoch 72\n",
      "iteration 28200, loss 0.32532376050949097\n",
      "iteration 28300, loss 0.38054704666137695\n",
      "iteration 28400, loss 0.3669183850288391\n",
      "iteration 28500, loss 0.3306073546409607\n",
      "TEST accuracy at end of epoch 72: 0.8126\n",
      "TRAIN accuracy at end of epoch 72: 0.82\n",
      "                          epoch 73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 28600, loss 0.36085188388824463\n",
      "iteration 28700, loss 0.23023469746112823\n",
      "iteration 28800, loss 0.3479803502559662\n",
      "iteration 28900, loss 0.34007084369659424\n",
      "TEST accuracy at end of epoch 73: 0.8195\n",
      "TRAIN accuracy at end of epoch 73: 0.9\n",
      "                          epoch 74\n",
      "iteration 29000, loss 0.32250815629959106\n",
      "iteration 29100, loss 0.3313613533973694\n",
      "iteration 29200, loss 0.2911568284034729\n",
      "iteration 29300, loss 0.3652699589729309\n",
      "TEST accuracy at end of epoch 74: 0.8092\n",
      "TRAIN accuracy at end of epoch 74: 0.88\n",
      "                          epoch 75\n",
      "iteration 29400, loss 0.3309026062488556\n",
      "iteration 29500, loss 0.33595770597457886\n",
      "iteration 29600, loss 0.4190617501735687\n",
      "iteration 29700, loss 0.2664419412612915\n",
      "TEST accuracy at end of epoch 75: 0.8273\n",
      "TRAIN accuracy at end of epoch 75: 0.94\n",
      "                          epoch 76\n",
      "iteration 29800, loss 0.3785754442214966\n",
      "iteration 29900, loss 0.446376234292984\n",
      "iteration 30000, loss 0.2273026406764984\n",
      "iteration 30100, loss 0.34939342737197876\n",
      "TEST accuracy at end of epoch 76: 0.8269\n",
      "TRAIN accuracy at end of epoch 76: 0.9\n",
      "                          epoch 77\n",
      "iteration 30200, loss 0.28471463918685913\n",
      "iteration 30300, loss 0.3314417600631714\n",
      "iteration 30400, loss 0.3492937684059143\n",
      "TEST accuracy at end of epoch 77: 0.8231\n",
      "TRAIN accuracy at end of epoch 77: 0.89\n",
      "                          epoch 78\n",
      "iteration 30500, loss 0.31429389119148254\n",
      "iteration 30600, loss 0.2868063151836395\n",
      "iteration 30700, loss 0.26764214038848877\n",
      "iteration 30800, loss 0.29609978199005127\n",
      "TEST accuracy at end of epoch 78: 0.8313\n",
      "TRAIN accuracy at end of epoch 78: 0.91\n",
      "                          epoch 79\n",
      "iteration 30900, loss 0.17642366886138916\n",
      "iteration 31000, loss 0.2617620825767517\n",
      "iteration 31100, loss 0.25013911724090576\n",
      "iteration 31200, loss 0.21808505058288574\n",
      "TEST accuracy at end of epoch 79: 0.8364\n",
      "TRAIN accuracy at end of epoch 79: 0.91\n",
      "                          epoch 80\n",
      "iteration 31300, loss 0.19711773097515106\n",
      "iteration 31400, loss 0.41949737071990967\n",
      "iteration 31500, loss 0.2330690622329712\n",
      "iteration 31600, loss 0.33017975091934204\n",
      "TEST accuracy at end of epoch 80: 0.8244\n",
      "TRAIN accuracy at end of epoch 80: 0.93\n",
      "                          epoch 81\n",
      "iteration 31700, loss 0.3404579162597656\n",
      "iteration 31800, loss 0.2536942958831787\n",
      "iteration 31900, loss 0.32642269134521484\n",
      "iteration 32000, loss 0.3973166048526764\n",
      "TEST accuracy at end of epoch 81: 0.8232\n",
      "TRAIN accuracy at end of epoch 81: 0.88\n",
      "                          epoch 82\n",
      "iteration 32100, loss 0.31914597749710083\n",
      "iteration 32200, loss 0.27005118131637573\n",
      "iteration 32300, loss 0.2235366553068161\n",
      "iteration 32400, loss 0.34804290533065796\n",
      "TEST accuracy at end of epoch 82: 0.8257\n",
      "TRAIN accuracy at end of epoch 82: 0.85\n",
      "                          epoch 83\n",
      "iteration 32500, loss 0.23662708699703217\n",
      "iteration 32600, loss 0.24279598891735077\n",
      "iteration 32700, loss 0.2413473278284073\n",
      "iteration 32800, loss 0.21843546628952026\n",
      "TEST accuracy at end of epoch 83: 0.8345\n",
      "TRAIN accuracy at end of epoch 83: 0.87\n",
      "                          epoch 84\n",
      "iteration 32900, loss 0.22870302200317383\n",
      "iteration 33000, loss 0.3501163125038147\n",
      "iteration 33100, loss 0.19340308010578156\n",
      "iteration 33200, loss 0.13739974796772003\n",
      "TEST accuracy at end of epoch 84: 0.8345\n",
      "TRAIN accuracy at end of epoch 84: 0.95\n",
      "                          epoch 85\n",
      "iteration 33300, loss 0.2840898036956787\n",
      "iteration 33400, loss 0.29940560460090637\n",
      "iteration 33500, loss 0.3570435345172882\n",
      "iteration 33600, loss 0.3873622417449951\n",
      "TEST accuracy at end of epoch 85: 0.7939\n",
      "TRAIN accuracy at end of epoch 85: 0.86\n",
      "                          epoch 86\n",
      "iteration 33700, loss 0.31367015838623047\n",
      "iteration 33800, loss 0.15321052074432373\n",
      "iteration 33900, loss 0.2446187138557434\n",
      "iteration 34000, loss 0.26133042573928833\n",
      "TEST accuracy at end of epoch 86: 0.8352\n",
      "TRAIN accuracy at end of epoch 86: 0.91\n",
      "                          epoch 87\n",
      "iteration 34100, loss 0.1805012822151184\n",
      "iteration 34200, loss 0.19560031592845917\n",
      "iteration 34300, loss 0.29451003670692444\n",
      "iteration 34400, loss 0.2045460343360901\n",
      "TEST accuracy at end of epoch 87: 0.8059\n",
      "TRAIN accuracy at end of epoch 87: 0.85\n",
      "                          epoch 88\n",
      "iteration 34500, loss 0.22447386384010315\n",
      "iteration 34600, loss 0.28194963932037354\n",
      "iteration 34700, loss 0.2736351490020752\n",
      "TEST accuracy at end of epoch 88: 0.8209\n",
      "TRAIN accuracy at end of epoch 88: 0.89\n",
      "                          epoch 89\n",
      "iteration 34800, loss 0.2733623683452606\n",
      "iteration 34900, loss 0.2334604263305664\n",
      "iteration 35000, loss 0.2881995737552643\n",
      "iteration 35100, loss 0.22921623289585114\n",
      "TEST accuracy at end of epoch 89: 0.7973\n",
      "TRAIN accuracy at end of epoch 89: 0.83\n",
      "                          epoch 90\n",
      "iteration 35200, loss 0.2980882525444031\n",
      "iteration 35300, loss 0.2161838561296463\n",
      "iteration 35400, loss 0.2598069906234741\n",
      "iteration 35500, loss 0.2418045997619629\n",
      "TEST accuracy at end of epoch 90: 0.8216\n",
      "TRAIN accuracy at end of epoch 90: 0.88\n",
      "                          epoch 91\n",
      "iteration 35600, loss 0.15315593779087067\n",
      "iteration 35700, loss 0.2265094518661499\n",
      "iteration 35800, loss 0.2381632775068283\n",
      "iteration 35900, loss 0.13875700533390045\n",
      "TEST accuracy at end of epoch 91: 0.8202\n",
      "TRAIN accuracy at end of epoch 91: 0.91\n",
      "                          epoch 92\n",
      "iteration 36000, loss 0.21465423703193665\n",
      "iteration 36100, loss 0.37674498558044434\n",
      "iteration 36200, loss 0.3120362162590027\n",
      "iteration 36300, loss 0.2969322204589844\n",
      "TEST accuracy at end of epoch 92: 0.8134\n",
      "TRAIN accuracy at end of epoch 92: 0.86\n",
      "                          epoch 93\n",
      "iteration 36400, loss 0.13090333342552185\n",
      "iteration 36500, loss 0.20115119218826294\n",
      "iteration 36600, loss 0.19874592125415802\n",
      "iteration 36700, loss 0.11330502480268478\n",
      "TEST accuracy at end of epoch 93: 0.8147\n",
      "TRAIN accuracy at end of epoch 93: 0.9\n",
      "                          epoch 94\n",
      "iteration 36800, loss 0.24645283818244934\n",
      "iteration 36900, loss 0.21067532896995544\n",
      "iteration 37000, loss 0.2222876101732254\n",
      "iteration 37100, loss 0.25441908836364746\n",
      "TEST accuracy at end of epoch 94: 0.8441\n",
      "TRAIN accuracy at end of epoch 94: 0.94\n",
      "                          epoch 95\n",
      "iteration 37200, loss 0.226077601313591\n",
      "iteration 37300, loss 0.28931695222854614\n",
      "iteration 37400, loss 0.24929800629615784\n",
      "iteration 37500, loss 0.18971101939678192\n",
      "TEST accuracy at end of epoch 95: 0.8268\n",
      "TRAIN accuracy at end of epoch 95: 0.9\n",
      "                          epoch 96\n",
      "iteration 37600, loss 0.2625398337841034\n",
      "iteration 37700, loss 0.18821005523204803\n",
      "iteration 37800, loss 0.24399209022521973\n",
      "iteration 37900, loss 0.21619227528572083\n",
      "TEST accuracy at end of epoch 96: 0.8257\n",
      "TRAIN accuracy at end of epoch 96: 0.9\n",
      "                          epoch 97\n",
      "iteration 38000, loss 0.2977044880390167\n",
      "iteration 38100, loss 0.30446016788482666\n",
      "iteration 38200, loss 0.2680721580982208\n",
      "iteration 38300, loss 0.29428040981292725\n",
      "TEST accuracy at end of epoch 97: 0.8436\n",
      "TRAIN accuracy at end of epoch 97: 0.89\n",
      "                          epoch 98\n",
      "iteration 38400, loss 0.1992221474647522\n",
      "iteration 38500, loss 0.2058967798948288\n",
      "iteration 38600, loss 0.1694854199886322\n",
      "iteration 38700, loss 0.1608109474182129\n",
      "TEST accuracy at end of epoch 98: 0.8356\n",
      "TRAIN accuracy at end of epoch 98: 0.88\n",
      "                          epoch 99\n",
      "iteration 38800, loss 0.1490451842546463\n",
      "iteration 38900, loss 0.20748907327651978\n",
      "iteration 39000, loss 0.3026207685470581\n",
      "iteration 39100, loss 0.23866863548755646\n",
      "TEST accuracy at end of epoch 99: 0.8368\n",
      "TRAIN accuracy at end of epoch 99: 0.87\n",
      "                          epoch 100\n",
      "iteration 39200, loss 0.20290417969226837\n",
      "iteration 39300, loss 0.2705337405204773\n",
      "iteration 39400, loss 0.24058027565479279\n",
      "TEST accuracy at end of epoch 100: 0.8203\n",
      "TRAIN accuracy at end of epoch 100: 0.94\n",
      "                          epoch 101\n",
      "iteration 39500, loss 0.267574667930603\n",
      "iteration 39600, loss 0.1821914166212082\n",
      "iteration 39700, loss 0.2611438035964966\n",
      "iteration 39800, loss 0.18080148100852966\n",
      "TEST accuracy at end of epoch 101: 0.8484\n",
      "TRAIN accuracy at end of epoch 101: 0.92\n",
      "                          epoch 102\n",
      "iteration 39900, loss 0.18345434963703156\n",
      "iteration 40000, loss 0.22005513310432434\n",
      "iteration 40100, loss 0.24359871447086334\n",
      "iteration 40200, loss 0.17435359954833984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST accuracy at end of epoch 102: 0.8325\n",
      "TRAIN accuracy at end of epoch 102: 0.9\n",
      "                          epoch 103\n",
      "iteration 40300, loss 0.16575978696346283\n",
      "iteration 40400, loss 0.2381753772497177\n",
      "iteration 40500, loss 0.2286761999130249\n",
      "iteration 40600, loss 0.2654663920402527\n",
      "TEST accuracy at end of epoch 103: 0.825\n",
      "TRAIN accuracy at end of epoch 103: 0.86\n",
      "                          epoch 104\n",
      "iteration 40700, loss 0.18168821930885315\n",
      "iteration 40800, loss 0.24268217384815216\n",
      "iteration 40900, loss 0.21301279962062836\n",
      "iteration 41000, loss 0.2910410165786743\n",
      "TEST accuracy at end of epoch 104: 0.8399\n",
      "TRAIN accuracy at end of epoch 104: 0.91\n",
      "                          epoch 105\n",
      "iteration 41100, loss 0.2610541880130768\n",
      "iteration 41200, loss 0.1745993196964264\n",
      "iteration 41300, loss 0.27374565601348877\n",
      "iteration 41400, loss 0.20826217532157898\n",
      "TEST accuracy at end of epoch 105: 0.8345\n",
      "TRAIN accuracy at end of epoch 105: 0.91\n",
      "                          epoch 106\n",
      "iteration 41500, loss 0.27811384201049805\n",
      "iteration 41600, loss 0.16345179080963135\n",
      "iteration 41700, loss 0.2626209259033203\n",
      "iteration 41800, loss 0.44901642203330994\n",
      "TEST accuracy at end of epoch 106: 0.8271\n",
      "TRAIN accuracy at end of epoch 106: 0.83\n",
      "                          epoch 107\n",
      "iteration 41900, loss 0.1557062268257141\n",
      "iteration 42000, loss 0.19227716326713562\n",
      "iteration 42100, loss 0.14959171414375305\n",
      "iteration 42200, loss 0.1955844908952713\n",
      "TEST accuracy at end of epoch 107: 0.8418\n",
      "TRAIN accuracy at end of epoch 107: 0.92\n",
      "                          epoch 108\n",
      "iteration 42300, loss 0.20765012502670288\n",
      "iteration 42400, loss 0.3042140305042267\n",
      "iteration 42500, loss 0.22727301716804504\n",
      "iteration 42600, loss 0.28708744049072266\n",
      "TEST accuracy at end of epoch 108: 0.8474\n",
      "TRAIN accuracy at end of epoch 108: 0.93\n",
      "                          epoch 109\n",
      "iteration 42700, loss 0.2201305329799652\n",
      "iteration 42800, loss 0.1750473976135254\n",
      "iteration 42900, loss 0.0822829157114029\n",
      "iteration 43000, loss 0.18314814567565918\n",
      "TEST accuracy at end of epoch 109: 0.844\n",
      "TRAIN accuracy at end of epoch 109: 0.91\n",
      "                          epoch 110\n",
      "iteration 43100, loss 0.1343870759010315\n",
      "iteration 43200, loss 0.21190428733825684\n",
      "iteration 43300, loss 0.14561957120895386\n",
      "iteration 43400, loss 0.15410375595092773\n",
      "TEST accuracy at end of epoch 110: 0.8442\n",
      "TRAIN accuracy at end of epoch 110: 0.95\n",
      "                          epoch 111\n",
      "iteration 43500, loss 0.23400935530662537\n",
      "iteration 43600, loss 0.13924020528793335\n",
      "iteration 43700, loss 0.22065767645835876\n",
      "TEST accuracy at end of epoch 111: 0.8355\n",
      "TRAIN accuracy at end of epoch 111: 0.96\n",
      "                          epoch 112\n",
      "iteration 43800, loss 0.14692190289497375\n",
      "iteration 43900, loss 0.13747566938400269\n",
      "iteration 44000, loss 0.1955743432044983\n",
      "iteration 44100, loss 0.23011691868305206\n",
      "TEST accuracy at end of epoch 112: 0.8329\n",
      "TRAIN accuracy at end of epoch 112: 0.89\n",
      "                          epoch 113\n",
      "iteration 44200, loss 0.24585393071174622\n",
      "iteration 44300, loss 0.1440751850605011\n",
      "iteration 44400, loss 0.14594578742980957\n",
      "iteration 44500, loss 0.16478759050369263\n",
      "TEST accuracy at end of epoch 113: 0.8518\n",
      "TRAIN accuracy at end of epoch 113: 0.96\n",
      "                          epoch 114\n",
      "iteration 44600, loss 0.1913710981607437\n",
      "iteration 44700, loss 0.12171235680580139\n",
      "iteration 44800, loss 0.27243149280548096\n",
      "iteration 44900, loss 0.1691652089357376\n",
      "TEST accuracy at end of epoch 114: 0.841\n",
      "TRAIN accuracy at end of epoch 114: 0.94\n",
      "                          epoch 115\n",
      "iteration 45000, loss 0.13773168623447418\n",
      "iteration 45100, loss 0.1370576024055481\n",
      "iteration 45200, loss 0.19164393842220306\n",
      "iteration 45300, loss 0.1703813672065735\n",
      "TEST accuracy at end of epoch 115: 0.8428\n",
      "TRAIN accuracy at end of epoch 115: 0.91\n",
      "                          epoch 116\n",
      "iteration 45400, loss 0.15390712022781372\n",
      "iteration 45500, loss 0.16527089476585388\n",
      "iteration 45600, loss 0.15742212533950806\n",
      "iteration 45700, loss 0.18371444940567017\n",
      "TEST accuracy at end of epoch 116: 0.8551\n",
      "TRAIN accuracy at end of epoch 116: 0.96\n",
      "                          epoch 117\n",
      "iteration 45800, loss 0.13392606377601624\n",
      "iteration 45900, loss 0.1789434254169464\n",
      "iteration 46000, loss 0.14999467134475708\n",
      "iteration 46100, loss 0.24088412523269653\n",
      "TEST accuracy at end of epoch 117: 0.8465\n",
      "TRAIN accuracy at end of epoch 117: 0.94\n",
      "                          epoch 118\n",
      "iteration 46200, loss 0.14244012534618378\n",
      "iteration 46300, loss 0.2114713042974472\n",
      "iteration 46400, loss 0.17414048314094543\n",
      "iteration 46500, loss 0.20671528577804565\n",
      "TEST accuracy at end of epoch 118: 0.8371\n",
      "TRAIN accuracy at end of epoch 118: 0.94\n",
      "                          epoch 119\n",
      "iteration 46600, loss 0.21472211182117462\n",
      "iteration 46700, loss 0.15904110670089722\n",
      "iteration 46800, loss 0.1125403642654419\n",
      "iteration 46900, loss 0.211028054356575\n",
      "TEST accuracy at end of epoch 119: 0.8419\n",
      "TRAIN accuracy at end of epoch 119: 0.96\n",
      "                          epoch 120\n",
      "iteration 47000, loss 0.045811302959918976\n",
      "iteration 47100, loss 0.21071727573871613\n",
      "iteration 47200, loss 0.2234620898962021\n",
      "iteration 47300, loss 0.11527681350708008\n",
      "TEST accuracy at end of epoch 120: 0.8542\n",
      "TRAIN accuracy at end of epoch 120: 0.97\n",
      "                          epoch 121\n",
      "iteration 47400, loss 0.09507207572460175\n",
      "iteration 47500, loss 0.14808863401412964\n",
      "iteration 47600, loss 0.1413441300392151\n",
      "iteration 47700, loss 0.31167304515838623\n",
      "TEST accuracy at end of epoch 121: 0.8485\n",
      "TRAIN accuracy at end of epoch 121: 0.96\n",
      "                          epoch 122\n",
      "iteration 47800, loss 0.20734968781471252\n",
      "iteration 47900, loss 0.0983109325170517\n",
      "iteration 48000, loss 0.270987868309021\n",
      "TEST accuracy at end of epoch 122: 0.8496\n",
      "TRAIN accuracy at end of epoch 122: 0.97\n",
      "                          epoch 123\n",
      "iteration 48100, loss 0.19815579056739807\n",
      "iteration 48200, loss 0.14790229499340057\n",
      "iteration 48300, loss 0.12217044830322266\n",
      "iteration 48400, loss 0.20568865537643433\n",
      "TEST accuracy at end of epoch 123: 0.8495\n",
      "TRAIN accuracy at end of epoch 123: 0.95\n",
      "                          epoch 124\n",
      "iteration 48500, loss 0.11520899832248688\n",
      "iteration 48600, loss 0.2002277672290802\n",
      "iteration 48700, loss 0.06540681421756744\n",
      "iteration 48800, loss 0.10859072208404541\n",
      "TEST accuracy at end of epoch 124: 0.8541\n",
      "TRAIN accuracy at end of epoch 124: 0.98\n",
      "                          epoch 125\n",
      "iteration 48900, loss 0.17141097784042358\n",
      "iteration 49000, loss 0.11094636470079422\n",
      "iteration 49100, loss 0.2706305980682373\n",
      "iteration 49200, loss 0.1802656054496765\n",
      "TEST accuracy at end of epoch 125: 0.853\n",
      "TRAIN accuracy at end of epoch 125: 0.95\n",
      "                          epoch 126\n",
      "iteration 49300, loss 0.08927187323570251\n",
      "iteration 49400, loss 0.22408008575439453\n",
      "iteration 49500, loss 0.14194174110889435\n",
      "iteration 49600, loss 0.22207069396972656\n",
      "TEST accuracy at end of epoch 126: 0.8463\n",
      "TRAIN accuracy at end of epoch 126: 0.91\n",
      "                          epoch 127\n",
      "iteration 49700, loss 0.23282931745052338\n",
      "iteration 49800, loss 0.20824581384658813\n",
      "iteration 49900, loss 0.10993817448616028\n",
      "iteration 50000, loss 0.0667339414358139\n",
      "TEST accuracy at end of epoch 127: 0.8455\n",
      "TRAIN accuracy at end of epoch 127: 0.99\n",
      "                          epoch 128\n",
      "iteration 50100, loss 0.1973334550857544\n",
      "iteration 50200, loss 0.1458522081375122\n",
      "iteration 50300, loss 0.13865649700164795\n",
      "iteration 50400, loss 0.11773490905761719\n",
      "TEST accuracy at end of epoch 128: 0.8532\n",
      "TRAIN accuracy at end of epoch 128: 1.0\n",
      "                          epoch 129\n",
      "iteration 50500, loss 0.14197736978530884\n",
      "iteration 50600, loss 0.13563355803489685\n",
      "iteration 50700, loss 0.19250646233558655\n",
      "iteration 50800, loss 0.15311764180660248\n",
      "TEST accuracy at end of epoch 129: 0.8611\n",
      "TRAIN accuracy at end of epoch 129: 0.98\n",
      "                          epoch 130\n",
      "iteration 50900, loss 0.10661853849887848\n",
      "iteration 51000, loss 0.10075351595878601\n",
      "iteration 51100, loss 0.18543821573257446\n",
      "iteration 51200, loss 0.1920894831418991\n",
      "TEST accuracy at end of epoch 130: 0.8473\n",
      "TRAIN accuracy at end of epoch 130: 0.96\n",
      "                          epoch 131\n",
      "iteration 51300, loss 0.1595514565706253\n",
      "iteration 51400, loss 0.18037700653076172\n",
      "iteration 51500, loss 0.09921373426914215\n",
      "iteration 51600, loss 0.1476602852344513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST accuracy at end of epoch 131: 0.8553\n",
      "TRAIN accuracy at end of epoch 131: 1.0\n",
      "                          epoch 132\n",
      "iteration 51700, loss 0.12064771354198456\n",
      "iteration 51800, loss 0.22027038037776947\n",
      "iteration 51900, loss 0.14616461098194122\n",
      "iteration 52000, loss 0.11830368638038635\n",
      "TEST accuracy at end of epoch 132: 0.8525\n",
      "TRAIN accuracy at end of epoch 132: 0.96\n",
      "                          epoch 133\n",
      "iteration 52100, loss 0.11685943603515625\n",
      "iteration 52200, loss 0.16822275519371033\n",
      "iteration 52300, loss 0.16852334141731262\n",
      "TEST accuracy at end of epoch 133: 0.8483\n",
      "TRAIN accuracy at end of epoch 133: 0.91\n",
      "                          epoch 134\n",
      "iteration 52400, loss 0.11551625281572342\n",
      "iteration 52500, loss 0.13104653358459473\n",
      "iteration 52600, loss 0.23570092022418976\n",
      "iteration 52700, loss 0.15590700507164001\n",
      "TEST accuracy at end of epoch 134: 0.8355\n",
      "TRAIN accuracy at end of epoch 134: 0.9\n",
      "                          epoch 135\n",
      "iteration 52800, loss 0.15098616480827332\n",
      "iteration 52900, loss 0.15643107891082764\n",
      "iteration 53000, loss 0.189024418592453\n",
      "iteration 53100, loss 0.10168659687042236\n",
      "TEST accuracy at end of epoch 135: 0.854\n",
      "TRAIN accuracy at end of epoch 135: 0.98\n",
      "                          epoch 136\n",
      "iteration 53200, loss 0.1410776525735855\n",
      "iteration 53300, loss 0.17600944638252258\n",
      "iteration 53400, loss 0.14833307266235352\n",
      "iteration 53500, loss 0.20244261622428894\n",
      "TEST accuracy at end of epoch 136: 0.8524\n",
      "TRAIN accuracy at end of epoch 136: 0.95\n",
      "                          epoch 137\n",
      "iteration 53600, loss 0.17172519862651825\n",
      "iteration 53700, loss 0.08271659165620804\n",
      "iteration 53800, loss 0.18570831418037415\n",
      "iteration 53900, loss 0.14602592587471008\n",
      "TEST accuracy at end of epoch 137: 0.8553\n",
      "TRAIN accuracy at end of epoch 137: 0.98\n",
      "                          epoch 138\n",
      "iteration 54000, loss 0.08280718326568604\n",
      "iteration 54100, loss 0.13265740871429443\n",
      "iteration 54200, loss 0.03689493238925934\n",
      "iteration 54300, loss 0.09987573325634003\n",
      "TEST accuracy at end of epoch 138: 0.8542\n",
      "TRAIN accuracy at end of epoch 138: 0.94\n",
      "                          epoch 139\n",
      "iteration 54400, loss 0.20739512145519257\n",
      "iteration 54500, loss 0.14083369076251984\n",
      "iteration 54600, loss 0.13901837170124054\n",
      "iteration 54700, loss 0.21767207980155945\n",
      "TEST accuracy at end of epoch 139: 0.8587\n",
      "TRAIN accuracy at end of epoch 139: 0.94\n",
      "                          epoch 140\n",
      "iteration 54800, loss 0.09501056373119354\n",
      "iteration 54900, loss 0.09261173009872437\n",
      "iteration 55000, loss 0.1732981652021408\n",
      "iteration 55100, loss 0.12250356376171112\n",
      "TEST accuracy at end of epoch 140: 0.8506\n",
      "TRAIN accuracy at end of epoch 140: 0.96\n",
      "                          epoch 141\n",
      "iteration 55200, loss 0.06724770367145538\n",
      "iteration 55300, loss 0.10271196812391281\n",
      "iteration 55400, loss 0.09875781834125519\n",
      "iteration 55500, loss 0.13535510003566742\n",
      "TEST accuracy at end of epoch 141: 0.8526\n",
      "TRAIN accuracy at end of epoch 141: 0.96\n",
      "                          epoch 142\n",
      "iteration 55600, loss 0.16831758618354797\n",
      "iteration 55700, loss 0.07429488748311996\n",
      "iteration 55800, loss 0.13977065682411194\n",
      "iteration 55900, loss 0.14541250467300415\n",
      "TEST accuracy at end of epoch 142: 0.8588\n",
      "TRAIN accuracy at end of epoch 142: 0.98\n",
      "                          epoch 143\n",
      "iteration 56000, loss 0.1439048945903778\n",
      "iteration 56100, loss 0.10280505567789078\n",
      "iteration 56200, loss 0.11004166305065155\n",
      "iteration 56300, loss 0.11779280006885529\n",
      "TEST accuracy at end of epoch 143: 0.8487\n",
      "TRAIN accuracy at end of epoch 143: 0.96\n",
      "                          epoch 144\n",
      "iteration 56400, loss 0.06582063436508179\n",
      "iteration 56500, loss 0.09011095762252808\n",
      "iteration 56600, loss 0.07336689531803131\n",
      "TEST accuracy at end of epoch 144: 0.8537\n",
      "TRAIN accuracy at end of epoch 144: 0.96\n",
      "                          epoch 145\n",
      "iteration 56700, loss 0.12141674011945724\n",
      "iteration 56800, loss 0.0700027272105217\n",
      "iteration 56900, loss 0.07127317786216736\n",
      "iteration 57000, loss 0.08873842656612396\n",
      "TEST accuracy at end of epoch 145: 0.852\n",
      "TRAIN accuracy at end of epoch 145: 0.98\n",
      "                          epoch 146\n",
      "iteration 57100, loss 0.07029273360967636\n",
      "iteration 57200, loss 0.05752477049827576\n",
      "iteration 57300, loss 0.21040037274360657\n",
      "iteration 57400, loss 0.1026042029261589\n",
      "TEST accuracy at end of epoch 146: 0.8496\n",
      "TRAIN accuracy at end of epoch 146: 0.98\n",
      "                          epoch 147\n",
      "iteration 57500, loss 0.0650932639837265\n",
      "iteration 57600, loss 0.06483763456344604\n",
      "iteration 57700, loss 0.1235547661781311\n",
      "iteration 57800, loss 0.1090507060289383\n",
      "TEST accuracy at end of epoch 147: 0.8543\n",
      "TRAIN accuracy at end of epoch 147: 0.97\n",
      "                          epoch 148\n",
      "iteration 57900, loss 0.14499519765377045\n",
      "iteration 58000, loss 0.10089296102523804\n",
      "iteration 58100, loss 0.1401817798614502\n",
      "iteration 58200, loss 0.16814947128295898\n",
      "TEST accuracy at end of epoch 148: 0.8488\n",
      "TRAIN accuracy at end of epoch 148: 0.99\n",
      "                          epoch 149\n",
      "iteration 58300, loss 0.05739281699061394\n",
      "iteration 58400, loss 0.11124355345964432\n",
      "iteration 58500, loss 0.1112399697303772\n",
      "iteration 58600, loss 0.1060543954372406\n",
      "TEST accuracy at end of epoch 149: 0.8557\n",
      "TRAIN accuracy at end of epoch 149: 0.97\n",
      "                          epoch 150\n",
      "iteration 58700, loss 0.10920841991901398\n",
      "iteration 58800, loss 0.09235605597496033\n",
      "iteration 58900, loss 0.12560635805130005\n",
      "iteration 59000, loss 0.13653028011322021\n",
      "TEST accuracy at end of epoch 150: 0.8514\n",
      "TRAIN accuracy at end of epoch 150: 0.99\n",
      "                          epoch 151\n",
      "iteration 59100, loss 0.08633466064929962\n",
      "iteration 59200, loss 0.1049611046910286\n",
      "iteration 59300, loss 0.09832300245761871\n",
      "iteration 59400, loss 0.10056748986244202\n",
      "TEST accuracy at end of epoch 151: 0.8592\n",
      "TRAIN accuracy at end of epoch 151: 0.99\n",
      "                          epoch 152\n",
      "iteration 59500, loss 0.08569645881652832\n",
      "iteration 59600, loss 0.06482347846031189\n",
      "iteration 59700, loss 0.10503269731998444\n",
      "iteration 59800, loss 0.09349245578050613\n",
      "TEST accuracy at end of epoch 152: 0.8536\n",
      "TRAIN accuracy at end of epoch 152: 0.99\n",
      "                          epoch 153\n",
      "iteration 59900, loss 0.07372172176837921\n",
      "iteration 60000, loss 0.09351141005754471\n",
      "iteration 60100, loss 0.08729001879692078\n",
      "iteration 60200, loss 0.06994184851646423\n",
      "TEST accuracy at end of epoch 153: 0.8565\n",
      "TRAIN accuracy at end of epoch 153: 0.97\n",
      "                          epoch 154\n",
      "iteration 60300, loss 0.09422095119953156\n",
      "iteration 60400, loss 0.09892094135284424\n",
      "iteration 60500, loss 0.09436740726232529\n",
      "iteration 60600, loss 0.13288037478923798\n",
      "TEST accuracy at end of epoch 154: 0.8556\n",
      "TRAIN accuracy at end of epoch 154: 1.0\n",
      "                          epoch 155\n",
      "iteration 60700, loss 0.06137914955615997\n",
      "iteration 60800, loss 0.06407627463340759\n",
      "iteration 60900, loss 0.10097035020589828\n",
      "TEST accuracy at end of epoch 155: 0.8565\n",
      "TRAIN accuracy at end of epoch 155: 1.0\n",
      "                          epoch 156\n",
      "iteration 61000, loss 0.10360913723707199\n",
      "iteration 61100, loss 0.11090134084224701\n",
      "iteration 61200, loss 0.03715451806783676\n",
      "iteration 61300, loss 0.06981203705072403\n",
      "TEST accuracy at end of epoch 156: 0.8595\n",
      "TRAIN accuracy at end of epoch 156: 0.99\n",
      "                          epoch 157\n",
      "iteration 61400, loss 0.0919206291437149\n",
      "iteration 61500, loss 0.08007824420928955\n",
      "iteration 61600, loss 0.13953062891960144\n",
      "iteration 61700, loss 0.15841345489025116\n",
      "TEST accuracy at end of epoch 157: 0.8548\n",
      "TRAIN accuracy at end of epoch 157: 0.99\n",
      "                          epoch 158\n",
      "iteration 61800, loss 0.154476597905159\n",
      "iteration 61900, loss 0.12938746809959412\n",
      "iteration 62000, loss 0.034162737429142\n",
      "iteration 62100, loss 0.04529501497745514\n",
      "TEST accuracy at end of epoch 158: 0.86\n",
      "TRAIN accuracy at end of epoch 158: 0.99\n",
      "                          epoch 159\n",
      "iteration 62200, loss 0.09560788422822952\n",
      "iteration 62300, loss 0.1291879564523697\n",
      "iteration 62400, loss 0.07585462927818298\n",
      "iteration 62500, loss 0.02019830048084259\n",
      "TEST accuracy at end of epoch 159: 0.8581\n",
      "TRAIN accuracy at end of epoch 159: 0.98\n",
      "                          epoch 160\n",
      "iteration 62600, loss 0.06721126288175583\n",
      "iteration 62700, loss 0.040877364575862885\n",
      "iteration 62800, loss 0.10573852062225342\n",
      "iteration 62900, loss 0.03366364538669586\n",
      "TEST accuracy at end of epoch 160: 0.8602\n",
      "TRAIN accuracy at end of epoch 160: 0.99\n",
      "                          epoch 161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 63000, loss 0.0379609540104866\n",
      "iteration 63100, loss 0.06535451114177704\n",
      "iteration 63200, loss 0.15217259526252747\n",
      "iteration 63300, loss 0.08977998793125153\n",
      "TEST accuracy at end of epoch 161: 0.8592\n",
      "TRAIN accuracy at end of epoch 161: 1.0\n",
      "                          epoch 162\n",
      "iteration 63400, loss 0.11171723157167435\n",
      "iteration 63500, loss 0.05513317883014679\n",
      "iteration 63600, loss 0.06603901833295822\n",
      "iteration 63700, loss 0.10386800765991211\n",
      "TEST accuracy at end of epoch 162: 0.8571\n",
      "TRAIN accuracy at end of epoch 162: 1.0\n",
      "                          epoch 163\n",
      "iteration 63800, loss 0.1418495923280716\n",
      "iteration 63900, loss 0.07567838579416275\n",
      "iteration 64000, loss 0.07934383302927017\n",
      "iteration 64100, loss 0.10242805629968643\n",
      "TEST accuracy at end of epoch 163: 0.8576\n",
      "TRAIN accuracy at end of epoch 163: 0.99\n",
      "                          epoch 164\n",
      "iteration 64200, loss 0.06206435710191727\n",
      "iteration 64300, loss 0.08017496764659882\n",
      "iteration 64400, loss 0.18780246376991272\n",
      "iteration 64500, loss 0.13651439547538757\n",
      "TEST accuracy at end of epoch 164: 0.86\n",
      "TRAIN accuracy at end of epoch 164: 1.0\n",
      "                          epoch 165\n",
      "iteration 64600, loss 0.13888822495937347\n",
      "iteration 64700, loss 0.09795700013637543\n",
      "iteration 64800, loss 0.09818869829177856\n",
      "iteration 64900, loss 0.08497542142868042\n",
      "TEST accuracy at end of epoch 165: 0.863\n",
      "TRAIN accuracy at end of epoch 165: 0.97\n",
      "                          epoch 166\n",
      "iteration 65000, loss 0.06608331203460693\n",
      "iteration 65100, loss 0.07873712480068207\n",
      "iteration 65200, loss 0.10656490921974182\n",
      "TEST accuracy at end of epoch 166: 0.8603\n",
      "TRAIN accuracy at end of epoch 166: 0.98\n",
      "                          epoch 167\n",
      "iteration 65300, loss 0.08432076871395111\n",
      "iteration 65400, loss 0.09023616462945938\n",
      "iteration 65500, loss 0.050529420375823975\n",
      "iteration 65600, loss 0.09408719092607498\n",
      "TEST accuracy at end of epoch 167: 0.8611\n",
      "TRAIN accuracy at end of epoch 167: 1.0\n",
      "                          epoch 168\n",
      "iteration 65700, loss 0.08967477083206177\n",
      "iteration 65800, loss 0.079424649477005\n",
      "iteration 65900, loss 0.07217098772525787\n",
      "iteration 66000, loss 0.05705134943127632\n",
      "TEST accuracy at end of epoch 168: 0.8649\n",
      "TRAIN accuracy at end of epoch 168: 0.99\n",
      "                          epoch 169\n",
      "iteration 66100, loss 0.04231931269168854\n",
      "iteration 66200, loss 0.04559139907360077\n",
      "iteration 66300, loss 0.140322744846344\n",
      "iteration 66400, loss 0.10144814848899841\n",
      "TEST accuracy at end of epoch 169: 0.8604\n",
      "TRAIN accuracy at end of epoch 169: 1.0\n",
      "                          epoch 170\n",
      "iteration 66500, loss 0.075949527323246\n",
      "iteration 66600, loss 0.026606783270835876\n",
      "iteration 66700, loss 0.0451260469853878\n",
      "iteration 66800, loss 0.09408386796712875\n",
      "TEST accuracy at end of epoch 170: 0.8603\n",
      "TRAIN accuracy at end of epoch 170: 0.97\n",
      "                          epoch 171\n",
      "iteration 66900, loss 0.10313307493925095\n",
      "iteration 67000, loss 0.04458377882838249\n",
      "iteration 67100, loss 0.043582770973443985\n",
      "iteration 67200, loss 0.07173455506563187\n",
      "TEST accuracy at end of epoch 171: 0.8603\n",
      "TRAIN accuracy at end of epoch 171: 1.0\n",
      "                          epoch 172\n",
      "iteration 67300, loss 0.05713394284248352\n",
      "iteration 67400, loss 0.03559640794992447\n",
      "iteration 67500, loss 0.13301679491996765\n",
      "iteration 67600, loss 0.02023324929177761\n",
      "TEST accuracy at end of epoch 172: 0.8615\n",
      "TRAIN accuracy at end of epoch 172: 0.98\n",
      "                          epoch 173\n",
      "iteration 67700, loss 0.09378057718276978\n",
      "iteration 67800, loss 0.12996576726436615\n",
      "iteration 67900, loss 0.06323499977588654\n",
      "iteration 68000, loss 0.0370078980922699\n",
      "TEST accuracy at end of epoch 173: 0.86\n",
      "TRAIN accuracy at end of epoch 173: 1.0\n",
      "                          epoch 174\n",
      "iteration 68100, loss 0.06951917707920074\n",
      "iteration 68200, loss 0.12405957281589508\n",
      "iteration 68300, loss 0.07647057622671127\n",
      "iteration 68400, loss 0.09297490119934082\n",
      "TEST accuracy at end of epoch 174: 0.862\n",
      "TRAIN accuracy at end of epoch 174: 0.99\n",
      "                          epoch 175\n",
      "iteration 68500, loss 0.06125853955745697\n",
      "iteration 68600, loss 0.08182857930660248\n",
      "iteration 68700, loss 0.06933212280273438\n",
      "iteration 68800, loss 0.0483279749751091\n",
      "TEST accuracy at end of epoch 175: 0.8631\n",
      "TRAIN accuracy at end of epoch 175: 0.99\n",
      "                          epoch 176\n",
      "iteration 68900, loss 0.0534389391541481\n",
      "iteration 69000, loss 0.09106822311878204\n",
      "iteration 69100, loss 0.0368148572742939\n",
      "iteration 69200, loss 0.0741378590464592\n",
      "TEST accuracy at end of epoch 176: 0.8626\n",
      "TRAIN accuracy at end of epoch 176: 0.99\n",
      "                          epoch 177\n",
      "iteration 69300, loss 0.06587925553321838\n",
      "iteration 69400, loss 0.039610832929611206\n",
      "iteration 69500, loss 0.06921997666358948\n",
      "TEST accuracy at end of epoch 177: 0.8629\n",
      "TRAIN accuracy at end of epoch 177: 0.99\n",
      "                          epoch 178\n",
      "iteration 69600, loss 0.09435625374317169\n",
      "iteration 69700, loss 0.04356629028916359\n",
      "iteration 69800, loss 0.06218837574124336\n",
      "iteration 69900, loss 0.09592358767986298\n",
      "TEST accuracy at end of epoch 178: 0.8634\n",
      "TRAIN accuracy at end of epoch 178: 0.99\n",
      "                          epoch 179\n",
      "iteration 70000, loss 0.09667627513408661\n",
      "iteration 70100, loss 0.030455738306045532\n",
      "iteration 70200, loss 0.060396306216716766\n",
      "iteration 70300, loss 0.09950751066207886\n",
      "TEST accuracy at end of epoch 179: 0.8624\n",
      "TRAIN accuracy at end of epoch 179: 0.99\n",
      "                          epoch 180\n",
      "iteration 70400, loss 0.16500136256217957\n",
      "iteration 70500, loss 0.0844120979309082\n",
      "iteration 70600, loss 0.059496693313121796\n",
      "iteration 70700, loss 0.09977227449417114\n",
      "TEST accuracy at end of epoch 180: 0.8631\n",
      "TRAIN accuracy at end of epoch 180: 1.0\n",
      "                          epoch 181\n",
      "iteration 70800, loss 0.08922986686229706\n",
      "iteration 70900, loss 0.05114799737930298\n",
      "iteration 71000, loss 0.0382843054831028\n",
      "iteration 71100, loss 0.11361680179834366\n",
      "TEST accuracy at end of epoch 181: 0.861\n",
      "TRAIN accuracy at end of epoch 181: 0.98\n",
      "                          epoch 182\n",
      "iteration 71200, loss 0.06000107526779175\n",
      "iteration 71300, loss 0.08157338947057724\n",
      "iteration 71400, loss 0.026848360896110535\n",
      "iteration 71500, loss 0.0701078549027443\n",
      "TEST accuracy at end of epoch 182: 0.8621\n",
      "TRAIN accuracy at end of epoch 182: 1.0\n",
      "                          epoch 183\n",
      "iteration 71600, loss 0.027281051501631737\n",
      "iteration 71700, loss 0.10655895620584488\n",
      "iteration 71800, loss 0.09378507733345032\n",
      "iteration 71900, loss 0.017660774290561676\n",
      "TEST accuracy at end of epoch 183: 0.8623\n",
      "TRAIN accuracy at end of epoch 183: 0.99\n",
      "                          epoch 184\n",
      "iteration 72000, loss 0.10778772830963135\n",
      "iteration 72100, loss 0.10983553528785706\n",
      "iteration 72200, loss 0.07940047979354858\n",
      "iteration 72300, loss 0.0443318709731102\n",
      "TEST accuracy at end of epoch 184: 0.8627\n",
      "TRAIN accuracy at end of epoch 184: 0.98\n",
      "                          epoch 185\n",
      "iteration 72400, loss 0.06736869364976883\n",
      "iteration 72500, loss 0.059585727751255035\n",
      "iteration 72600, loss 0.06722752749919891\n",
      "iteration 72700, loss 0.14138108491897583\n",
      "TEST accuracy at end of epoch 185: 0.8626\n",
      "TRAIN accuracy at end of epoch 185: 0.98\n",
      "                          epoch 186\n",
      "iteration 72800, loss 0.07156150043010712\n",
      "iteration 72900, loss 0.07747235894203186\n",
      "iteration 73000, loss 0.07641768455505371\n",
      "iteration 73100, loss 0.05392581969499588\n",
      "TEST accuracy at end of epoch 186: 0.8625\n",
      "TRAIN accuracy at end of epoch 186: 1.0\n",
      "                          epoch 187\n",
      "iteration 73200, loss 0.08606555312871933\n",
      "iteration 73300, loss 0.10459600389003754\n",
      "iteration 73400, loss 0.034050747752189636\n",
      "iteration 73500, loss 0.052394799888134\n",
      "TEST accuracy at end of epoch 187: 0.8614\n",
      "TRAIN accuracy at end of epoch 187: 0.98\n",
      "                          epoch 188\n",
      "iteration 73600, loss 0.08330929279327393\n",
      "iteration 73700, loss 0.059053920209407806\n",
      "iteration 73800, loss 0.11260423064231873\n",
      "TEST accuracy at end of epoch 188: 0.8633\n",
      "TRAIN accuracy at end of epoch 188: 1.0\n",
      "                          epoch 189\n",
      "iteration 73900, loss 0.06595735996961594\n",
      "iteration 74000, loss 0.052049074321985245\n",
      "iteration 74100, loss 0.05859057977795601\n",
      "iteration 74200, loss 0.15740475058555603\n",
      "TEST accuracy at end of epoch 189: 0.8614\n",
      "TRAIN accuracy at end of epoch 189: 0.99\n",
      "                          epoch 190\n",
      "iteration 74300, loss 0.05030079185962677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 74400, loss 0.0890277698636055\n",
      "iteration 74500, loss 0.08325426280498505\n",
      "iteration 74600, loss 0.08242438733577728\n",
      "TEST accuracy at end of epoch 190: 0.862\n",
      "TRAIN accuracy at end of epoch 190: 0.96\n",
      "                          epoch 191\n",
      "iteration 74700, loss 0.06760184466838837\n",
      "iteration 74800, loss 0.06435655802488327\n",
      "iteration 74900, loss 0.08386692404747009\n",
      "iteration 75000, loss 0.06550515443086624\n",
      "TEST accuracy at end of epoch 191: 0.8624\n",
      "TRAIN accuracy at end of epoch 191: 0.99\n",
      "                          epoch 192\n",
      "iteration 75100, loss 0.04927363246679306\n",
      "iteration 75200, loss 0.04702553525567055\n",
      "iteration 75300, loss 0.025045620277523994\n",
      "iteration 75400, loss 0.030039288103580475\n",
      "TEST accuracy at end of epoch 192: 0.862\n",
      "TRAIN accuracy at end of epoch 192: 0.99\n",
      "                          epoch 193\n",
      "iteration 75500, loss 0.0509256049990654\n",
      "iteration 75600, loss 0.03090880811214447\n",
      "iteration 75700, loss 0.04021736979484558\n",
      "iteration 75800, loss 0.04027742147445679\n",
      "TEST accuracy at end of epoch 193: 0.8621\n",
      "TRAIN accuracy at end of epoch 193: 0.98\n",
      "                          epoch 194\n",
      "iteration 75900, loss 0.04032215476036072\n",
      "iteration 76000, loss 0.029658837243914604\n",
      "iteration 76100, loss 0.08460469543933868\n",
      "iteration 76200, loss 0.14935442805290222\n",
      "TEST accuracy at end of epoch 194: 0.8625\n",
      "TRAIN accuracy at end of epoch 194: 0.98\n",
      "                          epoch 195\n",
      "iteration 76300, loss 0.057207461446523666\n",
      "iteration 76400, loss 0.02923309989273548\n",
      "iteration 76500, loss 0.04312748461961746\n",
      "iteration 76600, loss 0.13149330019950867\n",
      "TEST accuracy at end of epoch 195: 0.8624\n",
      "TRAIN accuracy at end of epoch 195: 1.0\n",
      "                          epoch 196\n",
      "iteration 76700, loss 0.05403319001197815\n",
      "iteration 76800, loss 0.08213306963443756\n",
      "iteration 76900, loss 0.10076276957988739\n",
      "iteration 77000, loss 0.06776117533445358\n",
      "TEST accuracy at end of epoch 196: 0.8617\n",
      "TRAIN accuracy at end of epoch 196: 1.0\n",
      "                          epoch 197\n",
      "iteration 77100, loss 0.03392517566680908\n",
      "iteration 77200, loss 0.048277270048856735\n",
      "iteration 77300, loss 0.04104389250278473\n",
      "iteration 77400, loss 0.08501438796520233\n",
      "TEST accuracy at end of epoch 197: 0.8621\n",
      "TRAIN accuracy at end of epoch 197: 1.0\n",
      "                          epoch 198\n",
      "iteration 77500, loss 0.07592593133449554\n",
      "iteration 77600, loss 0.06919139623641968\n",
      "iteration 77700, loss 0.07111215591430664\n",
      "iteration 77800, loss 0.040183957666158676\n",
      "TEST accuracy at end of epoch 198: 0.8624\n",
      "TRAIN accuracy at end of epoch 198: 1.0\n",
      "                          epoch 199\n",
      "iteration 77900, loss 0.146377295255661\n",
      "iteration 78000, loss 0.16652023792266846\n",
      "iteration 78100, loss 0.06567466259002686\n",
      "iteration 78200, loss 0.04760264605283737\n",
      "TEST accuracy at end of epoch 199: 0.8627\n",
      "TRAIN accuracy at end of epoch 199: 1.0\n",
      "╰┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╯\n",
      "╭┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╮\n",
      "                         n: 7, res: False\n",
      "size of reduce_mean: (?, 1, 1, 64)\n",
      "size of squeeze: (?, 64)\n",
      "training for 78125 iterations\n",
      "                          epoch 0\n",
      "iteration 100, loss 2.2224745750427246\n",
      "iteration 200, loss 1.98708176612854\n",
      "iteration 300, loss 2.056608200073242\n",
      "TEST accuracy at end of epoch 0: 0.2031\n",
      "TRAIN accuracy at end of epoch 0: 0.21\n",
      "                          epoch 1\n",
      "iteration 400, loss 1.8755412101745605\n",
      "iteration 500, loss 1.808575987815857\n",
      "iteration 600, loss 1.7812628746032715\n",
      "iteration 700, loss 1.687131643295288\n",
      "TEST accuracy at end of epoch 1: 0.3316\n",
      "TRAIN accuracy at end of epoch 1: 0.36\n",
      "                          epoch 2\n",
      "iteration 800, loss 1.7055901288986206\n",
      "iteration 900, loss 1.7462894916534424\n",
      "iteration 1000, loss 1.7566561698913574\n",
      "iteration 1100, loss 1.6128959655761719\n",
      "TEST accuracy at end of epoch 2: 0.2178\n",
      "TRAIN accuracy at end of epoch 2: 0.16\n",
      "                          epoch 3\n",
      "iteration 1200, loss 1.5714168548583984\n",
      "iteration 1300, loss 1.5482258796691895\n",
      "iteration 1400, loss 1.576873779296875\n",
      "iteration 1500, loss 1.4035606384277344\n",
      "TEST accuracy at end of epoch 3: 0.2138\n",
      "TRAIN accuracy at end of epoch 3: 0.19\n",
      "                          epoch 4\n",
      "iteration 1600, loss 1.4377659559249878\n",
      "iteration 1700, loss 1.4611603021621704\n",
      "iteration 1800, loss 1.3350961208343506\n",
      "iteration 1900, loss 1.3486958742141724\n",
      "TEST accuracy at end of epoch 4: 0.3931\n",
      "TRAIN accuracy at end of epoch 4: 0.38\n",
      "                          epoch 5\n",
      "iteration 2000, loss 1.4553321599960327\n",
      "iteration 2100, loss 1.2776724100112915\n",
      "iteration 2200, loss 1.1971781253814697\n",
      "iteration 2300, loss 1.3736090660095215\n",
      "TEST accuracy at end of epoch 5: 0.3884\n",
      "TRAIN accuracy at end of epoch 5: 0.41\n",
      "                          epoch 6\n",
      "iteration 2400, loss 1.2581608295440674\n",
      "iteration 2500, loss 1.2230558395385742\n",
      "iteration 2600, loss 1.2052745819091797\n",
      "iteration 2700, loss 1.2173532247543335\n",
      "TEST accuracy at end of epoch 6: 0.3685\n",
      "TRAIN accuracy at end of epoch 6: 0.33\n",
      "                          epoch 7\n",
      "iteration 2800, loss 1.150029182434082\n",
      "iteration 2900, loss 1.1821520328521729\n",
      "iteration 3000, loss 1.110455870628357\n",
      "iteration 3100, loss 1.149909257888794\n",
      "TEST accuracy at end of epoch 7: 0.4391\n",
      "TRAIN accuracy at end of epoch 7: 0.4\n",
      "                          epoch 8\n",
      "iteration 3200, loss 0.9703074097633362\n",
      "iteration 3300, loss 0.9610258340835571\n",
      "iteration 3400, loss 1.002394676208496\n",
      "iteration 3500, loss 1.158017873764038\n",
      "TEST accuracy at end of epoch 8: 0.6063\n",
      "TRAIN accuracy at end of epoch 8: 0.65\n",
      "                          epoch 9\n",
      "iteration 3600, loss 1.1576639413833618\n",
      "iteration 3700, loss 1.068875789642334\n",
      "iteration 3800, loss 1.0380854606628418\n",
      "iteration 3900, loss 0.9889496564865112\n",
      "TEST accuracy at end of epoch 9: 0.6063\n",
      "TRAIN accuracy at end of epoch 9: 0.67\n",
      "                          epoch 10\n",
      "iteration 4000, loss 1.015866994857788\n",
      "iteration 4100, loss 0.8350674510002136\n",
      "iteration 4200, loss 0.9875909090042114\n",
      "iteration 4300, loss 0.9443330764770508\n",
      "TEST accuracy at end of epoch 10: 0.5567\n",
      "TRAIN accuracy at end of epoch 10: 0.54\n",
      "                          epoch 11\n",
      "iteration 4400, loss 0.8492119312286377\n",
      "iteration 4500, loss 1.0344921350479126\n",
      "iteration 4600, loss 0.8840048909187317\n",
      "TEST accuracy at end of epoch 11: 0.5395\n",
      "TRAIN accuracy at end of epoch 11: 0.58\n",
      "                          epoch 12\n",
      "iteration 4700, loss 0.8501195907592773\n",
      "iteration 4800, loss 0.8297947645187378\n",
      "iteration 4900, loss 0.8252225518226624\n",
      "iteration 5000, loss 0.9395221471786499\n",
      "TEST accuracy at end of epoch 12: 0.6644\n",
      "TRAIN accuracy at end of epoch 12: 0.71\n",
      "                          epoch 13\n",
      "iteration 5100, loss 0.866059422492981\n",
      "iteration 5200, loss 0.7099471092224121\n",
      "iteration 5300, loss 0.8854581117630005\n",
      "iteration 5400, loss 0.8376185894012451\n",
      "TEST accuracy at end of epoch 13: 0.5012\n",
      "TRAIN accuracy at end of epoch 13: 0.53\n",
      "                          epoch 14\n",
      "iteration 5500, loss 0.6505683660507202\n",
      "iteration 5600, loss 0.6321199536323547\n",
      "iteration 5700, loss 0.7525049448013306\n",
      "iteration 5800, loss 0.6932185888290405\n",
      "TEST accuracy at end of epoch 14: 0.5829\n",
      "TRAIN accuracy at end of epoch 14: 0.51\n",
      "                          epoch 15\n",
      "iteration 5900, loss 0.7937859296798706\n",
      "iteration 6000, loss 0.6401212215423584\n",
      "iteration 6100, loss 0.8273659348487854\n",
      "iteration 6200, loss 0.7807664275169373\n",
      "TEST accuracy at end of epoch 15: 0.7202\n",
      "TRAIN accuracy at end of epoch 15: 0.77\n",
      "                          epoch 16\n",
      "iteration 6300, loss 0.7299738526344299\n",
      "iteration 6400, loss 0.5607693791389465\n",
      "iteration 6500, loss 0.5800065994262695\n",
      "iteration 6600, loss 0.5835309028625488\n",
      "TEST accuracy at end of epoch 16: 0.6874\n",
      "TRAIN accuracy at end of epoch 16: 0.73\n",
      "                          epoch 17\n",
      "iteration 6700, loss 0.8632767200469971\n",
      "iteration 6800, loss 0.7064684629440308\n",
      "iteration 6900, loss 0.6103559732437134\n",
      "iteration 7000, loss 0.529391884803772\n",
      "TEST accuracy at end of epoch 17: 0.6082\n",
      "TRAIN accuracy at end of epoch 17: 0.61\n",
      "                          epoch 18\n",
      "iteration 7100, loss 0.8339121341705322\n",
      "iteration 7200, loss 0.608910322189331\n",
      "iteration 7300, loss 0.5152106285095215\n",
      "iteration 7400, loss 0.7457166314125061\n",
      "TEST accuracy at end of epoch 18: 0.5947\n",
      "TRAIN accuracy at end of epoch 18: 0.56\n",
      "                          epoch 19\n",
      "iteration 7500, loss 0.5382445454597473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7600, loss 0.7376587986946106\n",
      "iteration 7700, loss 0.5717989802360535\n",
      "iteration 7800, loss 0.6997585296630859\n",
      "TEST accuracy at end of epoch 19: 0.552\n",
      "TRAIN accuracy at end of epoch 19: 0.53\n",
      "                          epoch 20\n",
      "iteration 7900, loss 0.5608763098716736\n",
      "iteration 8000, loss 0.6164039969444275\n",
      "iteration 8100, loss 0.6439205408096313\n",
      "iteration 8200, loss 0.6214886903762817\n",
      "TEST accuracy at end of epoch 20: 0.5938\n",
      "TRAIN accuracy at end of epoch 20: 0.66\n",
      "                          epoch 21\n",
      "iteration 8300, loss 0.6307752132415771\n",
      "iteration 8400, loss 0.5812615752220154\n",
      "iteration 8500, loss 0.6199463605880737\n",
      "iteration 8600, loss 0.5838382244110107\n",
      "TEST accuracy at end of epoch 21: 0.7427\n",
      "TRAIN accuracy at end of epoch 21: 0.8\n",
      "                          epoch 22\n",
      "iteration 8700, loss 0.4988291263580322\n",
      "iteration 8800, loss 0.4942095875740051\n",
      "iteration 8900, loss 0.6692761778831482\n",
      "TEST accuracy at end of epoch 22: 0.7355\n",
      "TRAIN accuracy at end of epoch 22: 0.78\n",
      "                          epoch 23\n",
      "iteration 9000, loss 0.6907729506492615\n",
      "iteration 9100, loss 0.6885284781455994\n",
      "iteration 9200, loss 0.5784901976585388\n",
      "iteration 9300, loss 0.6024723052978516\n",
      "TEST accuracy at end of epoch 23: 0.6611\n",
      "TRAIN accuracy at end of epoch 23: 0.86\n",
      "                          epoch 24\n",
      "iteration 9400, loss 0.5918689370155334\n",
      "iteration 9500, loss 0.5095282793045044\n",
      "iteration 9600, loss 0.4911078214645386\n",
      "iteration 9700, loss 0.5727258324623108\n",
      "TEST accuracy at end of epoch 24: 0.7667\n",
      "TRAIN accuracy at end of epoch 24: 0.82\n",
      "                          epoch 25\n",
      "iteration 9800, loss 0.4051854610443115\n",
      "iteration 9900, loss 0.5349618792533875\n",
      "iteration 10000, loss 0.44500792026519775\n",
      "iteration 10100, loss 0.4633393883705139\n",
      "TEST accuracy at end of epoch 25: 0.6767\n",
      "TRAIN accuracy at end of epoch 25: 0.72\n",
      "                          epoch 26\n",
      "iteration 10200, loss 0.5172085762023926\n",
      "iteration 10300, loss 0.42964205145835876\n",
      "iteration 10400, loss 0.578008770942688\n",
      "iteration 10500, loss 0.6443693041801453\n",
      "TEST accuracy at end of epoch 26: 0.7309\n",
      "TRAIN accuracy at end of epoch 26: 0.76\n",
      "                          epoch 27\n",
      "iteration 10600, loss 0.4242027997970581\n",
      "iteration 10700, loss 0.5085309743881226\n",
      "iteration 10800, loss 0.5549931526184082\n",
      "iteration 10900, loss 0.5410670042037964\n",
      "TEST accuracy at end of epoch 27: 0.7526\n",
      "TRAIN accuracy at end of epoch 27: 0.81\n",
      "                          epoch 28\n",
      "iteration 11000, loss 0.44015374779701233\n",
      "iteration 11100, loss 0.4407561719417572\n",
      "iteration 11200, loss 0.48590484261512756\n",
      "iteration 11300, loss 0.4981796443462372\n",
      "TEST accuracy at end of epoch 28: 0.6797\n",
      "TRAIN accuracy at end of epoch 28: 0.74\n",
      "                          epoch 29\n",
      "iteration 11400, loss 0.5821398496627808\n",
      "iteration 11500, loss 0.42075854539871216\n",
      "iteration 11600, loss 0.550207257270813\n",
      "iteration 11700, loss 0.5073747038841248\n",
      "TEST accuracy at end of epoch 29: 0.7649\n",
      "TRAIN accuracy at end of epoch 29: 0.76\n",
      "                          epoch 30\n",
      "iteration 11800, loss 0.4938022494316101\n",
      "iteration 11900, loss 0.46240076422691345\n",
      "iteration 12000, loss 0.44057852029800415\n",
      "iteration 12100, loss 0.4625966548919678\n",
      "TEST accuracy at end of epoch 30: 0.7517\n",
      "TRAIN accuracy at end of epoch 30: 0.82\n",
      "                          epoch 31\n",
      "iteration 12200, loss 0.501433789730072\n",
      "iteration 12300, loss 0.4486948251724243\n",
      "iteration 12400, loss 0.47964248061180115\n",
      "iteration 12500, loss 0.4218966066837311\n",
      "TEST accuracy at end of epoch 31: 0.7409\n",
      "TRAIN accuracy at end of epoch 31: 0.81\n",
      "                          epoch 32\n",
      "iteration 12600, loss 0.4447867274284363\n",
      "iteration 12700, loss 0.4632934629917145\n",
      "iteration 12800, loss 0.5507184267044067\n",
      "iteration 12900, loss 0.46827399730682373\n",
      "TEST accuracy at end of epoch 32: 0.7807\n",
      "TRAIN accuracy at end of epoch 32: 0.84\n",
      "                          epoch 33\n",
      "iteration 13000, loss 0.23139359056949615\n",
      "iteration 13100, loss 0.5141094923019409\n",
      "iteration 13200, loss 0.3503825068473816\n",
      "TEST accuracy at end of epoch 33: 0.7842\n",
      "TRAIN accuracy at end of epoch 33: 0.8\n",
      "                          epoch 34\n",
      "iteration 13300, loss 0.3637090027332306\n",
      "iteration 13400, loss 0.5243309736251831\n",
      "iteration 13500, loss 0.4035201966762543\n",
      "iteration 13600, loss 0.49002790451049805\n",
      "TEST accuracy at end of epoch 34: 0.7277\n",
      "TRAIN accuracy at end of epoch 34: 0.78\n",
      "                          epoch 35\n",
      "iteration 13700, loss 0.4387575089931488\n",
      "iteration 13800, loss 0.34220537543296814\n",
      "iteration 13900, loss 0.5044452548027039\n",
      "iteration 14000, loss 0.5161719918251038\n",
      "TEST accuracy at end of epoch 35: 0.7598\n",
      "TRAIN accuracy at end of epoch 35: 0.81\n",
      "                          epoch 36\n",
      "iteration 14100, loss 0.4999721646308899\n",
      "iteration 14200, loss 0.36143606901168823\n",
      "iteration 14300, loss 0.39043962955474854\n",
      "iteration 14400, loss 0.34808024764060974\n",
      "TEST accuracy at end of epoch 36: 0.776\n",
      "TRAIN accuracy at end of epoch 36: 0.89\n",
      "                          epoch 37\n",
      "iteration 14500, loss 0.5017846822738647\n",
      "iteration 14600, loss 0.4505388140678406\n",
      "iteration 14700, loss 0.45340630412101746\n",
      "iteration 14800, loss 0.3450697064399719\n",
      "TEST accuracy at end of epoch 37: 0.7754\n",
      "TRAIN accuracy at end of epoch 37: 0.81\n",
      "                          epoch 38\n",
      "iteration 14900, loss 0.46778297424316406\n",
      "iteration 15000, loss 0.4784238338470459\n",
      "iteration 15100, loss 0.5270216464996338\n",
      "iteration 15200, loss 0.3687695860862732\n",
      "TEST accuracy at end of epoch 38: 0.7864\n",
      "TRAIN accuracy at end of epoch 38: 0.78\n",
      "                          epoch 39\n",
      "iteration 15300, loss 0.37365269660949707\n",
      "iteration 15400, loss 0.4024808704853058\n",
      "iteration 15500, loss 0.29953041672706604\n",
      "iteration 15600, loss 0.3765983283519745\n",
      "TEST accuracy at end of epoch 39: 0.8221\n",
      "TRAIN accuracy at end of epoch 39: 0.85\n",
      "                          epoch 40\n",
      "iteration 15700, loss 0.5903770923614502\n",
      "iteration 15800, loss 0.24319617450237274\n",
      "iteration 15900, loss 0.3464282751083374\n",
      "iteration 16000, loss 0.44487816095352173\n",
      "TEST accuracy at end of epoch 40: 0.8073\n",
      "TRAIN accuracy at end of epoch 40: 0.93\n",
      "                          epoch 41\n",
      "iteration 16100, loss 0.42814862728118896\n",
      "iteration 16200, loss 0.45667195320129395\n",
      "iteration 16300, loss 0.4756476879119873\n",
      "iteration 16400, loss 0.2851208448410034\n",
      "TEST accuracy at end of epoch 41: 0.7982\n",
      "TRAIN accuracy at end of epoch 41: 0.88\n",
      "                          epoch 42\n",
      "iteration 16500, loss 0.25325673818588257\n",
      "iteration 16600, loss 0.3576242923736572\n",
      "iteration 16700, loss 0.5272037386894226\n",
      "iteration 16800, loss 0.37435027956962585\n",
      "TEST accuracy at end of epoch 42: 0.7373\n",
      "TRAIN accuracy at end of epoch 42: 0.73\n",
      "                          epoch 43\n",
      "iteration 16900, loss 0.3798723816871643\n",
      "iteration 17000, loss 0.4130723476409912\n",
      "iteration 17100, loss 0.4361804723739624\n",
      "iteration 17200, loss 0.3017669916152954\n",
      "TEST accuracy at end of epoch 43: 0.8101\n",
      "TRAIN accuracy at end of epoch 43: 0.88\n",
      "                          epoch 44\n",
      "iteration 17300, loss 0.4044549763202667\n",
      "iteration 17400, loss 0.3305119276046753\n",
      "iteration 17500, loss 0.3956863284111023\n",
      "TEST accuracy at end of epoch 44: 0.8079\n",
      "TRAIN accuracy at end of epoch 44: 0.92\n",
      "                          epoch 45\n",
      "iteration 17600, loss 0.3048264980316162\n",
      "iteration 17700, loss 0.32550084590911865\n",
      "iteration 17800, loss 0.4959583282470703\n",
      "iteration 17900, loss 0.30161404609680176\n",
      "TEST accuracy at end of epoch 45: 0.7898\n",
      "TRAIN accuracy at end of epoch 45: 0.79\n",
      "                          epoch 46\n",
      "iteration 18000, loss 0.4510555863380432\n",
      "iteration 18100, loss 0.36911362409591675\n",
      "iteration 18200, loss 0.5921921133995056\n",
      "iteration 18300, loss 0.40375658869743347\n",
      "TEST accuracy at end of epoch 46: 0.7901\n",
      "TRAIN accuracy at end of epoch 46: 0.85\n",
      "                          epoch 47\n",
      "iteration 18400, loss 0.3729115128517151\n",
      "iteration 18500, loss 0.4581301212310791\n",
      "iteration 18600, loss 0.3756309449672699\n",
      "iteration 18700, loss 0.27147185802459717\n",
      "TEST accuracy at end of epoch 47: 0.7842\n",
      "TRAIN accuracy at end of epoch 47: 0.75\n",
      "                          epoch 48\n",
      "iteration 18800, loss 0.2670271694660187\n",
      "iteration 18900, loss 0.4840853810310364\n",
      "iteration 19000, loss 0.3589138686656952\n",
      "iteration 19100, loss 0.34794342517852783\n",
      "TEST accuracy at end of epoch 48: 0.8097\n",
      "TRAIN accuracy at end of epoch 48: 0.85\n",
      "                          epoch 49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 19200, loss 0.3354564905166626\n",
      "iteration 19300, loss 0.3845059871673584\n",
      "iteration 19400, loss 0.3717205226421356\n",
      "iteration 19500, loss 0.39371031522750854\n",
      "TEST accuracy at end of epoch 49: 0.7889\n",
      "TRAIN accuracy at end of epoch 49: 0.82\n",
      "                          epoch 50\n",
      "iteration 19600, loss 0.47326958179473877\n",
      "iteration 19700, loss 0.31506025791168213\n",
      "iteration 19800, loss 0.3309708833694458\n",
      "iteration 19900, loss 0.2796933352947235\n",
      "TEST accuracy at end of epoch 50: 0.7739\n",
      "TRAIN accuracy at end of epoch 50: 0.83\n",
      "                          epoch 51\n",
      "iteration 20000, loss 0.33715829253196716\n",
      "iteration 20100, loss 0.3061404824256897\n",
      "iteration 20200, loss 0.3880988657474518\n",
      "iteration 20300, loss 0.3511033058166504\n",
      "TEST accuracy at end of epoch 51: 0.7933\n",
      "TRAIN accuracy at end of epoch 51: 0.78\n",
      "                          epoch 52\n",
      "iteration 20400, loss 0.3281763792037964\n",
      "iteration 20500, loss 0.29620984196662903\n",
      "iteration 20600, loss 0.2599935233592987\n",
      "iteration 20700, loss 0.3133963942527771\n",
      "TEST accuracy at end of epoch 52: 0.7939\n",
      "TRAIN accuracy at end of epoch 52: 0.81\n",
      "                          epoch 53\n",
      "iteration 20800, loss 0.4545089602470398\n",
      "iteration 20900, loss 0.271028608083725\n",
      "iteration 21000, loss 0.3231956958770752\n",
      "iteration 21100, loss 0.31684407591819763\n",
      "TEST accuracy at end of epoch 53: 0.8369\n",
      "TRAIN accuracy at end of epoch 53: 0.92\n",
      "                          epoch 54\n",
      "iteration 21200, loss 0.34552329778671265\n",
      "iteration 21300, loss 0.26956668496131897\n",
      "iteration 21400, loss 0.22512187063694\n",
      "iteration 21500, loss 0.3382427394390106\n",
      "TEST accuracy at end of epoch 54: 0.8105\n",
      "TRAIN accuracy at end of epoch 54: 0.89\n",
      "                          epoch 55\n",
      "iteration 21600, loss 0.32081684470176697\n",
      "iteration 21700, loss 0.40180376172065735\n",
      "iteration 21800, loss 0.23856499791145325\n",
      "TEST accuracy at end of epoch 55: 0.8201\n",
      "TRAIN accuracy at end of epoch 55: 0.9\n",
      "                          epoch 56\n",
      "iteration 21900, loss 0.2209792584180832\n",
      "iteration 22000, loss 0.2961682975292206\n",
      "iteration 22100, loss 0.33473092317581177\n",
      "iteration 22200, loss 0.5271261930465698\n",
      "TEST accuracy at end of epoch 56: 0.8157\n",
      "TRAIN accuracy at end of epoch 56: 0.82\n",
      "                          epoch 57\n",
      "iteration 22300, loss 0.19349455833435059\n",
      "iteration 22400, loss 0.24847003817558289\n",
      "iteration 22500, loss 0.2640646994113922\n",
      "iteration 22600, loss 0.348849356174469\n",
      "TEST accuracy at end of epoch 57: 0.8329\n",
      "TRAIN accuracy at end of epoch 57: 0.92\n",
      "                          epoch 58\n",
      "iteration 22700, loss 0.28999796509742737\n",
      "iteration 22800, loss 0.18534837663173676\n",
      "iteration 22900, loss 0.25692951679229736\n",
      "iteration 23000, loss 0.2138252854347229\n",
      "TEST accuracy at end of epoch 58: 0.828\n",
      "TRAIN accuracy at end of epoch 58: 0.89\n",
      "                          epoch 59\n",
      "iteration 23100, loss 0.17774136364459991\n",
      "iteration 23200, loss 0.2531249225139618\n",
      "iteration 23300, loss 0.2646726071834564\n",
      "iteration 23400, loss 0.38046932220458984\n",
      "TEST accuracy at end of epoch 59: 0.8244\n",
      "TRAIN accuracy at end of epoch 59: 0.85\n",
      "                          epoch 60\n",
      "iteration 23500, loss 0.2828161120414734\n",
      "iteration 23600, loss 0.37196701765060425\n",
      "iteration 23700, loss 0.3258836567401886\n",
      "iteration 23800, loss 0.23729507625102997\n",
      "TEST accuracy at end of epoch 60: 0.8083\n",
      "TRAIN accuracy at end of epoch 60: 0.83\n",
      "                          epoch 61\n",
      "iteration 23900, loss 0.3210195302963257\n",
      "iteration 24000, loss 0.3058914542198181\n",
      "iteration 24100, loss 0.3445080518722534\n",
      "iteration 24200, loss 0.4109398126602173\n",
      "TEST accuracy at end of epoch 61: 0.8357\n",
      "TRAIN accuracy at end of epoch 61: 0.81\n",
      "                          epoch 62\n",
      "iteration 24300, loss 0.23271769285202026\n",
      "iteration 24400, loss 0.25977373123168945\n",
      "iteration 24500, loss 0.205582395195961\n",
      "iteration 24600, loss 0.4400647282600403\n",
      "TEST accuracy at end of epoch 62: 0.8097\n",
      "TRAIN accuracy at end of epoch 62: 0.89\n",
      "                          epoch 63\n",
      "iteration 24700, loss 0.20643392205238342\n",
      "iteration 24800, loss 0.3015376329421997\n",
      "iteration 24900, loss 0.29303520917892456\n",
      "iteration 25000, loss 0.43258631229400635\n",
      "TEST accuracy at end of epoch 63: 0.8413\n",
      "TRAIN accuracy at end of epoch 63: 0.9\n",
      "                          epoch 64\n",
      "iteration 25100, loss 0.20801368355751038\n",
      "iteration 25200, loss 0.3268658220767975\n",
      "iteration 25300, loss 0.2851167619228363\n",
      "iteration 25400, loss 0.2672169804573059\n",
      "TEST accuracy at end of epoch 64: 0.8101\n",
      "TRAIN accuracy at end of epoch 64: 0.84\n",
      "                          epoch 65\n",
      "iteration 25500, loss 0.19920435547828674\n",
      "iteration 25600, loss 0.38653793931007385\n",
      "iteration 25700, loss 0.23217007517814636\n",
      "iteration 25800, loss 0.29944610595703125\n",
      "TEST accuracy at end of epoch 65: 0.8146\n",
      "TRAIN accuracy at end of epoch 65: 0.81\n",
      "                          epoch 66\n",
      "iteration 25900, loss 0.2669362425804138\n",
      "iteration 26000, loss 0.24322175979614258\n",
      "iteration 26100, loss 0.2573912739753723\n",
      "TEST accuracy at end of epoch 66: 0.8442\n",
      "TRAIN accuracy at end of epoch 66: 0.9\n",
      "                          epoch 67\n",
      "iteration 26200, loss 0.23510144650936127\n",
      "iteration 26300, loss 0.33014196157455444\n",
      "iteration 26400, loss 0.2548309862613678\n",
      "iteration 26500, loss 0.24167075753211975\n",
      "TEST accuracy at end of epoch 67: 0.8346\n",
      "TRAIN accuracy at end of epoch 67: 0.9\n",
      "                          epoch 68\n",
      "iteration 26600, loss 0.26762908697128296\n",
      "iteration 26700, loss 0.18660034239292145\n",
      "iteration 26800, loss 0.3292141556739807\n",
      "iteration 26900, loss 0.17118513584136963\n",
      "TEST accuracy at end of epoch 68: 0.8439\n",
      "TRAIN accuracy at end of epoch 68: 0.93\n",
      "                          epoch 69\n",
      "iteration 27000, loss 0.19484850764274597\n",
      "iteration 27100, loss 0.18951746821403503\n",
      "iteration 27200, loss 0.23454928398132324\n",
      "iteration 27300, loss 0.1707565188407898\n",
      "TEST accuracy at end of epoch 69: 0.8348\n",
      "TRAIN accuracy at end of epoch 69: 0.89\n",
      "                          epoch 70\n",
      "iteration 27400, loss 0.19086427986621857\n",
      "iteration 27500, loss 0.14829736948013306\n",
      "iteration 27600, loss 0.30860376358032227\n",
      "iteration 27700, loss 0.30359721183776855\n",
      "TEST accuracy at end of epoch 70: 0.8412\n",
      "TRAIN accuracy at end of epoch 70: 0.93\n",
      "                          epoch 71\n",
      "iteration 27800, loss 0.25372564792633057\n",
      "iteration 27900, loss 0.18324066698551178\n",
      "iteration 28000, loss 0.1855984628200531\n",
      "iteration 28100, loss 0.29538190364837646\n",
      "TEST accuracy at end of epoch 71: 0.796\n",
      "TRAIN accuracy at end of epoch 71: 0.82\n",
      "                          epoch 72\n",
      "iteration 28200, loss 0.2012353241443634\n",
      "iteration 28300, loss 0.22325266897678375\n",
      "iteration 28400, loss 0.2026328593492508\n",
      "iteration 28500, loss 0.28700098395347595\n",
      "TEST accuracy at end of epoch 72: 0.841\n",
      "TRAIN accuracy at end of epoch 72: 0.86\n",
      "                          epoch 73\n",
      "iteration 28600, loss 0.15104275941848755\n",
      "iteration 28700, loss 0.17496587336063385\n",
      "iteration 28800, loss 0.35150229930877686\n",
      "iteration 28900, loss 0.309314101934433\n",
      "TEST accuracy at end of epoch 73: 0.8433\n",
      "TRAIN accuracy at end of epoch 73: 0.93\n",
      "                          epoch 74\n",
      "iteration 29000, loss 0.2193501740694046\n",
      "iteration 29100, loss 0.33076319098472595\n",
      "iteration 29200, loss 0.3549598157405853\n",
      "iteration 29300, loss 0.21636557579040527\n",
      "TEST accuracy at end of epoch 74: 0.8299\n",
      "TRAIN accuracy at end of epoch 74: 0.92\n",
      "                          epoch 75\n",
      "iteration 29400, loss 0.19293969869613647\n",
      "iteration 29500, loss 0.24641260504722595\n",
      "iteration 29600, loss 0.20372861623764038\n",
      "iteration 29700, loss 0.21207009255886078\n",
      "TEST accuracy at end of epoch 75: 0.7927\n",
      "TRAIN accuracy at end of epoch 75: 0.87\n",
      "                          epoch 76\n",
      "iteration 29800, loss 0.2761170268058777\n",
      "iteration 29900, loss 0.22960013151168823\n",
      "iteration 30000, loss 0.2962018847465515\n",
      "iteration 30100, loss 0.26317501068115234\n",
      "TEST accuracy at end of epoch 76: 0.8318\n",
      "TRAIN accuracy at end of epoch 76: 0.88\n",
      "                          epoch 77\n",
      "iteration 30200, loss 0.21074500679969788\n",
      "iteration 30300, loss 0.13585853576660156\n",
      "iteration 30400, loss 0.20419223606586456\n",
      "TEST accuracy at end of epoch 77: 0.8457\n",
      "TRAIN accuracy at end of epoch 77: 0.89\n",
      "                          epoch 78\n",
      "iteration 30500, loss 0.1356685906648636\n",
      "iteration 30600, loss 0.1680803894996643\n",
      "iteration 30700, loss 0.3139055371284485\n",
      "iteration 30800, loss 0.2742822766304016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST accuracy at end of epoch 78: 0.823\n",
      "TRAIN accuracy at end of epoch 78: 0.93\n",
      "                          epoch 79\n",
      "iteration 30900, loss 0.2738649845123291\n",
      "iteration 31000, loss 0.13604500889778137\n",
      "iteration 31100, loss 0.24709883332252502\n",
      "iteration 31200, loss 0.27425050735473633\n",
      "TEST accuracy at end of epoch 79: 0.8126\n",
      "TRAIN accuracy at end of epoch 79: 0.85\n",
      "                          epoch 80\n",
      "iteration 31300, loss 0.16582830250263214\n",
      "iteration 31400, loss 0.17508871853351593\n",
      "iteration 31500, loss 0.24825550615787506\n",
      "iteration 31600, loss 0.24995312094688416\n",
      "TEST accuracy at end of epoch 80: 0.8495\n",
      "TRAIN accuracy at end of epoch 80: 0.93\n",
      "                          epoch 81\n",
      "iteration 31700, loss 0.229118213057518\n",
      "iteration 31800, loss 0.19523413479328156\n",
      "iteration 31900, loss 0.19989871978759766\n",
      "iteration 32000, loss 0.2606096565723419\n",
      "TEST accuracy at end of epoch 81: 0.8596\n",
      "TRAIN accuracy at end of epoch 81: 0.92\n",
      "                          epoch 82\n",
      "iteration 32100, loss 0.25290361046791077\n",
      "iteration 32200, loss 0.2749454379081726\n",
      "iteration 32300, loss 0.2009810358285904\n",
      "iteration 32400, loss 0.19115132093429565\n",
      "TEST accuracy at end of epoch 82: 0.862\n",
      "TRAIN accuracy at end of epoch 82: 0.9\n",
      "                          epoch 83\n",
      "iteration 32500, loss 0.18793068826198578\n",
      "iteration 32600, loss 0.18300214409828186\n",
      "iteration 32700, loss 0.21880421042442322\n",
      "iteration 32800, loss 0.300057977437973\n",
      "TEST accuracy at end of epoch 83: 0.8448\n",
      "TRAIN accuracy at end of epoch 83: 0.95\n",
      "                          epoch 84\n",
      "iteration 32900, loss 0.11036507785320282\n",
      "iteration 33000, loss 0.1869402527809143\n",
      "iteration 33100, loss 0.17903031408786774\n",
      "iteration 33200, loss 0.29538214206695557\n",
      "TEST accuracy at end of epoch 84: 0.8217\n",
      "TRAIN accuracy at end of epoch 84: 0.82\n",
      "                          epoch 85\n",
      "iteration 33300, loss 0.22502467036247253\n",
      "iteration 33400, loss 0.19834770262241364\n",
      "iteration 33500, loss 0.2690175175666809\n",
      "iteration 33600, loss 0.16936810314655304\n",
      "TEST accuracy at end of epoch 85: 0.8603\n",
      "TRAIN accuracy at end of epoch 85: 0.97\n",
      "                          epoch 86\n",
      "iteration 33700, loss 0.25428080558776855\n",
      "iteration 33800, loss 0.1005745530128479\n",
      "iteration 33900, loss 0.3384186029434204\n",
      "iteration 34000, loss 0.27365541458129883\n",
      "TEST accuracy at end of epoch 86: 0.8318\n",
      "TRAIN accuracy at end of epoch 86: 0.92\n",
      "                          epoch 87\n",
      "iteration 34100, loss 0.23867131769657135\n",
      "iteration 34200, loss 0.13777145743370056\n",
      "iteration 34300, loss 0.16372302174568176\n",
      "iteration 34400, loss 0.22152996063232422\n",
      "TEST accuracy at end of epoch 87: 0.7798\n",
      "TRAIN accuracy at end of epoch 87: 0.83\n",
      "                          epoch 88\n",
      "iteration 34500, loss 0.23389023542404175\n",
      "iteration 34600, loss 0.2355758249759674\n",
      "iteration 34700, loss 0.19361014664173126\n",
      "TEST accuracy at end of epoch 88: 0.8565\n",
      "TRAIN accuracy at end of epoch 88: 0.91\n",
      "                          epoch 89\n",
      "iteration 34800, loss 0.23411990702152252\n",
      "iteration 34900, loss 0.3476940393447876\n",
      "iteration 35000, loss 0.1542060524225235\n",
      "iteration 35100, loss 0.1937657594680786\n",
      "TEST accuracy at end of epoch 89: 0.8412\n",
      "TRAIN accuracy at end of epoch 89: 0.92\n",
      "                          epoch 90\n",
      "iteration 35200, loss 0.16882732510566711\n",
      "iteration 35300, loss 0.19896958768367767\n",
      "iteration 35400, loss 0.12094183266162872\n",
      "iteration 35500, loss 0.12534168362617493\n",
      "TEST accuracy at end of epoch 90: 0.8124\n",
      "TRAIN accuracy at end of epoch 90: 0.88\n",
      "                          epoch 91\n",
      "iteration 35600, loss 0.21033145487308502\n",
      "iteration 35700, loss 0.14829181134700775\n",
      "iteration 35800, loss 0.33465003967285156\n",
      "iteration 35900, loss 0.16813775897026062\n",
      "TEST accuracy at end of epoch 91: 0.845\n",
      "TRAIN accuracy at end of epoch 91: 0.87\n",
      "                          epoch 92\n",
      "iteration 36000, loss 0.1123015433549881\n",
      "iteration 36100, loss 0.18793267011642456\n",
      "iteration 36200, loss 0.31730708479881287\n",
      "iteration 36300, loss 0.08790641278028488\n",
      "TEST accuracy at end of epoch 92: 0.8454\n",
      "TRAIN accuracy at end of epoch 92: 0.93\n",
      "                          epoch 93\n",
      "iteration 36400, loss 0.18052369356155396\n",
      "iteration 36500, loss 0.23242445290088654\n",
      "iteration 36600, loss 0.15862441062927246\n",
      "iteration 36700, loss 0.19319388270378113\n",
      "TEST accuracy at end of epoch 93: 0.8445\n",
      "TRAIN accuracy at end of epoch 93: 0.95\n",
      "                          epoch 94\n",
      "iteration 36800, loss 0.24603673815727234\n",
      "iteration 36900, loss 0.18425290286540985\n",
      "iteration 37000, loss 0.08639771491289139\n",
      "iteration 37100, loss 0.16864463686943054\n",
      "TEST accuracy at end of epoch 94: 0.8691\n",
      "TRAIN accuracy at end of epoch 94: 0.89\n",
      "                          epoch 95\n",
      "iteration 37200, loss 0.1358608603477478\n",
      "iteration 37300, loss 0.1696884036064148\n",
      "iteration 37400, loss 0.12975627183914185\n",
      "iteration 37500, loss 0.18870985507965088\n",
      "TEST accuracy at end of epoch 95: 0.825\n",
      "TRAIN accuracy at end of epoch 95: 0.86\n",
      "                          epoch 96\n",
      "iteration 37600, loss 0.25732916593551636\n",
      "iteration 37700, loss 0.1999213993549347\n",
      "iteration 37800, loss 0.15568213164806366\n",
      "iteration 37900, loss 0.18511025607585907\n",
      "TEST accuracy at end of epoch 96: 0.8567\n",
      "TRAIN accuracy at end of epoch 96: 0.97\n",
      "                          epoch 97\n",
      "iteration 38000, loss 0.22148117423057556\n",
      "iteration 38100, loss 0.11322639137506485\n",
      "iteration 38200, loss 0.16394156217575073\n",
      "iteration 38300, loss 0.17019233107566833\n",
      "TEST accuracy at end of epoch 97: 0.8459\n",
      "TRAIN accuracy at end of epoch 97: 0.96\n",
      "                          epoch 98\n",
      "iteration 38400, loss 0.1586279273033142\n",
      "iteration 38500, loss 0.11101322621107101\n",
      "iteration 38600, loss 0.18135207891464233\n",
      "iteration 38700, loss 0.22400149703025818\n",
      "TEST accuracy at end of epoch 98: 0.8626\n",
      "TRAIN accuracy at end of epoch 98: 0.96\n",
      "                          epoch 99\n",
      "iteration 38800, loss 0.2997223138809204\n",
      "iteration 38900, loss 0.18745632469654083\n",
      "iteration 39000, loss 0.132333442568779\n",
      "iteration 39100, loss 0.21325358748435974\n",
      "TEST accuracy at end of epoch 99: 0.8542\n",
      "TRAIN accuracy at end of epoch 99: 0.94\n",
      "                          epoch 100\n",
      "iteration 39200, loss 0.033218421041965485\n",
      "iteration 39300, loss 0.11383102089166641\n",
      "iteration 39400, loss 0.15211030840873718\n",
      "TEST accuracy at end of epoch 100: 0.8445\n",
      "TRAIN accuracy at end of epoch 100: 0.96\n",
      "                          epoch 101\n",
      "iteration 39500, loss 0.08822676539421082\n",
      "iteration 39600, loss 0.2039869725704193\n",
      "iteration 39700, loss 0.08950379490852356\n",
      "iteration 39800, loss 0.25826266407966614\n",
      "TEST accuracy at end of epoch 101: 0.8519\n",
      "TRAIN accuracy at end of epoch 101: 0.91\n",
      "                          epoch 102\n",
      "iteration 39900, loss 0.1045110821723938\n",
      "iteration 40000, loss 0.15310034155845642\n",
      "iteration 40100, loss 0.13216239213943481\n",
      "iteration 40200, loss 0.13824298977851868\n",
      "TEST accuracy at end of epoch 102: 0.8664\n",
      "TRAIN accuracy at end of epoch 102: 0.96\n",
      "                          epoch 103\n",
      "iteration 40300, loss 0.18151047825813293\n",
      "iteration 40400, loss 0.15225985646247864\n",
      "iteration 40500, loss 0.1836605668067932\n",
      "iteration 40600, loss 0.1351066529750824\n",
      "TEST accuracy at end of epoch 103: 0.8665\n",
      "TRAIN accuracy at end of epoch 103: 0.92\n",
      "                          epoch 104\n",
      "iteration 40700, loss 0.1215704083442688\n",
      "iteration 40800, loss 0.16180990636348724\n",
      "iteration 40900, loss 0.15368692576885223\n",
      "iteration 41000, loss 0.12054786086082458\n",
      "TEST accuracy at end of epoch 104: 0.8607\n",
      "TRAIN accuracy at end of epoch 104: 0.97\n",
      "                          epoch 105\n",
      "iteration 41100, loss 0.13934868574142456\n",
      "iteration 41200, loss 0.12958911061286926\n",
      "iteration 41300, loss 0.07993943989276886\n",
      "iteration 41400, loss 0.09876085072755814\n",
      "TEST accuracy at end of epoch 105: 0.8624\n",
      "TRAIN accuracy at end of epoch 105: 0.98\n",
      "                          epoch 106\n",
      "iteration 41500, loss 0.1614822894334793\n",
      "iteration 41600, loss 0.15429803729057312\n",
      "iteration 41700, loss 0.03922067955136299\n",
      "iteration 41800, loss 0.14262500405311584\n",
      "TEST accuracy at end of epoch 106: 0.8454\n",
      "TRAIN accuracy at end of epoch 106: 0.96\n",
      "                          epoch 107\n",
      "iteration 41900, loss 0.2107650637626648\n",
      "iteration 42000, loss 0.09948672354221344\n",
      "iteration 42100, loss 0.20072492957115173\n",
      "iteration 42200, loss 0.1408270299434662\n",
      "TEST accuracy at end of epoch 107: 0.861\n",
      "TRAIN accuracy at end of epoch 107: 0.96\n",
      "                          epoch 108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 42300, loss 0.06406202167272568\n",
      "iteration 42400, loss 0.12383398413658142\n",
      "iteration 42500, loss 0.14053192734718323\n",
      "iteration 42600, loss 0.17733025550842285\n",
      "TEST accuracy at end of epoch 108: 0.8628\n",
      "TRAIN accuracy at end of epoch 108: 0.98\n",
      "                          epoch 109\n",
      "iteration 42700, loss 0.09916064143180847\n",
      "iteration 42800, loss 0.18961521983146667\n",
      "iteration 42900, loss 0.1921842247247696\n",
      "iteration 43000, loss 0.11640770733356476\n",
      "TEST accuracy at end of epoch 109: 0.8635\n",
      "TRAIN accuracy at end of epoch 109: 0.97\n",
      "                          epoch 110\n",
      "iteration 43100, loss 0.08879601955413818\n",
      "iteration 43200, loss 0.17023663222789764\n",
      "iteration 43300, loss 0.19899176061153412\n",
      "iteration 43400, loss 0.1486048400402069\n",
      "TEST accuracy at end of epoch 110: 0.8608\n",
      "TRAIN accuracy at end of epoch 110: 0.94\n",
      "                          epoch 111\n",
      "iteration 43500, loss 0.18098610639572144\n",
      "iteration 43600, loss 0.059087395668029785\n",
      "iteration 43700, loss 0.08535106480121613\n",
      "TEST accuracy at end of epoch 111: 0.8293\n",
      "TRAIN accuracy at end of epoch 111: 0.9\n",
      "                          epoch 112\n",
      "iteration 43800, loss 0.15454621613025665\n",
      "iteration 43900, loss 0.22343529760837555\n",
      "iteration 44000, loss 0.13390299677848816\n",
      "iteration 44100, loss 0.14641748368740082\n",
      "TEST accuracy at end of epoch 112: 0.8679\n",
      "TRAIN accuracy at end of epoch 112: 0.96\n",
      "                          epoch 113\n",
      "iteration 44200, loss 0.14345942437648773\n",
      "iteration 44300, loss 0.12425045669078827\n",
      "iteration 44400, loss 0.0889943465590477\n",
      "iteration 44500, loss 0.18424955010414124\n",
      "TEST accuracy at end of epoch 113: 0.866\n",
      "TRAIN accuracy at end of epoch 113: 0.94\n",
      "                          epoch 114\n",
      "iteration 44600, loss 0.08741636574268341\n",
      "iteration 44700, loss 0.1663031280040741\n",
      "iteration 44800, loss 0.0922980010509491\n",
      "iteration 44900, loss 0.1568032205104828\n",
      "TEST accuracy at end of epoch 114: 0.8613\n",
      "TRAIN accuracy at end of epoch 114: 0.94\n",
      "                          epoch 115\n",
      "iteration 45000, loss 0.09330485016107559\n",
      "iteration 45100, loss 0.056598320603370667\n",
      "iteration 45200, loss 0.185527503490448\n",
      "iteration 45300, loss 0.11435581743717194\n",
      "TEST accuracy at end of epoch 115: 0.8547\n",
      "TRAIN accuracy at end of epoch 115: 1.0\n",
      "                          epoch 116\n",
      "iteration 45400, loss 0.09822145104408264\n",
      "iteration 45500, loss 0.10690510272979736\n",
      "iteration 45600, loss 0.11751069873571396\n",
      "iteration 45700, loss 0.14310792088508606\n",
      "TEST accuracy at end of epoch 116: 0.8601\n",
      "TRAIN accuracy at end of epoch 116: 0.97\n",
      "                          epoch 117\n",
      "iteration 45800, loss 0.09795239567756653\n",
      "iteration 45900, loss 0.10379384458065033\n",
      "iteration 46000, loss 0.17693060636520386\n",
      "iteration 46100, loss 0.07584570348262787\n",
      "TEST accuracy at end of epoch 117: 0.8648\n",
      "TRAIN accuracy at end of epoch 117: 0.99\n",
      "                          epoch 118\n",
      "iteration 46200, loss 0.10148049145936966\n",
      "iteration 46300, loss 0.06970643252134323\n",
      "iteration 46400, loss 0.1236710250377655\n",
      "iteration 46500, loss 0.06659215688705444\n",
      "TEST accuracy at end of epoch 118: 0.8604\n",
      "TRAIN accuracy at end of epoch 118: 0.96\n",
      "                          epoch 119\n",
      "iteration 46600, loss 0.11377783864736557\n",
      "iteration 46700, loss 0.12691298127174377\n",
      "iteration 46800, loss 0.07792114466428757\n",
      "iteration 46900, loss 0.19860142469406128\n",
      "TEST accuracy at end of epoch 119: 0.8716\n",
      "TRAIN accuracy at end of epoch 119: 0.98\n",
      "                          epoch 120\n",
      "iteration 47000, loss 0.09862422943115234\n",
      "iteration 47100, loss 0.16513961553573608\n",
      "iteration 47200, loss 0.09844617545604706\n",
      "iteration 47300, loss 0.08002279698848724\n",
      "TEST accuracy at end of epoch 120: 0.8568\n",
      "TRAIN accuracy at end of epoch 120: 0.99\n",
      "                          epoch 121\n",
      "iteration 47400, loss 0.0931386947631836\n",
      "iteration 47500, loss 0.06849272549152374\n",
      "iteration 47600, loss 0.14243456721305847\n",
      "iteration 47700, loss 0.06604799628257751\n",
      "TEST accuracy at end of epoch 121: 0.872\n",
      "TRAIN accuracy at end of epoch 121: 0.96\n",
      "                          epoch 122\n",
      "iteration 47800, loss 0.06058408319950104\n",
      "iteration 47900, loss 0.06690037250518799\n",
      "iteration 48000, loss 0.1955273151397705\n",
      "TEST accuracy at end of epoch 122: 0.8712\n",
      "TRAIN accuracy at end of epoch 122: 0.98\n",
      "                          epoch 123\n",
      "iteration 48100, loss 0.059870727360248566\n",
      "iteration 48200, loss 0.09656225144863129\n",
      "iteration 48300, loss 0.14257775247097015\n",
      "iteration 48400, loss 0.07935258001089096\n",
      "TEST accuracy at end of epoch 123: 0.8666\n",
      "TRAIN accuracy at end of epoch 123: 0.99\n",
      "                          epoch 124\n",
      "iteration 48500, loss 0.09094932675361633\n",
      "iteration 48600, loss 0.05426052212715149\n",
      "iteration 48700, loss 0.0726146399974823\n",
      "iteration 48800, loss 0.2753303050994873\n",
      "TEST accuracy at end of epoch 124: 0.858\n",
      "TRAIN accuracy at end of epoch 124: 0.97\n",
      "                          epoch 125\n",
      "iteration 48900, loss 0.10443750768899918\n",
      "iteration 49000, loss 0.1523362547159195\n",
      "iteration 49100, loss 0.06541024148464203\n",
      "iteration 49200, loss 0.02931906096637249\n",
      "TEST accuracy at end of epoch 125: 0.8753\n",
      "TRAIN accuracy at end of epoch 125: 0.97\n",
      "                          epoch 126\n",
      "iteration 49300, loss 0.06170117110013962\n",
      "iteration 49400, loss 0.12559783458709717\n",
      "iteration 49500, loss 0.07768399268388748\n",
      "iteration 49600, loss 0.15542395412921906\n",
      "TEST accuracy at end of epoch 126: 0.8753\n",
      "TRAIN accuracy at end of epoch 126: 0.98\n",
      "                          epoch 127\n",
      "iteration 49700, loss 0.039807405322790146\n",
      "iteration 49800, loss 0.08146456629037857\n",
      "iteration 49900, loss 0.07532013952732086\n",
      "iteration 50000, loss 0.15189224481582642\n",
      "TEST accuracy at end of epoch 127: 0.872\n",
      "TRAIN accuracy at end of epoch 127: 0.96\n",
      "                          epoch 128\n",
      "iteration 50100, loss 0.03368818014860153\n",
      "iteration 50200, loss 0.11918994784355164\n",
      "iteration 50300, loss 0.05317354202270508\n",
      "iteration 50400, loss 0.1347847580909729\n",
      "TEST accuracy at end of epoch 128: 0.8713\n",
      "TRAIN accuracy at end of epoch 128: 0.98\n",
      "                          epoch 129\n",
      "iteration 50500, loss 0.048572324216365814\n",
      "iteration 50600, loss 0.08778868615627289\n",
      "iteration 50700, loss 0.09244735538959503\n",
      "iteration 50800, loss 0.0392913743853569\n",
      "TEST accuracy at end of epoch 129: 0.8721\n",
      "TRAIN accuracy at end of epoch 129: 0.96\n",
      "                          epoch 130\n",
      "iteration 50900, loss 0.1042531281709671\n",
      "iteration 51000, loss 0.07310900092124939\n",
      "iteration 51100, loss 0.09924538433551788\n",
      "iteration 51200, loss 0.10191810876131058\n",
      "TEST accuracy at end of epoch 130: 0.8673\n",
      "TRAIN accuracy at end of epoch 130: 0.98\n",
      "                          epoch 131\n",
      "iteration 51300, loss 0.14159804582595825\n",
      "iteration 51400, loss 0.07667265087366104\n",
      "iteration 51500, loss 0.07593649625778198\n",
      "iteration 51600, loss 0.03736111894249916\n",
      "TEST accuracy at end of epoch 131: 0.8741\n",
      "TRAIN accuracy at end of epoch 131: 0.99\n",
      "                          epoch 132\n",
      "iteration 51700, loss 0.06800030916929245\n",
      "iteration 51800, loss 0.05045619606971741\n",
      "iteration 51900, loss 0.06325341761112213\n",
      "iteration 52000, loss 0.12864525616168976\n",
      "TEST accuracy at end of epoch 132: 0.8606\n",
      "TRAIN accuracy at end of epoch 132: 0.95\n",
      "                          epoch 133\n",
      "iteration 52100, loss 0.11112724244594574\n",
      "iteration 52200, loss 0.0814281702041626\n",
      "iteration 52300, loss 0.024386737495660782\n",
      "TEST accuracy at end of epoch 133: 0.8699\n",
      "TRAIN accuracy at end of epoch 133: 0.99\n",
      "                          epoch 134\n",
      "iteration 52400, loss 0.043609440326690674\n",
      "iteration 52500, loss 0.16314181685447693\n",
      "iteration 52600, loss 0.07207553833723068\n",
      "iteration 52700, loss 0.1210549920797348\n",
      "TEST accuracy at end of epoch 134: 0.8745\n",
      "TRAIN accuracy at end of epoch 134: 0.99\n",
      "                          epoch 135\n",
      "iteration 52800, loss 0.06386345624923706\n",
      "iteration 52900, loss 0.08309849351644516\n",
      "iteration 53000, loss 0.09184812009334564\n",
      "iteration 53100, loss 0.0808207169175148\n",
      "TEST accuracy at end of epoch 135: 0.873\n",
      "TRAIN accuracy at end of epoch 135: 0.98\n",
      "                          epoch 136\n",
      "iteration 53200, loss 0.06647834181785583\n",
      "iteration 53300, loss 0.09206771850585938\n",
      "iteration 53400, loss 0.08030298352241516\n",
      "iteration 53500, loss 0.11469348520040512\n",
      "TEST accuracy at end of epoch 136: 0.8741\n",
      "TRAIN accuracy at end of epoch 136: 0.97\n",
      "                          epoch 137\n",
      "iteration 53600, loss 0.045883581042289734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 53700, loss 0.0435982272028923\n",
      "iteration 53800, loss 0.08458128571510315\n",
      "iteration 53900, loss 0.05779968574643135\n",
      "TEST accuracy at end of epoch 137: 0.8788\n",
      "TRAIN accuracy at end of epoch 137: 0.99\n",
      "                          epoch 138\n",
      "iteration 54000, loss 0.08871472626924515\n",
      "iteration 54100, loss 0.079686738550663\n",
      "iteration 54200, loss 0.0880758985877037\n",
      "iteration 54300, loss 0.06979342550039291\n",
      "TEST accuracy at end of epoch 138: 0.8717\n",
      "TRAIN accuracy at end of epoch 138: 0.98\n",
      "                          epoch 139\n",
      "iteration 54400, loss 0.03678430616855621\n",
      "iteration 54500, loss 0.1074666678905487\n",
      "iteration 54600, loss 0.10040445625782013\n",
      "iteration 54700, loss 0.09749269485473633\n",
      "TEST accuracy at end of epoch 139: 0.8718\n",
      "TRAIN accuracy at end of epoch 139: 0.99\n",
      "                          epoch 140\n",
      "iteration 54800, loss 0.032910872250795364\n",
      "iteration 54900, loss 0.1496882140636444\n",
      "iteration 55000, loss 0.024904705584049225\n",
      "iteration 55100, loss 0.0483991876244545\n",
      "TEST accuracy at end of epoch 140: 0.8764\n",
      "TRAIN accuracy at end of epoch 140: 0.98\n",
      "                          epoch 141\n",
      "iteration 55200, loss 0.0593007393181324\n",
      "iteration 55300, loss 0.1006077378988266\n",
      "iteration 55400, loss 0.13263240456581116\n",
      "iteration 55500, loss 0.024063408374786377\n",
      "TEST accuracy at end of epoch 141: 0.8612\n",
      "TRAIN accuracy at end of epoch 141: 0.97\n",
      "                          epoch 142\n",
      "iteration 55600, loss 0.06192328780889511\n",
      "iteration 55700, loss 0.1320682168006897\n",
      "iteration 55800, loss 0.05215703323483467\n",
      "iteration 55900, loss 0.06894833594560623\n",
      "TEST accuracy at end of epoch 142: 0.87\n",
      "TRAIN accuracy at end of epoch 142: 0.97\n",
      "                          epoch 143\n",
      "iteration 56000, loss 0.054768308997154236\n",
      "iteration 56100, loss 0.03210624307394028\n",
      "iteration 56200, loss 0.09502139687538147\n",
      "iteration 56300, loss 0.039991363883018494\n",
      "TEST accuracy at end of epoch 143: 0.8713\n",
      "TRAIN accuracy at end of epoch 143: 0.98\n",
      "                          epoch 144\n",
      "iteration 56400, loss 0.07577309012413025\n",
      "iteration 56500, loss 0.06999744474887848\n",
      "iteration 56600, loss 0.06893380731344223\n",
      "TEST accuracy at end of epoch 144: 0.8778\n",
      "TRAIN accuracy at end of epoch 144: 0.99\n",
      "                          epoch 145\n",
      "iteration 56700, loss 0.09675262868404388\n",
      "iteration 56800, loss 0.021844523027539253\n",
      "iteration 56900, loss 0.0614178329706192\n",
      "iteration 57000, loss 0.07424858212471008\n",
      "TEST accuracy at end of epoch 145: 0.8709\n",
      "TRAIN accuracy at end of epoch 145: 0.98\n",
      "                          epoch 146\n",
      "iteration 57100, loss 0.11306159198284149\n",
      "iteration 57200, loss 0.048849157989025116\n",
      "iteration 57300, loss 0.06239389255642891\n",
      "iteration 57400, loss 0.04362725839018822\n",
      "TEST accuracy at end of epoch 146: 0.8748\n",
      "TRAIN accuracy at end of epoch 146: 0.97\n",
      "                          epoch 147\n",
      "iteration 57500, loss 0.07152515649795532\n",
      "iteration 57600, loss 0.05860540270805359\n",
      "iteration 57700, loss 0.07130805402994156\n",
      "iteration 57800, loss 0.03567202761769295\n",
      "TEST accuracy at end of epoch 147: 0.871\n",
      "TRAIN accuracy at end of epoch 147: 0.97\n",
      "                          epoch 148\n",
      "iteration 57900, loss 0.06908201426267624\n",
      "iteration 58000, loss 0.04044916853308678\n",
      "iteration 58100, loss 0.035578422248363495\n",
      "iteration 58200, loss 0.110565185546875\n",
      "TEST accuracy at end of epoch 148: 0.8701\n",
      "TRAIN accuracy at end of epoch 148: 0.99\n",
      "                          epoch 149\n",
      "iteration 58300, loss 0.0285754706710577\n",
      "iteration 58400, loss 0.062271177768707275\n",
      "iteration 58500, loss 0.044944729655981064\n",
      "iteration 58600, loss 0.05449172854423523\n",
      "TEST accuracy at end of epoch 149: 0.8774\n",
      "TRAIN accuracy at end of epoch 149: 1.0\n",
      "                          epoch 150\n",
      "iteration 58700, loss 0.08398891240358353\n",
      "iteration 58800, loss 0.07793618738651276\n",
      "iteration 58900, loss 0.03683033213019371\n",
      "iteration 59000, loss 0.03927510604262352\n",
      "TEST accuracy at end of epoch 150: 0.8589\n",
      "TRAIN accuracy at end of epoch 150: 0.96\n",
      "                          epoch 151\n",
      "iteration 59100, loss 0.04593268781900406\n",
      "iteration 59200, loss 0.052546437829732895\n",
      "iteration 59300, loss 0.07489892840385437\n",
      "iteration 59400, loss 0.0555221363902092\n",
      "TEST accuracy at end of epoch 151: 0.8755\n",
      "TRAIN accuracy at end of epoch 151: 1.0\n",
      "                          epoch 152\n",
      "iteration 59500, loss 0.018858861178159714\n",
      "iteration 59600, loss 0.058310650289058685\n",
      "iteration 59700, loss 0.09613433480262756\n",
      "iteration 59800, loss 0.07357309013605118\n",
      "TEST accuracy at end of epoch 152: 0.8774\n",
      "TRAIN accuracy at end of epoch 152: 0.98\n",
      "                          epoch 153\n",
      "iteration 59900, loss 0.0551949106156826\n",
      "iteration 60000, loss 0.07363366335630417\n",
      "iteration 60100, loss 0.10038022696971893\n",
      "iteration 60200, loss 0.06658865511417389\n",
      "TEST accuracy at end of epoch 153: 0.8776\n",
      "TRAIN accuracy at end of epoch 153: 1.0\n",
      "                          epoch 154\n",
      "iteration 60300, loss 0.032259028404951096\n",
      "iteration 60400, loss 0.09749214351177216\n",
      "iteration 60500, loss 0.038265470415353775\n",
      "iteration 60600, loss 0.05588807910680771\n",
      "TEST accuracy at end of epoch 154: 0.8806\n",
      "TRAIN accuracy at end of epoch 154: 1.0\n",
      "                          epoch 155\n",
      "iteration 60700, loss 0.03454354405403137\n",
      "iteration 60800, loss 0.04738583415746689\n",
      "iteration 60900, loss 0.02782263606786728\n",
      "TEST accuracy at end of epoch 155: 0.877\n",
      "TRAIN accuracy at end of epoch 155: 0.99\n",
      "                          epoch 156\n",
      "iteration 61000, loss 0.044747624546289444\n",
      "iteration 61100, loss 0.06439429521560669\n",
      "iteration 61200, loss 0.057559311389923096\n",
      "iteration 61300, loss 0.14437934756278992\n",
      "TEST accuracy at end of epoch 156: 0.8776\n",
      "TRAIN accuracy at end of epoch 156: 0.99\n",
      "                          epoch 157\n",
      "iteration 61400, loss 0.019050033763051033\n",
      "iteration 61500, loss 0.04529503732919693\n",
      "iteration 61600, loss 0.07651996612548828\n",
      "iteration 61700, loss 0.07783055305480957\n",
      "TEST accuracy at end of epoch 157: 0.8787\n",
      "TRAIN accuracy at end of epoch 157: 1.0\n",
      "                          epoch 158\n",
      "iteration 61800, loss 0.0547894649207592\n",
      "iteration 61900, loss 0.022884458303451538\n",
      "iteration 62000, loss 0.0851813554763794\n",
      "iteration 62100, loss 0.09172184020280838\n",
      "TEST accuracy at end of epoch 158: 0.8767\n",
      "TRAIN accuracy at end of epoch 158: 0.98\n",
      "                          epoch 159\n",
      "iteration 62200, loss 0.01722179912030697\n",
      "iteration 62300, loss 0.10459968447685242\n",
      "iteration 62400, loss 0.05441081151366234\n",
      "iteration 62500, loss 0.03938835859298706\n",
      "TEST accuracy at end of epoch 159: 0.8757\n",
      "TRAIN accuracy at end of epoch 159: 0.99\n",
      "                          epoch 160\n",
      "iteration 62600, loss 0.039260901510715485\n",
      "iteration 62700, loss 0.06160138174891472\n",
      "iteration 62800, loss 0.0801662802696228\n",
      "iteration 62900, loss 0.07760084420442581\n",
      "TEST accuracy at end of epoch 160: 0.8811\n",
      "TRAIN accuracy at end of epoch 160: 0.97\n",
      "                          epoch 161\n",
      "iteration 63000, loss 0.08448582887649536\n",
      "iteration 63100, loss 0.13053110241889954\n",
      "iteration 63200, loss 0.015846211463212967\n",
      "iteration 63300, loss 0.02146974951028824\n",
      "TEST accuracy at end of epoch 161: 0.8816\n",
      "TRAIN accuracy at end of epoch 161: 1.0\n",
      "                          epoch 162\n",
      "iteration 63400, loss 0.044933997094631195\n",
      "iteration 63500, loss 0.03894881531596184\n",
      "iteration 63600, loss 0.05005037039518356\n",
      "iteration 63700, loss 0.08798778057098389\n",
      "TEST accuracy at end of epoch 162: 0.8811\n",
      "TRAIN accuracy at end of epoch 162: 1.0\n",
      "                          epoch 163\n",
      "iteration 63800, loss 0.014434294775128365\n",
      "iteration 63900, loss 0.10488376021385193\n",
      "iteration 64000, loss 0.07290951907634735\n",
      "iteration 64100, loss 0.04617682471871376\n",
      "TEST accuracy at end of epoch 163: 0.8835\n",
      "TRAIN accuracy at end of epoch 163: 0.99\n",
      "                          epoch 164\n",
      "iteration 64200, loss 0.016229301691055298\n",
      "iteration 64300, loss 0.045678652822971344\n",
      "iteration 64400, loss 0.08122573792934418\n",
      "iteration 64500, loss 0.030293622985482216\n",
      "TEST accuracy at end of epoch 164: 0.882\n",
      "TRAIN accuracy at end of epoch 164: 1.0\n",
      "                          epoch 165\n",
      "iteration 64600, loss 0.07141026854515076\n",
      "iteration 64700, loss 0.04446481913328171\n",
      "iteration 64800, loss 0.03972453624010086\n",
      "iteration 64900, loss 0.013863551430404186\n",
      "TEST accuracy at end of epoch 165: 0.883\n",
      "TRAIN accuracy at end of epoch 165: 0.99\n",
      "                          epoch 166\n",
      "iteration 65000, loss 0.05214901641011238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 65100, loss 0.005792560987174511\n",
      "iteration 65200, loss 0.11999322474002838\n",
      "TEST accuracy at end of epoch 166: 0.881\n",
      "TRAIN accuracy at end of epoch 166: 1.0\n",
      "                          epoch 167\n",
      "iteration 65300, loss 0.00945739634335041\n",
      "iteration 65400, loss 0.03124172054231167\n",
      "iteration 65500, loss 0.042482808232307434\n",
      "iteration 65600, loss 0.03026653826236725\n",
      "TEST accuracy at end of epoch 167: 0.8828\n",
      "TRAIN accuracy at end of epoch 167: 0.98\n",
      "                          epoch 168\n",
      "iteration 65700, loss 0.030951838940382004\n",
      "iteration 65800, loss 0.04893783852458\n",
      "iteration 65900, loss 0.062052324414253235\n",
      "iteration 66000, loss 0.07849305123090744\n",
      "TEST accuracy at end of epoch 168: 0.8827\n",
      "TRAIN accuracy at end of epoch 168: 1.0\n",
      "                          epoch 169\n",
      "iteration 66100, loss 0.02049271948635578\n",
      "iteration 66200, loss 0.020768089219927788\n",
      "iteration 66300, loss 0.040659330785274506\n",
      "iteration 66400, loss 0.020887207239866257\n",
      "TEST accuracy at end of epoch 169: 0.8831\n",
      "TRAIN accuracy at end of epoch 169: 1.0\n",
      "                          epoch 170\n",
      "iteration 66500, loss 0.031223289668560028\n",
      "iteration 66600, loss 0.03899737447500229\n",
      "iteration 66700, loss 0.02643164061009884\n",
      "iteration 66800, loss 0.03702269122004509\n",
      "TEST accuracy at end of epoch 170: 0.8817\n",
      "TRAIN accuracy at end of epoch 170: 1.0\n",
      "                          epoch 171\n",
      "iteration 66900, loss 0.05934577062726021\n",
      "iteration 67000, loss 0.061900120228528976\n",
      "iteration 67100, loss 0.006021942477673292\n",
      "iteration 67200, loss 0.036960095167160034\n",
      "TEST accuracy at end of epoch 171: 0.8814\n",
      "TRAIN accuracy at end of epoch 171: 0.99\n",
      "                          epoch 172\n",
      "iteration 67300, loss 0.05816307291388512\n",
      "iteration 67400, loss 0.04210503399372101\n",
      "iteration 67500, loss 0.03335213661193848\n",
      "iteration 67600, loss 0.0445244237780571\n",
      "TEST accuracy at end of epoch 172: 0.884\n",
      "TRAIN accuracy at end of epoch 172: 1.0\n",
      "                          epoch 173\n",
      "iteration 67700, loss 0.028010770678520203\n",
      "iteration 67800, loss 0.005866918247193098\n",
      "iteration 67900, loss 0.01890254020690918\n",
      "iteration 68000, loss 0.03861304745078087\n",
      "TEST accuracy at end of epoch 173: 0.8822\n",
      "TRAIN accuracy at end of epoch 173: 0.99\n",
      "                          epoch 174\n",
      "iteration 68100, loss 0.024460550397634506\n",
      "iteration 68200, loss 0.010367373935878277\n",
      "iteration 68300, loss 0.052840542048215866\n",
      "iteration 68400, loss 0.04017939791083336\n",
      "TEST accuracy at end of epoch 174: 0.882\n",
      "TRAIN accuracy at end of epoch 174: 0.99\n",
      "                          epoch 175\n",
      "iteration 68500, loss 0.032238125801086426\n",
      "iteration 68600, loss 0.0548006147146225\n",
      "iteration 68700, loss 0.015845462679862976\n",
      "iteration 68800, loss 0.03377624601125717\n",
      "TEST accuracy at end of epoch 175: 0.8822\n",
      "TRAIN accuracy at end of epoch 175: 1.0\n",
      "                          epoch 176\n",
      "iteration 68900, loss 0.047701746225357056\n",
      "iteration 69000, loss 0.0608801394701004\n",
      "iteration 69100, loss 0.08562785387039185\n",
      "iteration 69200, loss 0.029770340770483017\n",
      "TEST accuracy at end of epoch 176: 0.8831\n",
      "TRAIN accuracy at end of epoch 176: 1.0\n",
      "                          epoch 177\n",
      "iteration 69300, loss 0.0484141930937767\n",
      "iteration 69400, loss 0.05479045212268829\n",
      "iteration 69500, loss 0.02144452929496765\n",
      "TEST accuracy at end of epoch 177: 0.8831\n",
      "TRAIN accuracy at end of epoch 177: 1.0\n",
      "                          epoch 178\n",
      "iteration 69600, loss 0.022317662835121155\n",
      "iteration 69700, loss 0.10764090716838837\n",
      "iteration 69800, loss 0.031533025205135345\n",
      "iteration 69900, loss 0.009451501071453094\n",
      "TEST accuracy at end of epoch 178: 0.8843\n",
      "TRAIN accuracy at end of epoch 178: 1.0\n",
      "                          epoch 179\n",
      "iteration 70000, loss 0.024968769401311874\n",
      "iteration 70100, loss 0.02859138324856758\n",
      "iteration 70200, loss 0.03511423245072365\n",
      "iteration 70300, loss 0.007288279011845589\n",
      "TEST accuracy at end of epoch 179: 0.8836\n",
      "TRAIN accuracy at end of epoch 179: 1.0\n",
      "                          epoch 180\n",
      "iteration 70400, loss 0.03738858178257942\n",
      "iteration 70500, loss 0.025785962119698524\n",
      "iteration 70600, loss 0.0513441301882267\n",
      "iteration 70700, loss 0.014527110382914543\n",
      "TEST accuracy at end of epoch 180: 0.8807\n",
      "TRAIN accuracy at end of epoch 180: 1.0\n",
      "                          epoch 181\n",
      "iteration 70800, loss 0.042628683149814606\n",
      "iteration 70900, loss 0.029390286654233932\n",
      "iteration 71000, loss 0.025825507938861847\n",
      "iteration 71100, loss 0.011078188195824623\n",
      "TEST accuracy at end of epoch 181: 0.8829\n",
      "TRAIN accuracy at end of epoch 181: 1.0\n",
      "                          epoch 182\n",
      "iteration 71200, loss 0.01795468106865883\n",
      "iteration 71300, loss 0.04510516673326492\n",
      "iteration 71400, loss 0.0724383294582367\n",
      "iteration 71500, loss 0.03534045070409775\n",
      "TEST accuracy at end of epoch 182: 0.8835\n",
      "TRAIN accuracy at end of epoch 182: 1.0\n",
      "                          epoch 183\n",
      "iteration 71600, loss 0.014996405690908432\n",
      "iteration 71700, loss 0.04276394471526146\n",
      "iteration 71800, loss 0.012492240406572819\n",
      "iteration 71900, loss 0.008224347606301308\n",
      "TEST accuracy at end of epoch 183: 0.8826\n",
      "TRAIN accuracy at end of epoch 183: 1.0\n",
      "                          epoch 184\n",
      "iteration 72000, loss 0.014314554631710052\n",
      "iteration 72100, loss 0.01269087940454483\n",
      "iteration 72200, loss 0.02987358160316944\n",
      "iteration 72300, loss 0.008051294833421707\n",
      "TEST accuracy at end of epoch 184: 0.8825\n",
      "TRAIN accuracy at end of epoch 184: 1.0\n",
      "                          epoch 185\n",
      "iteration 72400, loss 0.05806149169802666\n",
      "iteration 72500, loss 0.027390509843826294\n",
      "iteration 72600, loss 0.02046913653612137\n",
      "iteration 72700, loss 0.017637604847550392\n",
      "TEST accuracy at end of epoch 185: 0.8841\n",
      "TRAIN accuracy at end of epoch 185: 1.0\n",
      "                          epoch 186\n",
      "iteration 72800, loss 0.021237436681985855\n",
      "iteration 72900, loss 0.031365178525447845\n",
      "iteration 73000, loss 0.015239560976624489\n",
      "iteration 73100, loss 0.010157479904592037\n",
      "TEST accuracy at end of epoch 186: 0.8835\n",
      "TRAIN accuracy at end of epoch 186: 1.0\n",
      "                          epoch 187\n",
      "iteration 73200, loss 0.06001037731766701\n",
      "iteration 73300, loss 0.05368039757013321\n",
      "iteration 73400, loss 0.06661701947450638\n",
      "iteration 73500, loss 0.0647548958659172\n",
      "TEST accuracy at end of epoch 187: 0.8841\n",
      "TRAIN accuracy at end of epoch 187: 0.99\n",
      "                          epoch 188\n",
      "iteration 73600, loss 0.020275859162211418\n",
      "iteration 73700, loss 0.03316958621144295\n",
      "iteration 73800, loss 0.02220461331307888\n",
      "TEST accuracy at end of epoch 188: 0.8851\n",
      "TRAIN accuracy at end of epoch 188: 1.0\n",
      "                          epoch 189\n",
      "iteration 73900, loss 0.03624368831515312\n",
      "iteration 74000, loss 0.03008701466023922\n",
      "iteration 74100, loss 0.030085602775216103\n",
      "iteration 74200, loss 0.014113154262304306\n",
      "TEST accuracy at end of epoch 189: 0.8839\n",
      "TRAIN accuracy at end of epoch 189: 1.0\n",
      "                          epoch 190\n",
      "iteration 74300, loss 0.03069203346967697\n",
      "iteration 74400, loss 0.054518576711416245\n",
      "iteration 74500, loss 0.026791978627443314\n",
      "iteration 74600, loss 0.03882628679275513\n",
      "TEST accuracy at end of epoch 190: 0.8847\n",
      "TRAIN accuracy at end of epoch 190: 1.0\n",
      "                          epoch 191\n",
      "iteration 74700, loss 0.016405291855335236\n",
      "iteration 74800, loss 0.008407875895500183\n",
      "iteration 74900, loss 0.058094512671232224\n",
      "iteration 75000, loss 0.01448485441505909\n",
      "TEST accuracy at end of epoch 191: 0.8847\n",
      "TRAIN accuracy at end of epoch 191: 1.0\n",
      "                          epoch 192\n",
      "iteration 75100, loss 0.04466529190540314\n",
      "iteration 75200, loss 0.037551578134298325\n",
      "iteration 75300, loss 0.04748620092868805\n",
      "iteration 75400, loss 0.05467098951339722\n",
      "TEST accuracy at end of epoch 192: 0.885\n",
      "TRAIN accuracy at end of epoch 192: 1.0\n",
      "                          epoch 193\n",
      "iteration 75500, loss 0.028050705790519714\n",
      "iteration 75600, loss 0.030433818697929382\n",
      "iteration 75700, loss 0.04016020894050598\n",
      "iteration 75800, loss 0.013633804395794868\n",
      "TEST accuracy at end of epoch 193: 0.8844\n",
      "TRAIN accuracy at end of epoch 193: 0.99\n",
      "                          epoch 194\n",
      "iteration 75900, loss 0.025853777304291725\n",
      "iteration 76000, loss 0.016803689301013947\n",
      "iteration 76100, loss 0.029287278652191162\n",
      "iteration 76200, loss 0.05105114355683327\n",
      "TEST accuracy at end of epoch 194: 0.8847\n",
      "TRAIN accuracy at end of epoch 194: 1.0\n",
      "                          epoch 195\n",
      "iteration 76300, loss 0.02386621944606304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 76400, loss 0.07823172211647034\n",
      "iteration 76500, loss 0.03364057093858719\n",
      "iteration 76600, loss 0.054523397237062454\n",
      "TEST accuracy at end of epoch 195: 0.8839\n",
      "TRAIN accuracy at end of epoch 195: 0.99\n",
      "                          epoch 196\n",
      "iteration 76700, loss 0.06389547884464264\n",
      "iteration 76800, loss 0.052730340510606766\n",
      "iteration 76900, loss 0.05969791114330292\n",
      "iteration 77000, loss 0.004494912922382355\n",
      "TEST accuracy at end of epoch 196: 0.8842\n",
      "TRAIN accuracy at end of epoch 196: 1.0\n",
      "                          epoch 197\n",
      "iteration 77100, loss 0.019380588084459305\n",
      "iteration 77200, loss 0.07593712210655212\n",
      "iteration 77300, loss 0.03881524130702019\n",
      "iteration 77400, loss 0.007665957324206829\n",
      "TEST accuracy at end of epoch 197: 0.8842\n",
      "TRAIN accuracy at end of epoch 197: 0.98\n",
      "                          epoch 198\n",
      "iteration 77500, loss 0.008177603594958782\n",
      "iteration 77600, loss 0.07369448244571686\n",
      "iteration 77700, loss 0.03178819268941879\n",
      "iteration 77800, loss 0.01642277091741562\n",
      "TEST accuracy at end of epoch 198: 0.8838\n",
      "TRAIN accuracy at end of epoch 198: 1.0\n",
      "                          epoch 199\n",
      "iteration 77900, loss 0.04282364621758461\n",
      "iteration 78000, loss 0.04165956377983093\n",
      "iteration 78100, loss 0.01719776540994644\n",
      "iteration 78200, loss 0.02665446326136589\n",
      "TEST accuracy at end of epoch 199: 0.8841\n",
      "TRAIN accuracy at end of epoch 199: 1.0\n",
      "╰┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╯\n",
      "╭┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╮\n",
      "                         n: 5, res: False\n",
      "size of reduce_mean: (?, 1, 1, 64)\n",
      "size of squeeze: (?, 64)\n",
      "training for 78125 iterations\n",
      "                          epoch 0\n",
      "iteration 100, loss 1.936298131942749\n",
      "iteration 200, loss 1.7386484146118164\n",
      "iteration 300, loss 1.7083876132965088\n",
      "TEST accuracy at end of epoch 0: 0.1616\n",
      "TRAIN accuracy at end of epoch 0: 0.18\n",
      "                          epoch 1\n",
      "iteration 400, loss 1.8646881580352783\n",
      "iteration 500, loss 1.7257188558578491\n",
      "iteration 600, loss 1.4704959392547607\n",
      "iteration 700, loss 1.4067573547363281\n",
      "TEST accuracy at end of epoch 1: 0.1569\n",
      "TRAIN accuracy at end of epoch 1: 0.18\n",
      "                          epoch 2\n",
      "iteration 800, loss 1.2290339469909668\n",
      "iteration 900, loss 1.1352906227111816\n",
      "iteration 1000, loss 1.1585655212402344\n",
      "iteration 1100, loss 1.1727346181869507\n",
      "TEST accuracy at end of epoch 2: 0.4787\n",
      "TRAIN accuracy at end of epoch 2: 0.33\n",
      "                          epoch 3\n",
      "iteration 1200, loss 1.2294108867645264\n",
      "iteration 1300, loss 1.1667017936706543\n",
      "iteration 1400, loss 1.2575099468231201\n",
      "iteration 1500, loss 1.0630285739898682\n",
      "TEST accuracy at end of epoch 3: 0.4366\n",
      "TRAIN accuracy at end of epoch 3: 0.4\n",
      "                          epoch 4\n",
      "iteration 1600, loss 1.0742623805999756\n",
      "iteration 1700, loss 0.9290879964828491\n",
      "iteration 1800, loss 0.892879068851471\n",
      "iteration 1900, loss 1.099628210067749\n",
      "TEST accuracy at end of epoch 4: 0.4128\n",
      "TRAIN accuracy at end of epoch 4: 0.48\n",
      "                          epoch 5\n",
      "iteration 2000, loss 0.9637254476547241\n",
      "iteration 2100, loss 0.8595813512802124\n",
      "iteration 2200, loss 1.023624300956726\n",
      "iteration 2300, loss 0.8567426204681396\n",
      "TEST accuracy at end of epoch 5: 0.4194\n",
      "TRAIN accuracy at end of epoch 5: 0.39\n",
      "                          epoch 6\n",
      "iteration 2400, loss 0.7861241102218628\n",
      "iteration 2500, loss 0.805640459060669\n",
      "iteration 2600, loss 0.8056340217590332\n",
      "iteration 2700, loss 0.7163504362106323\n",
      "TEST accuracy at end of epoch 6: 0.6953\n",
      "TRAIN accuracy at end of epoch 6: 0.77\n",
      "                          epoch 7\n",
      "iteration 2800, loss 0.7373755574226379\n",
      "iteration 2900, loss 0.8542435169219971\n",
      "iteration 3000, loss 0.6563293933868408\n",
      "iteration 3100, loss 0.8947275876998901\n",
      "TEST accuracy at end of epoch 7: 0.5289\n",
      "TRAIN accuracy at end of epoch 7: 0.5\n",
      "                          epoch 8\n",
      "iteration 3200, loss 0.6665880680084229\n",
      "iteration 3300, loss 0.7449048161506653\n",
      "iteration 3400, loss 0.7149261832237244\n",
      "iteration 3500, loss 0.7575138807296753\n",
      "TEST accuracy at end of epoch 8: 0.5954\n",
      "TRAIN accuracy at end of epoch 8: 0.62\n",
      "                          epoch 9\n",
      "iteration 3600, loss 0.5870103240013123\n",
      "iteration 3700, loss 0.6134385466575623\n",
      "iteration 3800, loss 0.7603362798690796\n",
      "iteration 3900, loss 0.7576600313186646\n",
      "TEST accuracy at end of epoch 9: 0.5241\n",
      "TRAIN accuracy at end of epoch 9: 0.51\n",
      "                          epoch 10\n",
      "iteration 4000, loss 0.6525154113769531\n",
      "iteration 4100, loss 0.7367579936981201\n",
      "iteration 4200, loss 0.7245954871177673\n",
      "iteration 4300, loss 0.6060035228729248\n",
      "TEST accuracy at end of epoch 10: 0.7475\n",
      "TRAIN accuracy at end of epoch 10: 0.81\n",
      "                          epoch 11\n",
      "iteration 4400, loss 0.46273404359817505\n",
      "iteration 4500, loss 0.6214864253997803\n",
      "iteration 4600, loss 0.49044060707092285\n",
      "TEST accuracy at end of epoch 11: 0.6705\n",
      "TRAIN accuracy at end of epoch 11: 0.67\n",
      "                          epoch 12\n",
      "iteration 4700, loss 0.6613574624061584\n",
      "iteration 4800, loss 0.6118693351745605\n",
      "iteration 4900, loss 0.6717817783355713\n",
      "iteration 5000, loss 0.6046386957168579\n",
      "TEST accuracy at end of epoch 12: 0.632\n",
      "TRAIN accuracy at end of epoch 12: 0.57\n",
      "                          epoch 13\n",
      "iteration 5100, loss 0.6294351816177368\n",
      "iteration 5200, loss 0.49104368686676025\n",
      "iteration 5300, loss 0.5778454542160034\n",
      "iteration 5400, loss 0.5400938987731934\n",
      "TEST accuracy at end of epoch 13: 0.782\n",
      "TRAIN accuracy at end of epoch 13: 0.79\n",
      "                          epoch 14\n",
      "iteration 5500, loss 0.48376891016960144\n",
      "iteration 5600, loss 0.4690800905227661\n",
      "iteration 5700, loss 0.6014528870582581\n",
      "iteration 5800, loss 0.528799295425415\n",
      "TEST accuracy at end of epoch 14: 0.732\n",
      "TRAIN accuracy at end of epoch 14: 0.77\n",
      "                          epoch 15\n",
      "iteration 5900, loss 0.3879637122154236\n",
      "iteration 6000, loss 0.5140687227249146\n",
      "iteration 6100, loss 0.4632111191749573\n",
      "iteration 6200, loss 0.3254503607749939\n",
      "TEST accuracy at end of epoch 15: 0.7211\n",
      "TRAIN accuracy at end of epoch 15: 0.71\n",
      "                          epoch 16\n",
      "iteration 6300, loss 0.44878560304641724\n",
      "iteration 6400, loss 0.4936716556549072\n",
      "iteration 6500, loss 0.5774399638175964\n",
      "iteration 6600, loss 0.3783831000328064\n",
      "TEST accuracy at end of epoch 16: 0.5309\n",
      "TRAIN accuracy at end of epoch 16: 0.57\n",
      "                          epoch 17\n",
      "iteration 6700, loss 0.5057210326194763\n",
      "iteration 6800, loss 0.5160366296768188\n",
      "iteration 6900, loss 0.6906517148017883\n",
      "iteration 7000, loss 0.5150253176689148\n",
      "TEST accuracy at end of epoch 17: 0.7115\n",
      "TRAIN accuracy at end of epoch 17: 0.76\n",
      "                          epoch 18\n",
      "iteration 7100, loss 0.5183357000350952\n",
      "iteration 7200, loss 0.38675591349601746\n",
      "iteration 7300, loss 0.4216272234916687\n",
      "iteration 7400, loss 0.6242530941963196\n",
      "TEST accuracy at end of epoch 18: 0.7308\n",
      "TRAIN accuracy at end of epoch 18: 0.76\n",
      "                          epoch 19\n",
      "iteration 7500, loss 0.37746378779411316\n",
      "iteration 7600, loss 0.4692695140838623\n",
      "iteration 7700, loss 0.4240117073059082\n",
      "iteration 7800, loss 0.46091508865356445\n",
      "TEST accuracy at end of epoch 19: 0.7714\n",
      "TRAIN accuracy at end of epoch 19: 0.83\n",
      "                          epoch 20\n",
      "iteration 7900, loss 0.47265130281448364\n",
      "iteration 8000, loss 0.5065855383872986\n",
      "iteration 8100, loss 0.5656681060791016\n",
      "iteration 8200, loss 0.35004687309265137\n",
      "TEST accuracy at end of epoch 20: 0.7193\n",
      "TRAIN accuracy at end of epoch 20: 0.77\n",
      "                          epoch 21\n",
      "iteration 8300, loss 0.46751832962036133\n",
      "iteration 8400, loss 0.35558390617370605\n",
      "iteration 8500, loss 0.4347604215145111\n",
      "iteration 8600, loss 0.38933056592941284\n",
      "TEST accuracy at end of epoch 21: 0.763\n",
      "TRAIN accuracy at end of epoch 21: 0.83\n",
      "                          epoch 22\n",
      "iteration 8700, loss 0.5028522610664368\n",
      "iteration 8800, loss 0.3745742738246918\n",
      "iteration 8900, loss 0.4287507236003876\n",
      "TEST accuracy at end of epoch 22: 0.7916\n",
      "TRAIN accuracy at end of epoch 22: 0.81\n",
      "                          epoch 23\n",
      "iteration 9000, loss 0.37149786949157715\n",
      "iteration 9100, loss 0.4454028308391571\n",
      "iteration 9200, loss 0.4855407774448395\n",
      "iteration 9300, loss 0.47150638699531555\n",
      "TEST accuracy at end of epoch 23: 0.7412\n",
      "TRAIN accuracy at end of epoch 23: 0.71\n",
      "                          epoch 24\n",
      "iteration 9400, loss 0.38485395908355713\n",
      "iteration 9500, loss 0.3586907386779785\n",
      "iteration 9600, loss 0.5129368305206299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9700, loss 0.5719149112701416\n",
      "TEST accuracy at end of epoch 24: 0.8184\n",
      "TRAIN accuracy at end of epoch 24: 0.86\n",
      "                          epoch 25\n",
      "iteration 9800, loss 0.34594765305519104\n",
      "iteration 9900, loss 0.3463764786720276\n",
      "iteration 10000, loss 0.6886972188949585\n",
      "iteration 10100, loss 0.34007418155670166\n",
      "TEST accuracy at end of epoch 25: 0.7939\n",
      "TRAIN accuracy at end of epoch 25: 0.8\n",
      "                          epoch 26\n",
      "iteration 10200, loss 0.3050770163536072\n",
      "iteration 10300, loss 0.25205934047698975\n",
      "iteration 10400, loss 0.3227061927318573\n",
      "iteration 10500, loss 0.40061429142951965\n",
      "TEST accuracy at end of epoch 26: 0.7996\n",
      "TRAIN accuracy at end of epoch 26: 0.85\n",
      "                          epoch 27\n",
      "iteration 10600, loss 0.3965458869934082\n",
      "iteration 10700, loss 0.5277149081230164\n",
      "iteration 10800, loss 0.4560052156448364\n",
      "iteration 10900, loss 0.4610234797000885\n",
      "TEST accuracy at end of epoch 27: 0.692\n",
      "TRAIN accuracy at end of epoch 27: 0.63\n",
      "                          epoch 28\n",
      "iteration 11000, loss 0.486674040555954\n",
      "iteration 11100, loss 0.35497748851776123\n",
      "iteration 11200, loss 0.3565000295639038\n",
      "iteration 11300, loss 0.30920350551605225\n",
      "TEST accuracy at end of epoch 28: 0.8067\n",
      "TRAIN accuracy at end of epoch 28: 0.79\n",
      "                          epoch 29\n",
      "iteration 11400, loss 0.4910639822483063\n",
      "iteration 11500, loss 0.4050944447517395\n",
      "iteration 11600, loss 0.4869765341281891\n",
      "iteration 11700, loss 0.3902937173843384\n",
      "TEST accuracy at end of epoch 29: 0.7951\n",
      "TRAIN accuracy at end of epoch 29: 0.81\n",
      "                          epoch 30\n",
      "iteration 11800, loss 0.3766476809978485\n",
      "iteration 11900, loss 0.36912766098976135\n",
      "iteration 12000, loss 0.3371514081954956\n",
      "iteration 12100, loss 0.32105278968811035\n",
      "TEST accuracy at end of epoch 30: 0.7685\n",
      "TRAIN accuracy at end of epoch 30: 0.86\n",
      "                          epoch 31\n",
      "iteration 12200, loss 0.3463909924030304\n",
      "iteration 12300, loss 0.38969576358795166\n",
      "iteration 12400, loss 0.3220409154891968\n",
      "iteration 12500, loss 0.28983044624328613\n",
      "TEST accuracy at end of epoch 31: 0.8262\n",
      "TRAIN accuracy at end of epoch 31: 0.9\n",
      "                          epoch 32\n",
      "iteration 12600, loss 0.43520593643188477\n",
      "iteration 12700, loss 0.4528101682662964\n",
      "iteration 12800, loss 0.3179572820663452\n",
      "iteration 12900, loss 0.38388940691947937\n",
      "TEST accuracy at end of epoch 32: 0.8053\n",
      "TRAIN accuracy at end of epoch 32: 0.87\n",
      "                          epoch 33\n",
      "iteration 13000, loss 0.27705907821655273\n",
      "iteration 13100, loss 0.26550009846687317\n",
      "iteration 13200, loss 0.3870321214199066\n",
      "TEST accuracy at end of epoch 33: 0.7749\n",
      "TRAIN accuracy at end of epoch 33: 0.81\n",
      "                          epoch 34\n",
      "iteration 13300, loss 0.3128335177898407\n",
      "iteration 13400, loss 0.3621768355369568\n",
      "iteration 13500, loss 0.2532341480255127\n",
      "iteration 13600, loss 0.29186028242111206\n",
      "TEST accuracy at end of epoch 34: 0.7699\n",
      "TRAIN accuracy at end of epoch 34: 0.87\n",
      "                          epoch 35\n",
      "iteration 13700, loss 0.2614414691925049\n",
      "iteration 13800, loss 0.6163956522941589\n",
      "iteration 13900, loss 0.22219909727573395\n",
      "iteration 14000, loss 0.3558127284049988\n",
      "TEST accuracy at end of epoch 35: 0.8055\n",
      "TRAIN accuracy at end of epoch 35: 0.83\n",
      "                          epoch 36\n",
      "iteration 14100, loss 0.3466964364051819\n",
      "iteration 14200, loss 0.3236652910709381\n",
      "iteration 14300, loss 0.41227778792381287\n",
      "iteration 14400, loss 0.37153905630111694\n",
      "TEST accuracy at end of epoch 36: 0.8155\n",
      "TRAIN accuracy at end of epoch 36: 0.85\n",
      "                          epoch 37\n",
      "iteration 14500, loss 0.322226345539093\n",
      "iteration 14600, loss 0.3477158844470978\n",
      "iteration 14700, loss 0.3039671778678894\n",
      "iteration 14800, loss 0.2447408139705658\n",
      "TEST accuracy at end of epoch 37: 0.8371\n",
      "TRAIN accuracy at end of epoch 37: 0.86\n",
      "                          epoch 38\n",
      "iteration 14900, loss 0.40727049112319946\n",
      "iteration 15000, loss 0.16684669256210327\n",
      "iteration 15100, loss 0.35372522473335266\n",
      "iteration 15200, loss 0.2534661293029785\n",
      "TEST accuracy at end of epoch 38: 0.8113\n",
      "TRAIN accuracy at end of epoch 38: 0.83\n",
      "                          epoch 39\n",
      "iteration 15300, loss 0.20639359951019287\n",
      "iteration 15400, loss 0.4012845754623413\n",
      "iteration 15500, loss 0.2630351185798645\n",
      "iteration 15600, loss 0.3902997672557831\n",
      "TEST accuracy at end of epoch 39: 0.8493\n",
      "TRAIN accuracy at end of epoch 39: 0.97\n",
      "                          epoch 40\n",
      "iteration 15700, loss 0.3490126132965088\n",
      "iteration 15800, loss 0.4024512767791748\n",
      "iteration 15900, loss 0.28561103343963623\n",
      "iteration 16000, loss 0.3370487093925476\n",
      "TEST accuracy at end of epoch 40: 0.8173\n",
      "TRAIN accuracy at end of epoch 40: 0.87\n",
      "                          epoch 41\n",
      "iteration 16100, loss 0.23720429837703705\n",
      "iteration 16200, loss 0.2391149401664734\n",
      "iteration 16300, loss 0.31032848358154297\n",
      "iteration 16400, loss 0.36250561475753784\n",
      "TEST accuracy at end of epoch 41: 0.8237\n",
      "TRAIN accuracy at end of epoch 41: 0.9\n",
      "                          epoch 42\n",
      "iteration 16500, loss 0.19629254937171936\n",
      "iteration 16600, loss 0.3184230327606201\n",
      "iteration 16700, loss 0.2539583444595337\n",
      "iteration 16800, loss 0.26940304040908813\n",
      "TEST accuracy at end of epoch 42: 0.7659\n",
      "TRAIN accuracy at end of epoch 42: 0.86\n",
      "                          epoch 43\n",
      "iteration 16900, loss 0.26019853353500366\n",
      "iteration 17000, loss 0.29482501745224\n",
      "iteration 17100, loss 0.4449242353439331\n",
      "iteration 17200, loss 0.4120297133922577\n",
      "TEST accuracy at end of epoch 43: 0.7941\n",
      "TRAIN accuracy at end of epoch 43: 0.83\n",
      "                          epoch 44\n",
      "iteration 17300, loss 0.3556959629058838\n",
      "iteration 17400, loss 0.3636122941970825\n",
      "iteration 17500, loss 0.3020070791244507\n",
      "TEST accuracy at end of epoch 44: 0.7955\n",
      "TRAIN accuracy at end of epoch 44: 0.82\n",
      "                          epoch 45\n",
      "iteration 17600, loss 0.21316197514533997\n",
      "iteration 17700, loss 0.32978957891464233\n",
      "iteration 17800, loss 0.18652737140655518\n",
      "iteration 17900, loss 0.2596316337585449\n",
      "TEST accuracy at end of epoch 45: 0.8568\n",
      "TRAIN accuracy at end of epoch 45: 0.94\n",
      "                          epoch 46\n",
      "iteration 18000, loss 0.18181206285953522\n",
      "iteration 18100, loss 0.2392338514328003\n",
      "iteration 18200, loss 0.28877729177474976\n",
      "iteration 18300, loss 0.32919594645500183\n",
      "TEST accuracy at end of epoch 46: 0.8165\n",
      "TRAIN accuracy at end of epoch 46: 0.9\n",
      "                          epoch 47\n",
      "iteration 18400, loss 0.16974422335624695\n",
      "iteration 18500, loss 0.3013710379600525\n",
      "iteration 18600, loss 0.4299117624759674\n",
      "iteration 18700, loss 0.32412832975387573\n",
      "TEST accuracy at end of epoch 47: 0.8516\n",
      "TRAIN accuracy at end of epoch 47: 0.89\n",
      "                          epoch 48\n",
      "iteration 18800, loss 0.33389681577682495\n",
      "iteration 18900, loss 0.3205210864543915\n",
      "iteration 19000, loss 0.22163034975528717\n",
      "iteration 19100, loss 0.15367938578128815\n",
      "TEST accuracy at end of epoch 48: 0.776\n",
      "TRAIN accuracy at end of epoch 48: 0.77\n",
      "                          epoch 49\n",
      "iteration 19200, loss 0.21685457229614258\n",
      "iteration 19300, loss 0.22139766812324524\n",
      "iteration 19400, loss 0.22855567932128906\n",
      "iteration 19500, loss 0.23396480083465576\n",
      "TEST accuracy at end of epoch 49: 0.8514\n",
      "TRAIN accuracy at end of epoch 49: 0.88\n",
      "                          epoch 50\n",
      "iteration 19600, loss 0.19337092339992523\n",
      "iteration 19700, loss 0.2031007707118988\n",
      "iteration 19800, loss 0.3391340374946594\n",
      "iteration 19900, loss 0.3664836287498474\n",
      "TEST accuracy at end of epoch 50: 0.8471\n",
      "TRAIN accuracy at end of epoch 50: 0.96\n",
      "                          epoch 51\n",
      "iteration 20000, loss 0.2575089931488037\n",
      "iteration 20100, loss 0.3856599032878876\n",
      "iteration 20200, loss 0.23610056936740875\n",
      "iteration 20300, loss 0.25595414638519287\n",
      "TEST accuracy at end of epoch 51: 0.8018\n",
      "TRAIN accuracy at end of epoch 51: 0.77\n",
      "                          epoch 52\n",
      "iteration 20400, loss 0.14008857309818268\n",
      "iteration 20500, loss 0.24912677705287933\n",
      "iteration 20600, loss 0.24629974365234375\n",
      "iteration 20700, loss 0.19384773075580597\n",
      "TEST accuracy at end of epoch 52: 0.8582\n",
      "TRAIN accuracy at end of epoch 52: 0.92\n",
      "                          epoch 53\n",
      "iteration 20800, loss 0.1843816637992859\n",
      "iteration 20900, loss 0.3480018377304077\n",
      "iteration 21000, loss 0.2615157961845398\n",
      "iteration 21100, loss 0.2747655510902405\n",
      "TEST accuracy at end of epoch 53: 0.8527\n",
      "TRAIN accuracy at end of epoch 53: 0.89\n",
      "                          epoch 54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 21200, loss 0.25721418857574463\n",
      "iteration 21300, loss 0.2587473392486572\n",
      "iteration 21400, loss 0.19256292283535004\n",
      "iteration 21500, loss 0.2139553427696228\n",
      "TEST accuracy at end of epoch 54: 0.8414\n",
      "TRAIN accuracy at end of epoch 54: 0.88\n",
      "                          epoch 55\n",
      "iteration 21600, loss 0.13527315855026245\n",
      "iteration 21700, loss 0.19575145840644836\n",
      "iteration 21800, loss 0.24851006269454956\n",
      "TEST accuracy at end of epoch 55: 0.8421\n",
      "TRAIN accuracy at end of epoch 55: 0.91\n",
      "                          epoch 56\n",
      "iteration 21900, loss 0.14831481873989105\n",
      "iteration 22000, loss 0.18210369348526\n",
      "iteration 22100, loss 0.23120306432247162\n",
      "iteration 22200, loss 0.21452569961547852\n",
      "TEST accuracy at end of epoch 56: 0.7981\n",
      "TRAIN accuracy at end of epoch 56: 0.82\n",
      "                          epoch 57\n",
      "iteration 22300, loss 0.20997600257396698\n",
      "iteration 22400, loss 0.2242458462715149\n",
      "iteration 22500, loss 0.20135712623596191\n",
      "iteration 22600, loss 0.265896201133728\n",
      "TEST accuracy at end of epoch 57: 0.8577\n",
      "TRAIN accuracy at end of epoch 57: 0.97\n",
      "                          epoch 58\n",
      "iteration 22700, loss 0.22948652505874634\n",
      "iteration 22800, loss 0.2043050229549408\n",
      "iteration 22900, loss 0.1581679880619049\n",
      "iteration 23000, loss 0.20958960056304932\n",
      "TEST accuracy at end of epoch 58: 0.8394\n",
      "TRAIN accuracy at end of epoch 58: 0.94\n",
      "                          epoch 59\n",
      "iteration 23100, loss 0.18141478300094604\n",
      "iteration 23200, loss 0.24455291032791138\n",
      "iteration 23300, loss 0.26225733757019043\n",
      "iteration 23400, loss 0.2914322018623352\n",
      "TEST accuracy at end of epoch 59: 0.832\n",
      "TRAIN accuracy at end of epoch 59: 0.91\n",
      "                          epoch 60\n",
      "iteration 23500, loss 0.2487107813358307\n",
      "iteration 23600, loss 0.191331684589386\n",
      "iteration 23700, loss 0.24625298380851746\n",
      "iteration 23800, loss 0.18517091870307922\n",
      "TEST accuracy at end of epoch 60: 0.8462\n",
      "TRAIN accuracy at end of epoch 60: 0.87\n",
      "                          epoch 61\n",
      "iteration 23900, loss 0.2839406430721283\n",
      "iteration 24000, loss 0.08193998783826828\n",
      "iteration 24100, loss 0.2155444324016571\n",
      "iteration 24200, loss 0.21853455901145935\n",
      "TEST accuracy at end of epoch 61: 0.823\n",
      "TRAIN accuracy at end of epoch 61: 0.83\n",
      "                          epoch 62\n",
      "iteration 24300, loss 0.28827768564224243\n",
      "iteration 24400, loss 0.30881357192993164\n",
      "iteration 24500, loss 0.14294186234474182\n",
      "iteration 24600, loss 0.15622031688690186\n",
      "TEST accuracy at end of epoch 62: 0.8345\n",
      "TRAIN accuracy at end of epoch 62: 0.91\n",
      "                          epoch 63\n",
      "iteration 24700, loss 0.20584672689437866\n",
      "iteration 24800, loss 0.1356133073568344\n",
      "iteration 24900, loss 0.12370067834854126\n",
      "iteration 25000, loss 0.08929187804460526\n",
      "TEST accuracy at end of epoch 63: 0.846\n",
      "TRAIN accuracy at end of epoch 63: 0.86\n",
      "                          epoch 64\n",
      "iteration 25100, loss 0.1359785795211792\n",
      "iteration 25200, loss 0.1437838226556778\n",
      "iteration 25300, loss 0.16955651342868805\n",
      "iteration 25400, loss 0.10243638604879379\n",
      "TEST accuracy at end of epoch 64: 0.8621\n",
      "TRAIN accuracy at end of epoch 64: 0.89\n",
      "                          epoch 65\n",
      "iteration 25500, loss 0.1985938996076584\n",
      "iteration 25600, loss 0.18660351634025574\n",
      "iteration 25700, loss 0.1652919054031372\n",
      "iteration 25800, loss 0.16319096088409424\n",
      "TEST accuracy at end of epoch 65: 0.8595\n",
      "TRAIN accuracy at end of epoch 65: 0.92\n",
      "                          epoch 66\n",
      "iteration 25900, loss 0.17280632257461548\n",
      "iteration 26000, loss 0.1684589385986328\n",
      "iteration 26100, loss 0.19163823127746582\n",
      "TEST accuracy at end of epoch 66: 0.857\n",
      "TRAIN accuracy at end of epoch 66: 0.9\n",
      "                          epoch 67\n",
      "iteration 26200, loss 0.11467690020799637\n",
      "iteration 26300, loss 0.1256313920021057\n",
      "iteration 26400, loss 0.15003558993339539\n",
      "iteration 26500, loss 0.11592278629541397\n",
      "TEST accuracy at end of epoch 67: 0.8222\n",
      "TRAIN accuracy at end of epoch 67: 0.89\n",
      "                          epoch 68\n",
      "iteration 26600, loss 0.1182238981127739\n",
      "iteration 26700, loss 0.21615391969680786\n",
      "iteration 26800, loss 0.07387793064117432\n",
      "iteration 26900, loss 0.17050103843212128\n",
      "TEST accuracy at end of epoch 68: 0.8673\n",
      "TRAIN accuracy at end of epoch 68: 0.96\n",
      "                          epoch 69\n",
      "iteration 27000, loss 0.23290908336639404\n",
      "iteration 27100, loss 0.12903252243995667\n",
      "iteration 27200, loss 0.16219747066497803\n",
      "iteration 27300, loss 0.14901328086853027\n",
      "TEST accuracy at end of epoch 69: 0.805\n",
      "TRAIN accuracy at end of epoch 69: 0.92\n",
      "                          epoch 70\n",
      "iteration 27400, loss 0.2035180628299713\n",
      "iteration 27500, loss 0.19915488362312317\n",
      "iteration 27600, loss 0.13387393951416016\n",
      "iteration 27700, loss 0.22627700865268707\n",
      "TEST accuracy at end of epoch 70: 0.8562\n",
      "TRAIN accuracy at end of epoch 70: 0.97\n",
      "                          epoch 71\n",
      "iteration 27800, loss 0.18842262029647827\n",
      "iteration 27900, loss 0.250501811504364\n",
      "iteration 28000, loss 0.143697589635849\n",
      "iteration 28100, loss 0.26646068692207336\n",
      "TEST accuracy at end of epoch 71: 0.843\n",
      "TRAIN accuracy at end of epoch 71: 0.92\n",
      "                          epoch 72\n",
      "iteration 28200, loss 0.08546505868434906\n",
      "iteration 28300, loss 0.07684026658535004\n",
      "iteration 28400, loss 0.16316162049770355\n",
      "iteration 28500, loss 0.23190513253211975\n",
      "TEST accuracy at end of epoch 72: 0.8519\n",
      "TRAIN accuracy at end of epoch 72: 0.92\n",
      "                          epoch 73\n",
      "iteration 28600, loss 0.17785990238189697\n",
      "iteration 28700, loss 0.11873336136341095\n",
      "iteration 28800, loss 0.2184711992740631\n",
      "iteration 28900, loss 0.1160551905632019\n",
      "TEST accuracy at end of epoch 73: 0.843\n",
      "TRAIN accuracy at end of epoch 73: 0.96\n",
      "                          epoch 74\n",
      "iteration 29000, loss 0.19824422895908356\n",
      "iteration 29100, loss 0.11918875575065613\n",
      "iteration 29200, loss 0.12710106372833252\n",
      "iteration 29300, loss 0.18753181397914886\n",
      "TEST accuracy at end of epoch 74: 0.839\n",
      "TRAIN accuracy at end of epoch 74: 0.86\n",
      "                          epoch 75\n",
      "iteration 29400, loss 0.19168458878993988\n",
      "iteration 29500, loss 0.12242208421230316\n",
      "iteration 29600, loss 0.059202395379543304\n",
      "iteration 29700, loss 0.12750422954559326\n",
      "TEST accuracy at end of epoch 75: 0.8466\n",
      "TRAIN accuracy at end of epoch 75: 0.97\n",
      "                          epoch 76\n",
      "iteration 29800, loss 0.19872911274433136\n",
      "iteration 29900, loss 0.21620288491249084\n",
      "iteration 30000, loss 0.1599605232477188\n",
      "iteration 30100, loss 0.1603674739599228\n",
      "TEST accuracy at end of epoch 76: 0.8474\n",
      "TRAIN accuracy at end of epoch 76: 0.99\n",
      "                          epoch 77\n",
      "iteration 30200, loss 0.09909288585186005\n",
      "iteration 30300, loss 0.28261077404022217\n",
      "iteration 30400, loss 0.12601390480995178\n",
      "TEST accuracy at end of epoch 77: 0.8569\n",
      "TRAIN accuracy at end of epoch 77: 0.93\n",
      "                          epoch 78\n",
      "iteration 30500, loss 0.11788302659988403\n",
      "iteration 30600, loss 0.13484276831150055\n",
      "iteration 30700, loss 0.12706997990608215\n",
      "iteration 30800, loss 0.10398939996957779\n",
      "TEST accuracy at end of epoch 78: 0.8584\n",
      "TRAIN accuracy at end of epoch 78: 0.96\n",
      "                          epoch 79\n",
      "iteration 30900, loss 0.13648615777492523\n",
      "iteration 31000, loss 0.12384847551584244\n",
      "iteration 31100, loss 0.1528720110654831\n",
      "iteration 31200, loss 0.1365349143743515\n",
      "TEST accuracy at end of epoch 79: 0.8318\n",
      "TRAIN accuracy at end of epoch 79: 0.92\n",
      "                          epoch 80\n",
      "iteration 31300, loss 0.09384553879499435\n",
      "iteration 31400, loss 0.12311379611492157\n",
      "iteration 31500, loss 0.2123621106147766\n",
      "iteration 31600, loss 0.2294064611196518\n",
      "TEST accuracy at end of epoch 80: 0.8665\n",
      "TRAIN accuracy at end of epoch 80: 0.96\n",
      "                          epoch 81\n",
      "iteration 31700, loss 0.18592385947704315\n",
      "iteration 31800, loss 0.09473198652267456\n",
      "iteration 31900, loss 0.2558189034461975\n",
      "iteration 32000, loss 0.18802380561828613\n",
      "TEST accuracy at end of epoch 81: 0.8595\n",
      "TRAIN accuracy at end of epoch 81: 0.94\n",
      "                          epoch 82\n",
      "iteration 32100, loss 0.08672633022069931\n",
      "iteration 32200, loss 0.13984255492687225\n",
      "iteration 32300, loss 0.18264687061309814\n",
      "iteration 32400, loss 0.13679151237010956\n",
      "TEST accuracy at end of epoch 82: 0.8619\n",
      "TRAIN accuracy at end of epoch 82: 0.97\n",
      "                          epoch 83\n",
      "iteration 32500, loss 0.16138118505477905\n",
      "iteration 32600, loss 0.12914621829986572\n",
      "iteration 32700, loss 0.14752988517284393\n",
      "iteration 32800, loss 0.07886268198490143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST accuracy at end of epoch 83: 0.8676\n",
      "TRAIN accuracy at end of epoch 83: 0.92\n",
      "                          epoch 84\n",
      "iteration 32900, loss 0.1699339747428894\n",
      "iteration 33000, loss 0.08154743909835815\n",
      "iteration 33100, loss 0.07091188430786133\n",
      "iteration 33200, loss 0.08302029967308044\n",
      "TEST accuracy at end of epoch 84: 0.8638\n",
      "TRAIN accuracy at end of epoch 84: 0.95\n",
      "                          epoch 85\n",
      "iteration 33300, loss 0.13575145602226257\n",
      "iteration 33400, loss 0.08276335895061493\n",
      "iteration 33500, loss 0.1345248818397522\n",
      "iteration 33600, loss 0.12162848562002182\n",
      "TEST accuracy at end of epoch 85: 0.8562\n",
      "TRAIN accuracy at end of epoch 85: 0.95\n",
      "                          epoch 86\n",
      "iteration 33700, loss 0.11923318356275558\n",
      "iteration 33800, loss 0.1061847060918808\n",
      "iteration 33900, loss 0.1509818434715271\n",
      "iteration 34000, loss 0.12841780483722687\n",
      "TEST accuracy at end of epoch 86: 0.865\n",
      "TRAIN accuracy at end of epoch 86: 0.93\n",
      "                          epoch 87\n",
      "iteration 34100, loss 0.08135795593261719\n",
      "iteration 34200, loss 0.11085596680641174\n",
      "iteration 34300, loss 0.08229347318410873\n",
      "iteration 34400, loss 0.08213222771883011\n",
      "TEST accuracy at end of epoch 87: 0.868\n",
      "TRAIN accuracy at end of epoch 87: 0.92\n",
      "                          epoch 88\n",
      "iteration 34500, loss 0.14643698930740356\n",
      "iteration 34600, loss 0.10724808275699615\n",
      "iteration 34700, loss 0.1137075126171112\n",
      "TEST accuracy at end of epoch 88: 0.8481\n",
      "TRAIN accuracy at end of epoch 88: 0.96\n",
      "                          epoch 89\n",
      "iteration 34800, loss 0.10727129876613617\n",
      "iteration 34900, loss 0.11308661848306656\n",
      "iteration 35000, loss 0.13117250800132751\n",
      "iteration 35100, loss 0.07223643362522125\n",
      "TEST accuracy at end of epoch 89: 0.8641\n",
      "TRAIN accuracy at end of epoch 89: 0.99\n",
      "                          epoch 90\n",
      "iteration 35200, loss 0.1329016089439392\n",
      "iteration 35300, loss 0.14709091186523438\n",
      "iteration 35400, loss 0.13933244347572327\n",
      "iteration 35500, loss 0.10097488015890121\n",
      "TEST accuracy at end of epoch 90: 0.8353\n",
      "TRAIN accuracy at end of epoch 90: 0.92\n",
      "                          epoch 91\n",
      "iteration 35600, loss 0.1164436787366867\n",
      "iteration 35700, loss 0.07792811095714569\n",
      "iteration 35800, loss 0.04544525966048241\n",
      "iteration 35900, loss 0.09739967435598373\n",
      "TEST accuracy at end of epoch 91: 0.8659\n",
      "TRAIN accuracy at end of epoch 91: 0.99\n",
      "                          epoch 92\n",
      "iteration 36000, loss 0.08743542432785034\n",
      "iteration 36100, loss 0.1365724503993988\n",
      "iteration 36200, loss 0.12350702285766602\n",
      "iteration 36300, loss 0.20979441702365875\n",
      "TEST accuracy at end of epoch 92: 0.861\n",
      "TRAIN accuracy at end of epoch 92: 0.97\n",
      "                          epoch 93\n",
      "iteration 36400, loss 0.06346641480922699\n",
      "iteration 36500, loss 0.1942368745803833\n",
      "iteration 36600, loss 0.11629068106412888\n",
      "iteration 36700, loss 0.15434858202934265\n",
      "TEST accuracy at end of epoch 93: 0.8723\n",
      "TRAIN accuracy at end of epoch 93: 0.99\n",
      "                          epoch 94\n",
      "iteration 36800, loss 0.1029742956161499\n",
      "iteration 36900, loss 0.11025342345237732\n",
      "iteration 37000, loss 0.08799298107624054\n",
      "iteration 37100, loss 0.10092479735612869\n",
      "TEST accuracy at end of epoch 94: 0.8734\n",
      "TRAIN accuracy at end of epoch 94: 0.99\n",
      "                          epoch 95\n",
      "iteration 37200, loss 0.09433309733867645\n",
      "iteration 37300, loss 0.08788146078586578\n",
      "iteration 37400, loss 0.12105491757392883\n",
      "iteration 37500, loss 0.12064497172832489\n",
      "TEST accuracy at end of epoch 95: 0.868\n",
      "TRAIN accuracy at end of epoch 95: 0.96\n",
      "                          epoch 96\n",
      "iteration 37600, loss 0.0364079475402832\n",
      "iteration 37700, loss 0.10653609037399292\n",
      "iteration 37800, loss 0.17117469012737274\n",
      "iteration 37900, loss 0.06670144945383072\n",
      "TEST accuracy at end of epoch 96: 0.8341\n",
      "TRAIN accuracy at end of epoch 96: 0.89\n",
      "                          epoch 97\n",
      "iteration 38000, loss 0.18312910199165344\n",
      "iteration 38100, loss 0.12534084916114807\n",
      "iteration 38200, loss 0.1374903917312622\n",
      "iteration 38300, loss 0.09727045893669128\n",
      "TEST accuracy at end of epoch 97: 0.8622\n",
      "TRAIN accuracy at end of epoch 97: 0.99\n",
      "                          epoch 98\n",
      "iteration 38400, loss 0.09405511617660522\n",
      "iteration 38500, loss 0.04852214455604553\n",
      "iteration 38600, loss 0.08381884545087814\n",
      "iteration 38700, loss 0.09329719841480255\n",
      "TEST accuracy at end of epoch 98: 0.8712\n",
      "TRAIN accuracy at end of epoch 98: 0.98\n",
      "                          epoch 99\n",
      "iteration 38800, loss 0.15918275713920593\n",
      "iteration 38900, loss 0.09170106053352356\n",
      "iteration 39000, loss 0.1962469220161438\n",
      "iteration 39100, loss 0.15048417448997498\n",
      "TEST accuracy at end of epoch 99: 0.8622\n",
      "TRAIN accuracy at end of epoch 99: 0.98\n",
      "                          epoch 100\n",
      "iteration 39200, loss 0.10313260555267334\n",
      "iteration 39300, loss 0.11480480432510376\n",
      "iteration 39400, loss 0.11626336723566055\n",
      "TEST accuracy at end of epoch 100: 0.8684\n",
      "TRAIN accuracy at end of epoch 100: 0.97\n",
      "                          epoch 101\n",
      "iteration 39500, loss 0.17491737008094788\n",
      "iteration 39600, loss 0.07084626704454422\n",
      "iteration 39700, loss 0.08285124599933624\n",
      "iteration 39800, loss 0.05704743415117264\n",
      "TEST accuracy at end of epoch 101: 0.8808\n",
      "TRAIN accuracy at end of epoch 101: 0.98\n",
      "                          epoch 102\n",
      "iteration 39900, loss 0.13899114727973938\n",
      "iteration 40000, loss 0.06464464217424393\n",
      "iteration 40100, loss 0.06647004932165146\n",
      "iteration 40200, loss 0.094235360622406\n",
      "TEST accuracy at end of epoch 102: 0.8705\n",
      "TRAIN accuracy at end of epoch 102: 0.98\n",
      "                          epoch 103\n",
      "iteration 40300, loss 0.10196396708488464\n",
      "iteration 40400, loss 0.1327495127916336\n",
      "iteration 40500, loss 0.07161442935466766\n",
      "iteration 40600, loss 0.11070266366004944\n",
      "TEST accuracy at end of epoch 103: 0.8615\n",
      "TRAIN accuracy at end of epoch 103: 0.95\n",
      "                          epoch 104\n",
      "iteration 40700, loss 0.06249064579606056\n",
      "iteration 40800, loss 0.06517837941646576\n",
      "iteration 40900, loss 0.09513577073812485\n",
      "iteration 41000, loss 0.055181510746479034\n",
      "TEST accuracy at end of epoch 104: 0.8737\n",
      "TRAIN accuracy at end of epoch 104: 0.96\n",
      "                          epoch 105\n",
      "iteration 41100, loss 0.06134083867073059\n",
      "iteration 41200, loss 0.08855678141117096\n",
      "iteration 41300, loss 0.05090044438838959\n",
      "iteration 41400, loss 0.19731146097183228\n",
      "TEST accuracy at end of epoch 105: 0.8516\n",
      "TRAIN accuracy at end of epoch 105: 0.96\n",
      "                          epoch 106\n",
      "iteration 41500, loss 0.16013574600219727\n",
      "iteration 41600, loss 0.13621696829795837\n",
      "iteration 41700, loss 0.09562265872955322\n",
      "iteration 41800, loss 0.0647527426481247\n",
      "TEST accuracy at end of epoch 106: 0.8694\n",
      "TRAIN accuracy at end of epoch 106: 0.99\n",
      "                          epoch 107\n",
      "iteration 41900, loss 0.054342348128557205\n",
      "iteration 42000, loss 0.10462196916341782\n",
      "iteration 42100, loss 0.12470383942127228\n",
      "iteration 42200, loss 0.11969386041164398\n",
      "TEST accuracy at end of epoch 107: 0.8833\n",
      "TRAIN accuracy at end of epoch 107: 0.99\n",
      "                          epoch 108\n",
      "iteration 42300, loss 0.05009178817272186\n",
      "iteration 42400, loss 0.06334163248538971\n",
      "iteration 42500, loss 0.11901368200778961\n",
      "iteration 42600, loss 0.09393762052059174\n",
      "TEST accuracy at end of epoch 108: 0.8779\n",
      "TRAIN accuracy at end of epoch 108: 0.99\n",
      "                          epoch 109\n",
      "iteration 42700, loss 0.10903845727443695\n",
      "iteration 42800, loss 0.031191740185022354\n",
      "iteration 42900, loss 0.084769606590271\n",
      "iteration 43000, loss 0.06563897430896759\n",
      "TEST accuracy at end of epoch 109: 0.8687\n",
      "TRAIN accuracy at end of epoch 109: 0.95\n",
      "                          epoch 110\n",
      "iteration 43100, loss 0.04654214158654213\n",
      "iteration 43200, loss 0.04584529250860214\n",
      "iteration 43300, loss 0.16341280937194824\n",
      "iteration 43400, loss 0.06274525076150894\n",
      "TEST accuracy at end of epoch 110: 0.8748\n",
      "TRAIN accuracy at end of epoch 110: 0.97\n",
      "                          epoch 111\n",
      "iteration 43500, loss 0.1007225513458252\n",
      "iteration 43600, loss 0.0768471211194992\n",
      "iteration 43700, loss 0.039234161376953125\n",
      "TEST accuracy at end of epoch 111: 0.8692\n",
      "TRAIN accuracy at end of epoch 111: 0.98\n",
      "                          epoch 112\n",
      "iteration 43800, loss 0.08075615763664246\n",
      "iteration 43900, loss 0.14852380752563477\n",
      "iteration 44000, loss 0.04823817312717438\n",
      "iteration 44100, loss 0.06595003604888916\n",
      "TEST accuracy at end of epoch 112: 0.8747\n",
      "TRAIN accuracy at end of epoch 112: 1.0\n",
      "                          epoch 113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 44200, loss 0.0718877762556076\n",
      "iteration 44300, loss 0.07364633679389954\n",
      "iteration 44400, loss 0.012414354830980301\n",
      "iteration 44500, loss 0.13637666404247284\n",
      "TEST accuracy at end of epoch 113: 0.87\n",
      "TRAIN accuracy at end of epoch 113: 0.96\n",
      "                          epoch 114\n",
      "iteration 44600, loss 0.030468495562672615\n",
      "iteration 44700, loss 0.04522779583930969\n",
      "iteration 44800, loss 0.06712327152490616\n",
      "iteration 44900, loss 0.07542333751916885\n",
      "TEST accuracy at end of epoch 114: 0.8759\n",
      "TRAIN accuracy at end of epoch 114: 0.98\n",
      "                          epoch 115\n",
      "iteration 45000, loss 0.11249065399169922\n",
      "iteration 45100, loss 0.027797585353255272\n",
      "iteration 45200, loss 0.10405623912811279\n",
      "iteration 45300, loss 0.0727904811501503\n",
      "TEST accuracy at end of epoch 115: 0.8704\n",
      "TRAIN accuracy at end of epoch 115: 0.97\n",
      "                          epoch 116\n",
      "iteration 45400, loss 0.05420240759849548\n",
      "iteration 45500, loss 0.06160265952348709\n",
      "iteration 45600, loss 0.05056552588939667\n",
      "iteration 45700, loss 0.06940806657075882\n",
      "TEST accuracy at end of epoch 116: 0.8616\n",
      "TRAIN accuracy at end of epoch 116: 0.96\n",
      "                          epoch 117\n",
      "iteration 45800, loss 0.11747035384178162\n",
      "iteration 45900, loss 0.09100822359323502\n",
      "iteration 46000, loss 0.09157467633485794\n",
      "iteration 46100, loss 0.10113954544067383\n",
      "TEST accuracy at end of epoch 117: 0.8764\n",
      "TRAIN accuracy at end of epoch 117: 0.98\n",
      "                          epoch 118\n",
      "iteration 46200, loss 0.05870223045349121\n",
      "iteration 46300, loss 0.12370707839727402\n",
      "iteration 46400, loss 0.11017733812332153\n",
      "iteration 46500, loss 0.10144016146659851\n",
      "TEST accuracy at end of epoch 118: 0.88\n",
      "TRAIN accuracy at end of epoch 118: 0.99\n",
      "                          epoch 119\n",
      "iteration 46600, loss 0.05593706667423248\n",
      "iteration 46700, loss 0.11914313584566116\n",
      "iteration 46800, loss 0.05659414827823639\n",
      "iteration 46900, loss 0.03823705017566681\n",
      "TEST accuracy at end of epoch 119: 0.8758\n",
      "TRAIN accuracy at end of epoch 119: 0.96\n",
      "                          epoch 120\n",
      "iteration 47000, loss 0.06404519826173782\n",
      "iteration 47100, loss 0.0556962750852108\n",
      "iteration 47200, loss 0.02427956834435463\n",
      "iteration 47300, loss 0.029650511220097542\n",
      "TEST accuracy at end of epoch 120: 0.8738\n",
      "TRAIN accuracy at end of epoch 120: 0.97\n",
      "                          epoch 121\n",
      "iteration 47400, loss 0.059304382652044296\n",
      "iteration 47500, loss 0.06250264495611191\n",
      "iteration 47600, loss 0.05485682189464569\n",
      "iteration 47700, loss 0.09543432295322418\n",
      "TEST accuracy at end of epoch 121: 0.886\n",
      "TRAIN accuracy at end of epoch 121: 0.97\n",
      "                          epoch 122\n",
      "iteration 47800, loss 0.044391047209501266\n",
      "iteration 47900, loss 0.13263066112995148\n",
      "iteration 48000, loss 0.09211726486682892\n",
      "TEST accuracy at end of epoch 122: 0.8847\n",
      "TRAIN accuracy at end of epoch 122: 1.0\n",
      "                          epoch 123\n",
      "iteration 48100, loss 0.06011350452899933\n",
      "iteration 48200, loss 0.03202568739652634\n",
      "iteration 48300, loss 0.05491211265325546\n",
      "iteration 48400, loss 0.054934266954660416\n",
      "TEST accuracy at end of epoch 123: 0.8726\n",
      "TRAIN accuracy at end of epoch 123: 0.99\n",
      "                          epoch 124\n",
      "iteration 48500, loss 0.07900895178318024\n",
      "iteration 48600, loss 0.16156750917434692\n",
      "iteration 48700, loss 0.013150626793503761\n",
      "iteration 48800, loss 0.07312457263469696\n",
      "TEST accuracy at end of epoch 124: 0.8784\n",
      "TRAIN accuracy at end of epoch 124: 1.0\n",
      "                          epoch 125\n",
      "iteration 48900, loss 0.1277340203523636\n",
      "iteration 49000, loss 0.04234093427658081\n",
      "iteration 49100, loss 0.11007176339626312\n",
      "iteration 49200, loss 0.053748778998851776\n",
      "TEST accuracy at end of epoch 125: 0.8824\n",
      "TRAIN accuracy at end of epoch 125: 1.0\n",
      "                          epoch 126\n",
      "iteration 49300, loss 0.10051969438791275\n",
      "iteration 49400, loss 0.024441951885819435\n",
      "iteration 49500, loss 0.0285435039550066\n",
      "iteration 49600, loss 0.08297029137611389\n",
      "TEST accuracy at end of epoch 126: 0.8855\n",
      "TRAIN accuracy at end of epoch 126: 0.99\n",
      "                          epoch 127\n",
      "iteration 49700, loss 0.05562776327133179\n",
      "iteration 49800, loss 0.03796838968992233\n",
      "iteration 49900, loss 0.06934951990842819\n",
      "iteration 50000, loss 0.02889455109834671\n",
      "TEST accuracy at end of epoch 127: 0.874\n",
      "TRAIN accuracy at end of epoch 127: 1.0\n",
      "                          epoch 128\n",
      "iteration 50100, loss 0.12521252036094666\n",
      "iteration 50200, loss 0.03613562881946564\n",
      "iteration 50300, loss 0.13004225492477417\n",
      "iteration 50400, loss 0.13147377967834473\n",
      "TEST accuracy at end of epoch 128: 0.8789\n",
      "TRAIN accuracy at end of epoch 128: 0.98\n",
      "                          epoch 129\n",
      "iteration 50500, loss 0.05425722151994705\n",
      "iteration 50600, loss 0.062065742909908295\n",
      "iteration 50700, loss 0.0293068066239357\n",
      "iteration 50800, loss 0.04538660869002342\n",
      "TEST accuracy at end of epoch 129: 0.8773\n",
      "TRAIN accuracy at end of epoch 129: 0.99\n",
      "                          epoch 130\n",
      "iteration 50900, loss 0.03808949887752533\n",
      "iteration 51000, loss 0.02444814145565033\n",
      "iteration 51100, loss 0.02336711250245571\n",
      "iteration 51200, loss 0.049441248178482056\n",
      "TEST accuracy at end of epoch 130: 0.882\n",
      "TRAIN accuracy at end of epoch 130: 1.0\n",
      "                          epoch 131\n",
      "iteration 51300, loss 0.10864847898483276\n",
      "iteration 51400, loss 0.051994770765304565\n",
      "iteration 51500, loss 0.06112522631883621\n",
      "iteration 51600, loss 0.01408536545932293\n",
      "TEST accuracy at end of epoch 131: 0.8839\n",
      "TRAIN accuracy at end of epoch 131: 0.99\n",
      "                          epoch 132\n",
      "iteration 51700, loss 0.02214350923895836\n",
      "iteration 51800, loss 0.012364227324724197\n",
      "iteration 51900, loss 0.08475954085588455\n",
      "iteration 52000, loss 0.039896346628665924\n",
      "TEST accuracy at end of epoch 132: 0.8817\n",
      "TRAIN accuracy at end of epoch 132: 0.99\n",
      "                          epoch 133\n",
      "iteration 52100, loss 0.03409917652606964\n",
      "iteration 52200, loss 0.027396008372306824\n",
      "iteration 52300, loss 0.05695311352610588\n",
      "TEST accuracy at end of epoch 133: 0.8871\n",
      "TRAIN accuracy at end of epoch 133: 1.0\n",
      "                          epoch 134\n",
      "iteration 52400, loss 0.04830259829759598\n",
      "iteration 52500, loss 0.06630120426416397\n",
      "iteration 52600, loss 0.05065710470080376\n",
      "iteration 52700, loss 0.03512880951166153\n",
      "TEST accuracy at end of epoch 134: 0.8816\n",
      "TRAIN accuracy at end of epoch 134: 1.0\n",
      "                          epoch 135\n",
      "iteration 52800, loss 0.032300062477588654\n",
      "iteration 52900, loss 0.06708166003227234\n",
      "iteration 53000, loss 0.030197840183973312\n",
      "iteration 53100, loss 0.016150156036019325\n",
      "TEST accuracy at end of epoch 135: 0.8811\n",
      "TRAIN accuracy at end of epoch 135: 0.99\n",
      "                          epoch 136\n",
      "iteration 53200, loss 0.06244775280356407\n",
      "iteration 53300, loss 0.08818411827087402\n",
      "iteration 53400, loss 0.0200022142380476\n",
      "iteration 53500, loss 0.09707501530647278\n",
      "TEST accuracy at end of epoch 136: 0.8836\n",
      "TRAIN accuracy at end of epoch 136: 1.0\n",
      "                          epoch 137\n",
      "iteration 53600, loss 0.02041606605052948\n",
      "iteration 53700, loss 0.04345966875553131\n",
      "iteration 53800, loss 0.022882483899593353\n",
      "iteration 53900, loss 0.04713357239961624\n",
      "TEST accuracy at end of epoch 137: 0.8854\n",
      "TRAIN accuracy at end of epoch 137: 0.99\n",
      "                          epoch 138\n",
      "iteration 54000, loss 0.027383800595998764\n",
      "iteration 54100, loss 0.030083918944001198\n",
      "iteration 54200, loss 0.08045533299446106\n",
      "iteration 54300, loss 0.03368689864873886\n",
      "TEST accuracy at end of epoch 138: 0.8828\n",
      "TRAIN accuracy at end of epoch 138: 1.0\n",
      "                          epoch 139\n",
      "iteration 54400, loss 0.1280459761619568\n",
      "iteration 54500, loss 0.013206738978624344\n",
      "iteration 54600, loss 0.08341129869222641\n",
      "iteration 54700, loss 0.045634254813194275\n",
      "TEST accuracy at end of epoch 139: 0.8838\n",
      "TRAIN accuracy at end of epoch 139: 1.0\n",
      "                          epoch 140\n",
      "iteration 54800, loss 0.03185112029314041\n",
      "iteration 54900, loss 0.028298377990722656\n",
      "iteration 55000, loss 0.04330902546644211\n",
      "iteration 55100, loss 0.07391546666622162\n",
      "TEST accuracy at end of epoch 140: 0.8832\n",
      "TRAIN accuracy at end of epoch 140: 1.0\n",
      "                          epoch 141\n",
      "iteration 55200, loss 0.04559648409485817\n",
      "iteration 55300, loss 0.02829565852880478\n",
      "iteration 55400, loss 0.03971198573708534\n",
      "iteration 55500, loss 0.03559392690658569\n",
      "TEST accuracy at end of epoch 141: 0.8814\n",
      "TRAIN accuracy at end of epoch 141: 1.0\n",
      "                          epoch 142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 55600, loss 0.09085985273122787\n",
      "iteration 55700, loss 0.021982021629810333\n",
      "iteration 55800, loss 0.060258250683546066\n",
      "iteration 55900, loss 0.0170435793697834\n",
      "TEST accuracy at end of epoch 142: 0.8868\n",
      "TRAIN accuracy at end of epoch 142: 1.0\n",
      "                          epoch 143\n",
      "iteration 56000, loss 0.02615153230726719\n",
      "iteration 56100, loss 0.031952135264873505\n",
      "iteration 56200, loss 0.027796965092420578\n",
      "iteration 56300, loss 0.011382290162146091\n",
      "TEST accuracy at end of epoch 143: 0.8875\n",
      "TRAIN accuracy at end of epoch 143: 0.99\n",
      "                          epoch 144\n",
      "iteration 56400, loss 0.028516709804534912\n",
      "iteration 56500, loss 0.02659296616911888\n",
      "iteration 56600, loss 0.027689900249242783\n",
      "TEST accuracy at end of epoch 144: 0.8838\n",
      "TRAIN accuracy at end of epoch 144: 0.99\n",
      "                          epoch 145\n",
      "iteration 56700, loss 0.03628856688737869\n",
      "iteration 56800, loss 0.011586964130401611\n",
      "iteration 56900, loss 0.020143046975135803\n",
      "iteration 57000, loss 0.04330502450466156\n",
      "TEST accuracy at end of epoch 145: 0.8827\n",
      "TRAIN accuracy at end of epoch 145: 1.0\n",
      "                          epoch 146\n",
      "iteration 57100, loss 0.03196343034505844\n",
      "iteration 57200, loss 0.08351580798625946\n",
      "iteration 57300, loss 0.02467327192425728\n",
      "iteration 57400, loss 0.015041498467326164\n",
      "TEST accuracy at end of epoch 146: 0.8872\n",
      "TRAIN accuracy at end of epoch 146: 1.0\n",
      "                          epoch 147\n",
      "iteration 57500, loss 0.017141137272119522\n",
      "iteration 57600, loss 0.005044925957918167\n",
      "iteration 57700, loss 0.004976950585842133\n",
      "iteration 57800, loss 0.034380946308374405\n",
      "TEST accuracy at end of epoch 147: 0.886\n",
      "TRAIN accuracy at end of epoch 147: 1.0\n",
      "                          epoch 148\n",
      "iteration 57900, loss 0.022793233394622803\n",
      "iteration 58000, loss 0.09087862819433212\n",
      "iteration 58100, loss 0.02080361545085907\n",
      "iteration 58200, loss 0.06262341141700745\n",
      "TEST accuracy at end of epoch 148: 0.8876\n",
      "TRAIN accuracy at end of epoch 148: 0.99\n",
      "                          epoch 149\n",
      "iteration 58300, loss 0.03470767289400101\n",
      "iteration 58400, loss 0.03556404262781143\n",
      "iteration 58500, loss 0.018917609006166458\n",
      "iteration 58600, loss 0.007448056247085333\n",
      "TEST accuracy at end of epoch 149: 0.8837\n",
      "TRAIN accuracy at end of epoch 149: 1.0\n",
      "                          epoch 150\n",
      "iteration 58700, loss 0.02667376399040222\n",
      "iteration 58800, loss 0.027453197166323662\n",
      "iteration 58900, loss 0.04987456277012825\n",
      "iteration 59000, loss 0.025255175307393074\n",
      "TEST accuracy at end of epoch 150: 0.8862\n",
      "TRAIN accuracy at end of epoch 150: 1.0\n",
      "                          epoch 151\n",
      "iteration 59100, loss 0.05953586846590042\n",
      "iteration 59200, loss 0.009510453790426254\n",
      "iteration 59300, loss 0.05055413395166397\n",
      "iteration 59400, loss 0.13304542005062103\n",
      "TEST accuracy at end of epoch 151: 0.8855\n",
      "TRAIN accuracy at end of epoch 151: 0.99\n",
      "                          epoch 152\n",
      "iteration 59500, loss 0.017473049461841583\n",
      "iteration 59600, loss 0.007229028269648552\n",
      "iteration 59700, loss 0.05661389231681824\n",
      "iteration 59800, loss 0.027729269117116928\n",
      "TEST accuracy at end of epoch 152: 0.8835\n",
      "TRAIN accuracy at end of epoch 152: 0.99\n",
      "                          epoch 153\n",
      "iteration 59900, loss 0.038757503032684326\n",
      "iteration 60000, loss 0.11321680247783661\n",
      "iteration 60100, loss 0.041947074234485626\n",
      "iteration 60200, loss 0.027356823906302452\n",
      "TEST accuracy at end of epoch 153: 0.8831\n",
      "TRAIN accuracy at end of epoch 153: 1.0\n",
      "                          epoch 154\n",
      "iteration 60300, loss 0.008153147995471954\n",
      "iteration 60400, loss 0.044804178178310394\n",
      "iteration 60500, loss 0.018627841025590897\n",
      "iteration 60600, loss 0.0111636221408844\n",
      "TEST accuracy at end of epoch 154: 0.8855\n",
      "TRAIN accuracy at end of epoch 154: 0.99\n",
      "                          epoch 155\n",
      "iteration 60700, loss 0.014536715112626553\n",
      "iteration 60800, loss 0.013905521482229233\n",
      "iteration 60900, loss 0.026632964611053467\n",
      "TEST accuracy at end of epoch 155: 0.8872\n",
      "TRAIN accuracy at end of epoch 155: 1.0\n",
      "                          epoch 156\n",
      "iteration 61000, loss 0.05721394717693329\n",
      "iteration 61100, loss 0.03365842252969742\n",
      "iteration 61200, loss 0.06118428707122803\n",
      "iteration 61300, loss 0.0034583336673676968\n",
      "TEST accuracy at end of epoch 156: 0.8888\n",
      "TRAIN accuracy at end of epoch 156: 1.0\n",
      "                          epoch 157\n",
      "iteration 61400, loss 0.018496762961149216\n",
      "iteration 61500, loss 0.06267915666103363\n",
      "iteration 61600, loss 0.009291509166359901\n",
      "iteration 61700, loss 0.015380961820483208\n",
      "TEST accuracy at end of epoch 157: 0.8881\n",
      "TRAIN accuracy at end of epoch 157: 1.0\n",
      "                          epoch 158\n",
      "iteration 61800, loss 0.011554906144738197\n",
      "iteration 61900, loss 0.026769079267978668\n",
      "iteration 62000, loss 0.01783858984708786\n",
      "iteration 62100, loss 0.019485585391521454\n",
      "TEST accuracy at end of epoch 158: 0.8861\n",
      "TRAIN accuracy at end of epoch 158: 1.0\n",
      "                          epoch 159\n",
      "iteration 62200, loss 0.020162684842944145\n",
      "iteration 62300, loss 0.015360823832452297\n",
      "iteration 62400, loss 0.028438422828912735\n",
      "iteration 62500, loss 0.0110400952398777\n",
      "TEST accuracy at end of epoch 159: 0.8885\n",
      "TRAIN accuracy at end of epoch 159: 1.0\n",
      "                          epoch 160\n",
      "iteration 62600, loss 0.013775069266557693\n",
      "iteration 62700, loss 0.012078328989446163\n",
      "iteration 62800, loss 0.08066791296005249\n",
      "iteration 62900, loss 0.00959421880543232\n",
      "TEST accuracy at end of epoch 160: 0.888\n",
      "TRAIN accuracy at end of epoch 160: 1.0\n",
      "                          epoch 161\n",
      "iteration 63000, loss 0.009094072505831718\n",
      "iteration 63100, loss 0.013144051656126976\n",
      "iteration 63200, loss 0.03265435993671417\n",
      "iteration 63300, loss 0.025070101022720337\n",
      "TEST accuracy at end of epoch 161: 0.8873\n",
      "TRAIN accuracy at end of epoch 161: 1.0\n",
      "                          epoch 162\n",
      "iteration 63400, loss 0.01106969639658928\n",
      "iteration 63500, loss 0.011086555197834969\n",
      "iteration 63600, loss 0.009446067735552788\n",
      "iteration 63700, loss 0.00907367654144764\n",
      "TEST accuracy at end of epoch 162: 0.888\n",
      "TRAIN accuracy at end of epoch 162: 1.0\n",
      "                          epoch 163\n",
      "iteration 63800, loss 0.028415899723768234\n",
      "iteration 63900, loss 0.015647999942302704\n",
      "iteration 64000, loss 0.04199422895908356\n",
      "iteration 64100, loss 0.03558732569217682\n",
      "TEST accuracy at end of epoch 163: 0.891\n",
      "TRAIN accuracy at end of epoch 163: 1.0\n",
      "                          epoch 164\n",
      "iteration 64200, loss 0.006870747543871403\n",
      "iteration 64300, loss 0.008485633879899979\n",
      "iteration 64400, loss 0.05057330057024956\n",
      "iteration 64500, loss 0.009857643395662308\n",
      "TEST accuracy at end of epoch 164: 0.8901\n",
      "TRAIN accuracy at end of epoch 164: 1.0\n",
      "                          epoch 165\n",
      "iteration 64600, loss 0.0435263067483902\n",
      "iteration 64700, loss 0.03452218323945999\n",
      "iteration 64800, loss 0.035879526287317276\n",
      "iteration 64900, loss 0.002887070644646883\n",
      "TEST accuracy at end of epoch 165: 0.8891\n",
      "TRAIN accuracy at end of epoch 165: 1.0\n",
      "                          epoch 166\n",
      "iteration 65000, loss 0.00968601368367672\n",
      "iteration 65100, loss 0.0019278781255707145\n",
      "iteration 65200, loss 0.03831968456506729\n",
      "TEST accuracy at end of epoch 166: 0.8886\n",
      "TRAIN accuracy at end of epoch 166: 1.0\n",
      "                          epoch 167\n",
      "iteration 65300, loss 0.02693374641239643\n",
      "iteration 65400, loss 0.009403731673955917\n",
      "iteration 65500, loss 0.0014484775019809604\n",
      "iteration 65600, loss 0.01446828804910183\n",
      "TEST accuracy at end of epoch 167: 0.8908\n",
      "TRAIN accuracy at end of epoch 167: 1.0\n",
      "                          epoch 168\n",
      "iteration 65700, loss 0.01869579590857029\n",
      "iteration 65800, loss 0.04710671305656433\n",
      "iteration 65900, loss 0.005394086707383394\n",
      "iteration 66000, loss 0.020113687962293625\n",
      "TEST accuracy at end of epoch 168: 0.8907\n",
      "TRAIN accuracy at end of epoch 168: 1.0\n",
      "                          epoch 169\n",
      "iteration 66100, loss 0.006440495140850544\n",
      "iteration 66200, loss 0.022364994511008263\n",
      "iteration 66300, loss 0.014119768515229225\n",
      "iteration 66400, loss 0.06698744744062424\n",
      "TEST accuracy at end of epoch 169: 0.8907\n",
      "TRAIN accuracy at end of epoch 169: 1.0\n",
      "                          epoch 170\n",
      "iteration 66500, loss 0.003228226210922003\n",
      "iteration 66600, loss 0.010943436995148659\n",
      "iteration 66700, loss 0.01137373223900795\n",
      "iteration 66800, loss 0.017147526144981384\n",
      "TEST accuracy at end of epoch 170: 0.8866\n",
      "TRAIN accuracy at end of epoch 170: 1.0\n",
      "                          epoch 171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 66900, loss 0.008683517575263977\n",
      "iteration 67000, loss 0.02025688998401165\n",
      "iteration 67100, loss 0.04154966399073601\n",
      "iteration 67200, loss 0.0025202478282153606\n",
      "TEST accuracy at end of epoch 171: 0.8908\n",
      "TRAIN accuracy at end of epoch 171: 1.0\n",
      "                          epoch 172\n",
      "iteration 67300, loss 0.021284058690071106\n",
      "iteration 67400, loss 0.03471050038933754\n",
      "iteration 67500, loss 0.012944478541612625\n",
      "iteration 67600, loss 0.019834529608488083\n",
      "TEST accuracy at end of epoch 172: 0.8907\n",
      "TRAIN accuracy at end of epoch 172: 1.0\n",
      "                          epoch 173\n",
      "iteration 67700, loss 0.010312017053365707\n",
      "iteration 67800, loss 0.020304523408412933\n",
      "iteration 67900, loss 0.000987958163022995\n",
      "iteration 68000, loss 0.038281939923763275\n",
      "TEST accuracy at end of epoch 173: 0.8918\n",
      "TRAIN accuracy at end of epoch 173: 1.0\n",
      "                          epoch 174\n",
      "iteration 68100, loss 0.004867921583354473\n",
      "iteration 68200, loss 0.012302125804126263\n",
      "iteration 68300, loss 0.017813334241509438\n",
      "iteration 68400, loss 0.005113857798278332\n",
      "TEST accuracy at end of epoch 174: 0.8901\n",
      "TRAIN accuracy at end of epoch 174: 1.0\n",
      "                          epoch 175\n",
      "iteration 68500, loss 0.012243855744600296\n",
      "iteration 68600, loss 0.03108711540699005\n",
      "iteration 68700, loss 0.03003343939781189\n",
      "iteration 68800, loss 0.009991221129894257\n",
      "TEST accuracy at end of epoch 175: 0.8917\n",
      "TRAIN accuracy at end of epoch 175: 1.0\n",
      "                          epoch 176\n",
      "iteration 68900, loss 0.03572836145758629\n",
      "iteration 69000, loss 0.04932788759469986\n",
      "iteration 69100, loss 0.004139608237892389\n",
      "iteration 69200, loss 0.00965746957808733\n",
      "TEST accuracy at end of epoch 176: 0.8913\n",
      "TRAIN accuracy at end of epoch 176: 1.0\n",
      "                          epoch 177\n",
      "iteration 69300, loss 0.004359654150903225\n",
      "iteration 69400, loss 0.009112119674682617\n",
      "iteration 69500, loss 0.03884594142436981\n",
      "TEST accuracy at end of epoch 177: 0.8921\n",
      "TRAIN accuracy at end of epoch 177: 1.0\n",
      "                          epoch 178\n",
      "iteration 69600, loss 0.004271018318831921\n",
      "iteration 69700, loss 0.004487416706979275\n",
      "iteration 69800, loss 0.0051882099360227585\n",
      "iteration 69900, loss 0.007927699945867062\n",
      "TEST accuracy at end of epoch 178: 0.8921\n",
      "TRAIN accuracy at end of epoch 178: 1.0\n",
      "                          epoch 179\n",
      "iteration 70000, loss 0.015780897811055183\n",
      "iteration 70100, loss 0.0379784032702446\n",
      "iteration 70200, loss 0.040288522839546204\n",
      "iteration 70300, loss 0.03232118487358093\n",
      "TEST accuracy at end of epoch 179: 0.8917\n",
      "TRAIN accuracy at end of epoch 179: 1.0\n",
      "                          epoch 180\n",
      "iteration 70400, loss 0.0048365904949605465\n",
      "iteration 70500, loss 0.007264736108481884\n",
      "iteration 70600, loss 0.02728917822241783\n",
      "iteration 70700, loss 0.006402921862900257\n",
      "TEST accuracy at end of epoch 180: 0.8918\n",
      "TRAIN accuracy at end of epoch 180: 1.0\n",
      "                          epoch 181\n",
      "iteration 70800, loss 0.007118691690266132\n",
      "iteration 70900, loss 0.0405706912279129\n",
      "iteration 71000, loss 0.019602496176958084\n",
      "iteration 71100, loss 0.011159656569361687\n",
      "TEST accuracy at end of epoch 181: 0.8911\n",
      "TRAIN accuracy at end of epoch 181: 1.0\n",
      "                          epoch 182\n",
      "iteration 71200, loss 0.0027384518180042505\n",
      "iteration 71300, loss 0.008222486823797226\n",
      "iteration 71400, loss 0.014199844561517239\n",
      "iteration 71500, loss 0.04272913187742233\n",
      "TEST accuracy at end of epoch 182: 0.8917\n",
      "TRAIN accuracy at end of epoch 182: 1.0\n",
      "                          epoch 183\n",
      "iteration 71600, loss 0.006222852505743504\n",
      "iteration 71700, loss 0.032526131719350815\n",
      "iteration 71800, loss 0.013677679933607578\n",
      "iteration 71900, loss 0.018607553094625473\n",
      "TEST accuracy at end of epoch 183: 0.8899\n",
      "TRAIN accuracy at end of epoch 183: 1.0\n",
      "                          epoch 184\n",
      "iteration 72000, loss 0.020067861303687096\n",
      "iteration 72100, loss 0.01265534944832325\n",
      "iteration 72200, loss 0.026523733511567116\n",
      "iteration 72300, loss 0.05463365837931633\n",
      "TEST accuracy at end of epoch 184: 0.8904\n",
      "TRAIN accuracy at end of epoch 184: 1.0\n",
      "                          epoch 185\n",
      "iteration 72400, loss 0.007130562327802181\n",
      "iteration 72500, loss 0.009977960959076881\n",
      "iteration 72600, loss 0.012420786544680595\n",
      "iteration 72700, loss 0.07319576293230057\n",
      "TEST accuracy at end of epoch 185: 0.8896\n",
      "TRAIN accuracy at end of epoch 185: 1.0\n",
      "                          epoch 186\n",
      "iteration 72800, loss 0.04103360325098038\n",
      "iteration 72900, loss 0.012993440963327885\n",
      "iteration 73000, loss 0.02355719730257988\n",
      "iteration 73100, loss 0.0072305514477193356\n",
      "TEST accuracy at end of epoch 186: 0.8903\n",
      "TRAIN accuracy at end of epoch 186: 1.0\n",
      "                          epoch 187\n",
      "iteration 73200, loss 0.011375141330063343\n",
      "iteration 73300, loss 0.02945106104016304\n",
      "iteration 73400, loss 0.01113918237388134\n",
      "iteration 73500, loss 0.0036021177656948566\n",
      "TEST accuracy at end of epoch 187: 0.8895\n",
      "TRAIN accuracy at end of epoch 187: 0.99\n",
      "                          epoch 188\n",
      "iteration 73600, loss 0.003351082094013691\n",
      "iteration 73700, loss 0.001804168103262782\n",
      "iteration 73800, loss 0.012953010387718678\n",
      "TEST accuracy at end of epoch 188: 0.8912\n",
      "TRAIN accuracy at end of epoch 188: 1.0\n",
      "                          epoch 189\n",
      "iteration 73900, loss 0.012412040494382381\n",
      "iteration 74000, loss 0.03710357844829559\n",
      "iteration 74100, loss 0.00633271224796772\n",
      "iteration 74200, loss 0.01715432107448578\n",
      "TEST accuracy at end of epoch 189: 0.8908\n",
      "TRAIN accuracy at end of epoch 189: 1.0\n",
      "                          epoch 190\n",
      "iteration 74300, loss 0.007981671020388603\n",
      "iteration 74400, loss 0.02807459980249405\n",
      "iteration 74500, loss 0.05858500301837921\n",
      "iteration 74600, loss 0.03047213703393936\n",
      "TEST accuracy at end of epoch 190: 0.8909\n",
      "TRAIN accuracy at end of epoch 190: 1.0\n",
      "                          epoch 191\n",
      "iteration 74700, loss 0.017744451761245728\n",
      "iteration 74800, loss 0.0067583248019218445\n",
      "iteration 74900, loss 0.009154866449534893\n",
      "iteration 75000, loss 0.013735739514231682\n",
      "TEST accuracy at end of epoch 191: 0.8908\n",
      "TRAIN accuracy at end of epoch 191: 1.0\n",
      "                          epoch 192\n",
      "iteration 75100, loss 0.018374238163232803\n",
      "iteration 75200, loss 0.03628008812665939\n",
      "iteration 75300, loss 0.017474766820669174\n",
      "iteration 75400, loss 0.06475384533405304\n",
      "TEST accuracy at end of epoch 192: 0.8911\n",
      "TRAIN accuracy at end of epoch 192: 1.0\n",
      "                          epoch 193\n",
      "iteration 75500, loss 0.02193225547671318\n",
      "iteration 75600, loss 0.004201996140182018\n",
      "iteration 75700, loss 0.04400649666786194\n",
      "iteration 75800, loss 0.008318599313497543\n",
      "TEST accuracy at end of epoch 193: 0.8914\n",
      "TRAIN accuracy at end of epoch 193: 1.0\n",
      "                          epoch 194\n",
      "iteration 75900, loss 0.012952334247529507\n",
      "iteration 76000, loss 0.03126847743988037\n",
      "iteration 76100, loss 0.005246131680905819\n",
      "iteration 76200, loss 0.04390854388475418\n",
      "TEST accuracy at end of epoch 194: 0.8915\n",
      "TRAIN accuracy at end of epoch 194: 1.0\n",
      "                          epoch 195\n",
      "iteration 76300, loss 0.008071916177868843\n",
      "iteration 76400, loss 0.010359648615121841\n",
      "iteration 76500, loss 0.015605503693223\n",
      "iteration 76600, loss 0.004192839376628399\n",
      "TEST accuracy at end of epoch 195: 0.8918\n",
      "TRAIN accuracy at end of epoch 195: 1.0\n",
      "                          epoch 196\n",
      "iteration 76700, loss 0.060842327773571014\n",
      "iteration 76800, loss 0.007333574816584587\n",
      "iteration 76900, loss 0.029083050787448883\n",
      "iteration 77000, loss 0.029293879866600037\n",
      "TEST accuracy at end of epoch 196: 0.891\n",
      "TRAIN accuracy at end of epoch 196: 1.0\n",
      "                          epoch 197\n",
      "iteration 77100, loss 0.0016265485901385546\n",
      "iteration 77200, loss 0.014388618059456348\n",
      "iteration 77300, loss 0.01341564767062664\n",
      "iteration 77400, loss 0.011649266816675663\n",
      "TEST accuracy at end of epoch 197: 0.8912\n",
      "TRAIN accuracy at end of epoch 197: 1.0\n",
      "                          epoch 198\n",
      "iteration 77500, loss 0.006757755763828754\n",
      "iteration 77600, loss 0.006630781572312117\n",
      "iteration 77700, loss 0.053656622767448425\n",
      "iteration 77800, loss 0.01801668107509613\n",
      "TEST accuracy at end of epoch 198: 0.8911\n",
      "TRAIN accuracy at end of epoch 198: 1.0\n",
      "                          epoch 199\n",
      "iteration 77900, loss 0.018173733726143837\n",
      "iteration 78000, loss 0.019338343292474747\n",
      "iteration 78100, loss 0.004759506788104773\n",
      "iteration 78200, loss 0.03478884696960449\n",
      "TEST accuracy at end of epoch 199: 0.8911\n",
      "TRAIN accuracy at end of epoch 199: 1.0\n",
      "╰┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╯\n",
      "╭┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╮\n",
      "                         n: 3, res: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of reduce_mean: (?, 1, 1, 64)\n",
      "size of squeeze: (?, 64)\n",
      "training for 78125 iterations\n",
      "                          epoch 0\n",
      "iteration 100, loss 1.932978630065918\n",
      "iteration 200, loss 1.7389403581619263\n",
      "iteration 300, loss 1.426910400390625\n",
      "TEST accuracy at end of epoch 0: 0.209\n",
      "TRAIN accuracy at end of epoch 0: 0.26\n",
      "                          epoch 1\n",
      "iteration 400, loss 1.2281529903411865\n",
      "iteration 500, loss 1.3895256519317627\n",
      "iteration 600, loss 1.050903558731079\n",
      "iteration 700, loss 0.9340241551399231\n",
      "TEST accuracy at end of epoch 1: 0.2437\n",
      "TRAIN accuracy at end of epoch 1: 0.24\n",
      "                          epoch 2\n",
      "iteration 800, loss 0.8202320337295532\n",
      "iteration 900, loss 0.9581519365310669\n",
      "iteration 1000, loss 0.9046660661697388\n",
      "iteration 1100, loss 0.9975936412811279\n",
      "TEST accuracy at end of epoch 2: 0.439\n",
      "TRAIN accuracy at end of epoch 2: 0.41\n",
      "                          epoch 3\n",
      "iteration 1200, loss 0.7111362218856812\n",
      "iteration 1300, loss 0.8005509376525879\n",
      "iteration 1400, loss 0.8652355670928955\n",
      "iteration 1500, loss 0.7907440066337585\n",
      "TEST accuracy at end of epoch 3: 0.551\n",
      "TRAIN accuracy at end of epoch 3: 0.6\n",
      "                          epoch 4\n",
      "iteration 1600, loss 0.7729867100715637\n",
      "iteration 1700, loss 0.7806574702262878\n",
      "iteration 1800, loss 0.6951730847358704\n",
      "iteration 1900, loss 0.5893067717552185\n",
      "TEST accuracy at end of epoch 4: 0.5902\n",
      "TRAIN accuracy at end of epoch 4: 0.62\n",
      "                          epoch 5\n",
      "iteration 2000, loss 0.6404585838317871\n",
      "iteration 2100, loss 0.43358656764030457\n",
      "iteration 2200, loss 0.6422537565231323\n",
      "iteration 2300, loss 0.7517869472503662\n",
      "TEST accuracy at end of epoch 5: 0.5916\n",
      "TRAIN accuracy at end of epoch 5: 0.62\n",
      "                          epoch 6\n",
      "iteration 2400, loss 0.4915456175804138\n",
      "iteration 2500, loss 0.631140947341919\n",
      "iteration 2600, loss 0.6230776309967041\n",
      "iteration 2700, loss 0.6232236623764038\n",
      "TEST accuracy at end of epoch 6: 0.5737\n",
      "TRAIN accuracy at end of epoch 6: 0.58\n",
      "                          epoch 7\n",
      "iteration 2800, loss 0.4625898003578186\n",
      "iteration 2900, loss 0.6389439702033997\n",
      "iteration 3000, loss 0.5510817766189575\n",
      "iteration 3100, loss 0.5651659965515137\n",
      "TEST accuracy at end of epoch 7: 0.733\n",
      "TRAIN accuracy at end of epoch 7: 0.77\n",
      "                          epoch 8\n",
      "iteration 3200, loss 0.5102093815803528\n",
      "iteration 3300, loss 0.4650760293006897\n",
      "iteration 3400, loss 0.6556848287582397\n",
      "iteration 3500, loss 0.5813733339309692\n",
      "TEST accuracy at end of epoch 8: 0.7596\n",
      "TRAIN accuracy at end of epoch 8: 0.8\n",
      "                          epoch 9\n",
      "iteration 3600, loss 0.6525185108184814\n",
      "iteration 3700, loss 0.42596983909606934\n",
      "iteration 3800, loss 0.40900540351867676\n",
      "iteration 3900, loss 0.44660893082618713\n",
      "TEST accuracy at end of epoch 9: 0.7321\n",
      "TRAIN accuracy at end of epoch 9: 0.69\n",
      "                          epoch 10\n",
      "iteration 4000, loss 0.5567999482154846\n",
      "iteration 4100, loss 0.4654591679573059\n",
      "iteration 4200, loss 0.46231189370155334\n",
      "iteration 4300, loss 0.502668023109436\n",
      "TEST accuracy at end of epoch 10: 0.7302\n",
      "TRAIN accuracy at end of epoch 10: 0.78\n",
      "                          epoch 11\n",
      "iteration 4400, loss 0.5303848385810852\n",
      "iteration 4500, loss 0.5692883729934692\n",
      "iteration 4600, loss 0.4171043634414673\n",
      "TEST accuracy at end of epoch 11: 0.7694\n",
      "TRAIN accuracy at end of epoch 11: 0.73\n",
      "                          epoch 12\n",
      "iteration 4700, loss 0.4763854146003723\n",
      "iteration 4800, loss 0.425750195980072\n",
      "iteration 4900, loss 0.6181074380874634\n",
      "iteration 5000, loss 0.46929895877838135\n",
      "TEST accuracy at end of epoch 12: 0.7586\n",
      "TRAIN accuracy at end of epoch 12: 0.75\n",
      "                          epoch 13\n",
      "iteration 5100, loss 0.45520344376564026\n",
      "iteration 5200, loss 0.40874040126800537\n",
      "iteration 5300, loss 0.2930454611778259\n",
      "iteration 5400, loss 0.41209501028060913\n",
      "TEST accuracy at end of epoch 13: 0.6951\n",
      "TRAIN accuracy at end of epoch 13: 0.7\n",
      "                          epoch 14\n",
      "iteration 5500, loss 0.5386315584182739\n",
      "iteration 5600, loss 0.5274596214294434\n",
      "iteration 5700, loss 0.379416286945343\n",
      "iteration 5800, loss 0.43346887826919556\n",
      "TEST accuracy at end of epoch 14: 0.7828\n",
      "TRAIN accuracy at end of epoch 14: 0.85\n",
      "                          epoch 15\n",
      "iteration 5900, loss 0.480764776468277\n",
      "iteration 6000, loss 0.4891698360443115\n",
      "iteration 6100, loss 0.6004229784011841\n",
      "iteration 6200, loss 0.37234675884246826\n",
      "TEST accuracy at end of epoch 15: 0.6797\n",
      "TRAIN accuracy at end of epoch 15: 0.74\n",
      "                          epoch 16\n",
      "iteration 6300, loss 0.4117564558982849\n",
      "iteration 6400, loss 0.26727408170700073\n",
      "iteration 6500, loss 0.3859032392501831\n",
      "iteration 6600, loss 0.3568628430366516\n",
      "TEST accuracy at end of epoch 16: 0.8203\n",
      "TRAIN accuracy at end of epoch 16: 0.85\n",
      "                          epoch 17\n",
      "iteration 6700, loss 0.32816368341445923\n",
      "iteration 6800, loss 0.3386830985546112\n",
      "iteration 6900, loss 0.3912222981452942\n",
      "iteration 7000, loss 0.42840856313705444\n",
      "TEST accuracy at end of epoch 17: 0.7882\n",
      "TRAIN accuracy at end of epoch 17: 0.76\n",
      "                          epoch 18\n",
      "iteration 7100, loss 0.3859938383102417\n",
      "iteration 7200, loss 0.5958935618400574\n",
      "iteration 7300, loss 0.4001726508140564\n",
      "iteration 7400, loss 0.33974796533584595\n",
      "TEST accuracy at end of epoch 18: 0.7828\n",
      "TRAIN accuracy at end of epoch 18: 0.77\n",
      "                          epoch 19\n",
      "iteration 7500, loss 0.3933366537094116\n",
      "iteration 7600, loss 0.4293897747993469\n",
      "iteration 7700, loss 0.24765673279762268\n",
      "iteration 7800, loss 0.2815852761268616\n",
      "TEST accuracy at end of epoch 19: 0.8362\n",
      "TRAIN accuracy at end of epoch 19: 0.85\n",
      "                          epoch 20\n",
      "iteration 7900, loss 0.32517921924591064\n",
      "iteration 8000, loss 0.36791718006134033\n",
      "iteration 8100, loss 0.38527292013168335\n",
      "iteration 8200, loss 0.417033851146698\n",
      "TEST accuracy at end of epoch 20: 0.8338\n",
      "TRAIN accuracy at end of epoch 20: 0.9\n",
      "                          epoch 21\n",
      "iteration 8300, loss 0.37383347749710083\n",
      "iteration 8400, loss 0.3638421595096588\n",
      "iteration 8500, loss 0.23283475637435913\n",
      "iteration 8600, loss 0.3552780747413635\n",
      "TEST accuracy at end of epoch 21: 0.8322\n",
      "TRAIN accuracy at end of epoch 21: 0.85\n",
      "                          epoch 22\n",
      "iteration 8700, loss 0.22098861634731293\n",
      "iteration 8800, loss 0.36473923921585083\n",
      "iteration 8900, loss 0.27604079246520996\n",
      "TEST accuracy at end of epoch 22: 0.6902\n",
      "TRAIN accuracy at end of epoch 22: 0.67\n",
      "                          epoch 23\n",
      "iteration 9000, loss 0.2664386034011841\n",
      "iteration 9100, loss 0.3042498826980591\n",
      "iteration 9200, loss 0.32355451583862305\n",
      "iteration 9300, loss 0.3927098512649536\n",
      "TEST accuracy at end of epoch 23: 0.8192\n",
      "TRAIN accuracy at end of epoch 23: 0.89\n",
      "                          epoch 24\n",
      "iteration 9400, loss 0.44015297293663025\n",
      "iteration 9500, loss 0.42347973585128784\n",
      "iteration 9600, loss 0.40333861112594604\n",
      "iteration 9700, loss 0.35433119535446167\n",
      "TEST accuracy at end of epoch 24: 0.8457\n",
      "TRAIN accuracy at end of epoch 24: 0.86\n",
      "                          epoch 25\n",
      "iteration 9800, loss 0.30378657579421997\n",
      "iteration 9900, loss 0.38703975081443787\n",
      "iteration 10000, loss 0.46850594878196716\n",
      "iteration 10100, loss 0.45773935317993164\n",
      "TEST accuracy at end of epoch 25: 0.78\n",
      "TRAIN accuracy at end of epoch 25: 0.8\n",
      "                          epoch 26\n",
      "iteration 10200, loss 0.30017513036727905\n",
      "iteration 10300, loss 0.33668053150177\n",
      "iteration 10400, loss 0.27030813694000244\n",
      "iteration 10500, loss 0.3087393045425415\n",
      "TEST accuracy at end of epoch 26: 0.7894\n",
      "TRAIN accuracy at end of epoch 26: 0.8\n",
      "                          epoch 27\n",
      "iteration 10600, loss 0.25974154472351074\n",
      "iteration 10700, loss 0.34191909432411194\n",
      "iteration 10800, loss 0.42861923575401306\n",
      "iteration 10900, loss 0.41990387439727783\n",
      "TEST accuracy at end of epoch 27: 0.8395\n",
      "TRAIN accuracy at end of epoch 27: 0.84\n",
      "                          epoch 28\n",
      "iteration 11000, loss 0.36798202991485596\n",
      "iteration 11100, loss 0.3325069546699524\n",
      "iteration 11200, loss 0.22365784645080566\n",
      "iteration 11300, loss 0.29065749049186707\n",
      "TEST accuracy at end of epoch 28: 0.8104\n",
      "TRAIN accuracy at end of epoch 28: 0.82\n",
      "                          epoch 29\n",
      "iteration 11400, loss 0.219752699136734\n",
      "iteration 11500, loss 0.36225688457489014\n",
      "iteration 11600, loss 0.3080647587776184\n",
      "iteration 11700, loss 0.3351075053215027\n",
      "TEST accuracy at end of epoch 29: 0.827\n",
      "TRAIN accuracy at end of epoch 29: 0.83\n",
      "                          epoch 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 11800, loss 0.1449499875307083\n",
      "iteration 11900, loss 0.2862008213996887\n",
      "iteration 12000, loss 0.35474374890327454\n",
      "iteration 12100, loss 0.30367493629455566\n",
      "TEST accuracy at end of epoch 30: 0.8127\n",
      "TRAIN accuracy at end of epoch 30: 0.82\n",
      "                          epoch 31\n",
      "iteration 12200, loss 0.2698718011379242\n",
      "iteration 12300, loss 0.29886138439178467\n",
      "iteration 12400, loss 0.33486801385879517\n",
      "iteration 12500, loss 0.28699401021003723\n",
      "TEST accuracy at end of epoch 31: 0.8251\n",
      "TRAIN accuracy at end of epoch 31: 0.86\n",
      "                          epoch 32\n",
      "iteration 12600, loss 0.18160876631736755\n",
      "iteration 12700, loss 0.2335689663887024\n",
      "iteration 12800, loss 0.24628543853759766\n",
      "iteration 12900, loss 0.22565597295761108\n",
      "TEST accuracy at end of epoch 32: 0.8359\n",
      "TRAIN accuracy at end of epoch 32: 0.9\n",
      "                          epoch 33\n",
      "iteration 13000, loss 0.2548414468765259\n",
      "iteration 13100, loss 0.3276596665382385\n",
      "iteration 13200, loss 0.20307716727256775\n",
      "TEST accuracy at end of epoch 33: 0.8603\n",
      "TRAIN accuracy at end of epoch 33: 0.97\n",
      "                          epoch 34\n",
      "iteration 13300, loss 0.20531244575977325\n",
      "iteration 13400, loss 0.29654666781425476\n",
      "iteration 13500, loss 0.1956891119480133\n",
      "iteration 13600, loss 0.24588747322559357\n",
      "TEST accuracy at end of epoch 34: 0.8058\n",
      "TRAIN accuracy at end of epoch 34: 0.83\n",
      "                          epoch 35\n",
      "iteration 13700, loss 0.23729684948921204\n",
      "iteration 13800, loss 0.21067070960998535\n",
      "iteration 13900, loss 0.3683779239654541\n",
      "iteration 14000, loss 0.2318440079689026\n",
      "TEST accuracy at end of epoch 35: 0.8433\n",
      "TRAIN accuracy at end of epoch 35: 0.92\n",
      "                          epoch 36\n",
      "iteration 14100, loss 0.2539532482624054\n",
      "iteration 14200, loss 0.23171155154705048\n",
      "iteration 14300, loss 0.23070894181728363\n",
      "iteration 14400, loss 0.22653931379318237\n",
      "TEST accuracy at end of epoch 36: 0.8408\n",
      "TRAIN accuracy at end of epoch 36: 0.9\n",
      "                          epoch 37\n",
      "iteration 14500, loss 0.14262235164642334\n",
      "iteration 14600, loss 0.24140562117099762\n",
      "iteration 14700, loss 0.45007437467575073\n",
      "iteration 14800, loss 0.1631084680557251\n",
      "TEST accuracy at end of epoch 37: 0.8642\n",
      "TRAIN accuracy at end of epoch 37: 0.91\n",
      "                          epoch 38\n",
      "iteration 14900, loss 0.2614913582801819\n",
      "iteration 15000, loss 0.2830759286880493\n",
      "iteration 15100, loss 0.3422948122024536\n",
      "iteration 15200, loss 0.2257515788078308\n",
      "TEST accuracy at end of epoch 38: 0.8531\n",
      "TRAIN accuracy at end of epoch 38: 0.89\n",
      "                          epoch 39\n",
      "iteration 15300, loss 0.29976940155029297\n",
      "iteration 15400, loss 0.26042407751083374\n",
      "iteration 15500, loss 0.22994911670684814\n",
      "iteration 15600, loss 0.24989044666290283\n",
      "TEST accuracy at end of epoch 39: 0.8541\n",
      "TRAIN accuracy at end of epoch 39: 0.86\n",
      "                          epoch 40\n",
      "iteration 15700, loss 0.39970824122428894\n",
      "iteration 15800, loss 0.22063350677490234\n",
      "iteration 15900, loss 0.14842340350151062\n",
      "iteration 16000, loss 0.3389413356781006\n",
      "TEST accuracy at end of epoch 40: 0.8472\n",
      "TRAIN accuracy at end of epoch 40: 0.85\n",
      "                          epoch 41\n",
      "iteration 16100, loss 0.22775150835514069\n",
      "iteration 16200, loss 0.20269374549388885\n",
      "iteration 16300, loss 0.2618412375450134\n",
      "iteration 16400, loss 0.17929285764694214\n",
      "TEST accuracy at end of epoch 41: 0.8387\n",
      "TRAIN accuracy at end of epoch 41: 0.91\n",
      "                          epoch 42\n",
      "iteration 16500, loss 0.27967479825019836\n",
      "iteration 16600, loss 0.2614947557449341\n",
      "iteration 16700, loss 0.21373653411865234\n",
      "iteration 16800, loss 0.1439467966556549\n",
      "TEST accuracy at end of epoch 42: 0.8474\n",
      "TRAIN accuracy at end of epoch 42: 0.9\n",
      "                          epoch 43\n",
      "iteration 16900, loss 0.177130788564682\n",
      "iteration 17000, loss 0.21605807542800903\n",
      "iteration 17100, loss 0.2539639174938202\n",
      "iteration 17200, loss 0.20056027173995972\n",
      "TEST accuracy at end of epoch 43: 0.8003\n",
      "TRAIN accuracy at end of epoch 43: 0.78\n",
      "                          epoch 44\n",
      "iteration 17300, loss 0.22580453753471375\n",
      "iteration 17400, loss 0.16113117337226868\n",
      "iteration 17500, loss 0.2617257833480835\n",
      "TEST accuracy at end of epoch 44: 0.8476\n",
      "TRAIN accuracy at end of epoch 44: 0.89\n",
      "                          epoch 45\n",
      "iteration 17600, loss 0.29438814520835876\n",
      "iteration 17700, loss 0.19693732261657715\n",
      "iteration 17800, loss 0.24644693732261658\n",
      "iteration 17900, loss 0.17730514705181122\n",
      "TEST accuracy at end of epoch 45: 0.8625\n",
      "TRAIN accuracy at end of epoch 45: 0.94\n",
      "                          epoch 46\n",
      "iteration 18000, loss 0.17805558443069458\n",
      "iteration 18100, loss 0.13043339550495148\n",
      "iteration 18200, loss 0.26622337102890015\n",
      "iteration 18300, loss 0.24703951179981232\n",
      "TEST accuracy at end of epoch 46: 0.8624\n",
      "TRAIN accuracy at end of epoch 46: 0.94\n",
      "                          epoch 47\n",
      "iteration 18400, loss 0.2736358046531677\n",
      "iteration 18500, loss 0.1318315863609314\n",
      "iteration 18600, loss 0.19240647554397583\n",
      "iteration 18700, loss 0.18448597192764282\n",
      "TEST accuracy at end of epoch 47: 0.8415\n",
      "TRAIN accuracy at end of epoch 47: 0.86\n",
      "                          epoch 48\n",
      "iteration 18800, loss 0.27047109603881836\n",
      "iteration 18900, loss 0.22973698377609253\n",
      "iteration 19000, loss 0.20547309517860413\n",
      "iteration 19100, loss 0.097441665828228\n",
      "TEST accuracy at end of epoch 48: 0.8518\n",
      "TRAIN accuracy at end of epoch 48: 0.87\n",
      "                          epoch 49\n",
      "iteration 19200, loss 0.1372137814760208\n",
      "iteration 19300, loss 0.2707374691963196\n",
      "iteration 19400, loss 0.2303990125656128\n",
      "iteration 19500, loss 0.30901455879211426\n",
      "TEST accuracy at end of epoch 49: 0.8788\n",
      "TRAIN accuracy at end of epoch 49: 0.92\n",
      "                          epoch 50\n",
      "iteration 19600, loss 0.13791099190711975\n",
      "iteration 19700, loss 0.11496010422706604\n",
      "iteration 19800, loss 0.2803008556365967\n",
      "iteration 19900, loss 0.21422232687473297\n",
      "TEST accuracy at end of epoch 50: 0.8521\n",
      "TRAIN accuracy at end of epoch 50: 0.92\n",
      "                          epoch 51\n",
      "iteration 20000, loss 0.20745472609996796\n",
      "iteration 20100, loss 0.14946946501731873\n",
      "iteration 20200, loss 0.3109111189842224\n",
      "iteration 20300, loss 0.2568270266056061\n",
      "TEST accuracy at end of epoch 51: 0.8717\n",
      "TRAIN accuracy at end of epoch 51: 0.93\n",
      "                          epoch 52\n",
      "iteration 20400, loss 0.2417697310447693\n",
      "iteration 20500, loss 0.15409816801548004\n",
      "iteration 20600, loss 0.2161334753036499\n",
      "iteration 20700, loss 0.21957547962665558\n",
      "TEST accuracy at end of epoch 52: 0.8679\n",
      "TRAIN accuracy at end of epoch 52: 0.92\n",
      "                          epoch 53\n",
      "iteration 20800, loss 0.20462088286876678\n",
      "iteration 20900, loss 0.13504280149936676\n",
      "iteration 21000, loss 0.13656315207481384\n",
      "iteration 21100, loss 0.12163261324167252\n",
      "TEST accuracy at end of epoch 53: 0.8526\n",
      "TRAIN accuracy at end of epoch 53: 0.84\n",
      "                          epoch 54\n",
      "iteration 21200, loss 0.17192339897155762\n",
      "iteration 21300, loss 0.14793215692043304\n",
      "iteration 21400, loss 0.27987968921661377\n",
      "iteration 21500, loss 0.1059858500957489\n",
      "TEST accuracy at end of epoch 54: 0.8582\n",
      "TRAIN accuracy at end of epoch 54: 0.94\n",
      "                          epoch 55\n",
      "iteration 21600, loss 0.22145482897758484\n",
      "iteration 21700, loss 0.1615595817565918\n",
      "iteration 21800, loss 0.11812207102775574\n",
      "TEST accuracy at end of epoch 55: 0.8486\n",
      "TRAIN accuracy at end of epoch 55: 0.92\n",
      "                          epoch 56\n",
      "iteration 21900, loss 0.09881818294525146\n",
      "iteration 22000, loss 0.19793394207954407\n",
      "iteration 22100, loss 0.21397578716278076\n",
      "iteration 22200, loss 0.20009487867355347\n",
      "TEST accuracy at end of epoch 56: 0.8723\n",
      "TRAIN accuracy at end of epoch 56: 0.93\n",
      "                          epoch 57\n",
      "iteration 22300, loss 0.17630809545516968\n",
      "iteration 22400, loss 0.0756765753030777\n",
      "iteration 22500, loss 0.21136480569839478\n",
      "iteration 22600, loss 0.3050720989704132\n",
      "TEST accuracy at end of epoch 57: 0.8267\n",
      "TRAIN accuracy at end of epoch 57: 0.9\n",
      "                          epoch 58\n",
      "iteration 22700, loss 0.2486121654510498\n",
      "iteration 22800, loss 0.1452423334121704\n",
      "iteration 22900, loss 0.2861916422843933\n",
      "iteration 23000, loss 0.2711764872074127\n",
      "TEST accuracy at end of epoch 58: 0.8701\n",
      "TRAIN accuracy at end of epoch 58: 0.97\n",
      "                          epoch 59\n",
      "iteration 23100, loss 0.1004173755645752\n",
      "iteration 23200, loss 0.14123743772506714\n",
      "iteration 23300, loss 0.12980683147907257\n",
      "iteration 23400, loss 0.3612639009952545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST accuracy at end of epoch 59: 0.8361\n",
      "TRAIN accuracy at end of epoch 59: 0.91\n",
      "                          epoch 60\n",
      "iteration 23500, loss 0.08279465138912201\n",
      "iteration 23600, loss 0.20291973650455475\n",
      "iteration 23700, loss 0.19618627429008484\n",
      "iteration 23800, loss 0.1557682305574417\n",
      "TEST accuracy at end of epoch 60: 0.8615\n",
      "TRAIN accuracy at end of epoch 60: 0.93\n",
      "                          epoch 61\n",
      "iteration 23900, loss 0.17023834586143494\n",
      "iteration 24000, loss 0.18106308579444885\n",
      "iteration 24100, loss 0.1916317641735077\n",
      "iteration 24200, loss 0.13745538890361786\n",
      "TEST accuracy at end of epoch 61: 0.8577\n",
      "TRAIN accuracy at end of epoch 61: 0.95\n",
      "                          epoch 62\n",
      "iteration 24300, loss 0.1636410802602768\n",
      "iteration 24400, loss 0.20321042835712433\n",
      "iteration 24500, loss 0.1872178614139557\n",
      "iteration 24600, loss 0.20213976502418518\n",
      "TEST accuracy at end of epoch 62: 0.8559\n",
      "TRAIN accuracy at end of epoch 62: 0.99\n",
      "                          epoch 63\n",
      "iteration 24700, loss 0.13800469040870667\n",
      "iteration 24800, loss 0.12161198258399963\n",
      "iteration 24900, loss 0.10108388960361481\n",
      "iteration 25000, loss 0.0956985354423523\n",
      "TEST accuracy at end of epoch 63: 0.8686\n",
      "TRAIN accuracy at end of epoch 63: 0.95\n",
      "                          epoch 64\n",
      "iteration 25100, loss 0.11317788064479828\n",
      "iteration 25200, loss 0.0964842438697815\n",
      "iteration 25300, loss 0.13704945147037506\n",
      "iteration 25400, loss 0.19485899806022644\n",
      "TEST accuracy at end of epoch 64: 0.8577\n",
      "TRAIN accuracy at end of epoch 64: 0.92\n",
      "                          epoch 65\n",
      "iteration 25500, loss 0.1217351034283638\n",
      "iteration 25600, loss 0.1125822365283966\n",
      "iteration 25700, loss 0.1331826001405716\n",
      "iteration 25800, loss 0.16787517070770264\n",
      "TEST accuracy at end of epoch 65: 0.8546\n",
      "TRAIN accuracy at end of epoch 65: 0.91\n",
      "                          epoch 66\n",
      "iteration 25900, loss 0.0967371016740799\n",
      "iteration 26000, loss 0.29271191358566284\n",
      "iteration 26100, loss 0.17398488521575928\n",
      "TEST accuracy at end of epoch 66: 0.8698\n",
      "TRAIN accuracy at end of epoch 66: 0.96\n",
      "                          epoch 67\n",
      "iteration 26200, loss 0.14834290742874146\n",
      "iteration 26300, loss 0.11890314519405365\n",
      "iteration 26400, loss 0.14158356189727783\n",
      "iteration 26500, loss 0.1626797616481781\n",
      "TEST accuracy at end of epoch 67: 0.8655\n",
      "TRAIN accuracy at end of epoch 67: 0.95\n",
      "                          epoch 68\n",
      "iteration 26600, loss 0.14663586020469666\n",
      "iteration 26700, loss 0.14802947640419006\n",
      "iteration 26800, loss 0.16659487783908844\n",
      "iteration 26900, loss 0.10247565805912018\n",
      "TEST accuracy at end of epoch 68: 0.8708\n",
      "TRAIN accuracy at end of epoch 68: 0.92\n",
      "                          epoch 69\n",
      "iteration 27000, loss 0.11885572969913483\n",
      "iteration 27100, loss 0.08132227510213852\n",
      "iteration 27200, loss 0.17652103304862976\n",
      "iteration 27300, loss 0.13936707377433777\n",
      "TEST accuracy at end of epoch 69: 0.8637\n",
      "TRAIN accuracy at end of epoch 69: 0.95\n",
      "                          epoch 70\n",
      "iteration 27400, loss 0.1806110143661499\n",
      "iteration 27500, loss 0.17527440190315247\n",
      "iteration 27600, loss 0.12529179453849792\n",
      "iteration 27700, loss 0.17433276772499084\n",
      "TEST accuracy at end of epoch 70: 0.861\n",
      "TRAIN accuracy at end of epoch 70: 0.91\n",
      "                          epoch 71\n",
      "iteration 27800, loss 0.1809282898902893\n",
      "iteration 27900, loss 0.2395421862602234\n",
      "iteration 28000, loss 0.08312301337718964\n",
      "iteration 28100, loss 0.11548589169979095\n",
      "TEST accuracy at end of epoch 71: 0.8735\n",
      "TRAIN accuracy at end of epoch 71: 0.98\n",
      "                          epoch 72\n",
      "iteration 28200, loss 0.20813216269016266\n",
      "iteration 28300, loss 0.10734176635742188\n",
      "iteration 28400, loss 0.12237387895584106\n",
      "iteration 28500, loss 0.18178020417690277\n",
      "TEST accuracy at end of epoch 72: 0.8658\n",
      "TRAIN accuracy at end of epoch 72: 0.94\n",
      "                          epoch 73\n",
      "iteration 28600, loss 0.08439602702856064\n",
      "iteration 28700, loss 0.09018418937921524\n",
      "iteration 28800, loss 0.14017018675804138\n",
      "iteration 28900, loss 0.13862305879592896\n",
      "TEST accuracy at end of epoch 73: 0.8676\n",
      "TRAIN accuracy at end of epoch 73: 0.91\n",
      "                          epoch 74\n",
      "iteration 29000, loss 0.159784734249115\n",
      "iteration 29100, loss 0.08333227783441544\n",
      "iteration 29200, loss 0.12702372670173645\n",
      "iteration 29300, loss 0.13653403520584106\n",
      "TEST accuracy at end of epoch 74: 0.8621\n",
      "TRAIN accuracy at end of epoch 74: 0.94\n",
      "                          epoch 75\n",
      "iteration 29400, loss 0.07360809296369553\n",
      "iteration 29500, loss 0.21179157495498657\n",
      "iteration 29600, loss 0.11302055418491364\n",
      "iteration 29700, loss 0.18271689116954803\n",
      "TEST accuracy at end of epoch 75: 0.8717\n",
      "TRAIN accuracy at end of epoch 75: 0.96\n",
      "                          epoch 76\n",
      "iteration 29800, loss 0.08368311077356339\n",
      "iteration 29900, loss 0.10848990082740784\n",
      "iteration 30000, loss 0.2281993180513382\n",
      "iteration 30100, loss 0.07137104123830795\n",
      "TEST accuracy at end of epoch 76: 0.8596\n",
      "TRAIN accuracy at end of epoch 76: 0.93\n",
      "                          epoch 77\n",
      "iteration 30200, loss 0.11998659372329712\n",
      "iteration 30300, loss 0.12695680558681488\n",
      "iteration 30400, loss 0.06584426760673523\n",
      "TEST accuracy at end of epoch 77: 0.8687\n",
      "TRAIN accuracy at end of epoch 77: 0.95\n",
      "                          epoch 78\n",
      "iteration 30500, loss 0.06987965106964111\n",
      "iteration 30600, loss 0.10989506542682648\n",
      "iteration 30700, loss 0.12635107338428497\n",
      "iteration 30800, loss 0.16121166944503784\n",
      "TEST accuracy at end of epoch 78: 0.8729\n",
      "TRAIN accuracy at end of epoch 78: 0.95\n",
      "                          epoch 79\n",
      "iteration 30900, loss 0.10352951288223267\n",
      "iteration 31000, loss 0.057485394179821014\n",
      "iteration 31100, loss 0.16032832860946655\n",
      "iteration 31200, loss 0.1986931413412094\n",
      "TEST accuracy at end of epoch 79: 0.8692\n",
      "TRAIN accuracy at end of epoch 79: 0.99\n",
      "                          epoch 80\n",
      "iteration 31300, loss 0.10376466810703278\n",
      "iteration 31400, loss 0.14279921352863312\n",
      "iteration 31500, loss 0.12898120284080505\n",
      "iteration 31600, loss 0.051732733845710754\n",
      "TEST accuracy at end of epoch 80: 0.8631\n",
      "TRAIN accuracy at end of epoch 80: 0.93\n",
      "                          epoch 81\n",
      "iteration 31700, loss 0.06674383580684662\n",
      "iteration 31800, loss 0.0430380143225193\n",
      "iteration 31900, loss 0.10644762963056564\n",
      "iteration 32000, loss 0.15147443115711212\n",
      "TEST accuracy at end of epoch 81: 0.8814\n",
      "TRAIN accuracy at end of epoch 81: 0.96\n",
      "                          epoch 82\n",
      "iteration 32100, loss 0.14652124047279358\n",
      "iteration 32200, loss 0.10786300152540207\n",
      "iteration 32300, loss 0.17148257791996002\n",
      "iteration 32400, loss 0.10097342729568481\n",
      "TEST accuracy at end of epoch 82: 0.8762\n",
      "TRAIN accuracy at end of epoch 82: 0.97\n",
      "                          epoch 83\n",
      "iteration 32500, loss 0.04766746237874031\n",
      "iteration 32600, loss 0.09572635591030121\n",
      "iteration 32700, loss 0.07065832614898682\n",
      "iteration 32800, loss 0.17068663239479065\n",
      "TEST accuracy at end of epoch 83: 0.8774\n",
      "TRAIN accuracy at end of epoch 83: 0.96\n",
      "                          epoch 84\n",
      "iteration 32900, loss 0.08888399600982666\n",
      "iteration 33000, loss 0.12300337105989456\n",
      "iteration 33100, loss 0.21180255711078644\n",
      "iteration 33200, loss 0.11517917364835739\n",
      "TEST accuracy at end of epoch 84: 0.8697\n",
      "TRAIN accuracy at end of epoch 84: 0.96\n",
      "                          epoch 85\n",
      "iteration 33300, loss 0.08831371366977692\n",
      "iteration 33400, loss 0.11228623986244202\n",
      "iteration 33500, loss 0.14737096428871155\n",
      "iteration 33600, loss 0.12223626673221588\n",
      "TEST accuracy at end of epoch 85: 0.8767\n",
      "TRAIN accuracy at end of epoch 85: 0.98\n",
      "                          epoch 86\n",
      "iteration 33700, loss 0.08553978055715561\n",
      "iteration 33800, loss 0.11057659983634949\n",
      "iteration 33900, loss 0.14998272061347961\n",
      "iteration 34000, loss 0.08612680435180664\n",
      "TEST accuracy at end of epoch 86: 0.8717\n",
      "TRAIN accuracy at end of epoch 86: 0.98\n",
      "                          epoch 87\n",
      "iteration 34100, loss 0.14449146389961243\n",
      "iteration 34200, loss 0.14472545683383942\n",
      "iteration 34300, loss 0.10034068673849106\n",
      "iteration 34400, loss 0.056030504405498505\n",
      "TEST accuracy at end of epoch 87: 0.8616\n",
      "TRAIN accuracy at end of epoch 87: 0.98\n",
      "                          epoch 88\n",
      "iteration 34500, loss 0.1353297084569931\n",
      "iteration 34600, loss 0.14020377397537231\n",
      "iteration 34700, loss 0.09743864089250565\n",
      "TEST accuracy at end of epoch 88: 0.8629\n",
      "TRAIN accuracy at end of epoch 88: 0.96\n",
      "                          epoch 89\n",
      "iteration 34800, loss 0.19892580807209015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 34900, loss 0.04619318246841431\n",
      "iteration 35000, loss 0.07872207462787628\n",
      "iteration 35100, loss 0.15022046864032745\n",
      "TEST accuracy at end of epoch 89: 0.8701\n",
      "TRAIN accuracy at end of epoch 89: 0.93\n",
      "                          epoch 90\n",
      "iteration 35200, loss 0.12704172730445862\n",
      "iteration 35300, loss 0.044222958385944366\n",
      "iteration 35400, loss 0.09116409718990326\n",
      "iteration 35500, loss 0.1441473811864853\n",
      "TEST accuracy at end of epoch 90: 0.8685\n",
      "TRAIN accuracy at end of epoch 90: 0.96\n",
      "                          epoch 91\n",
      "iteration 35600, loss 0.1969369798898697\n",
      "iteration 35700, loss 0.08224023878574371\n",
      "iteration 35800, loss 0.15522998571395874\n",
      "iteration 35900, loss 0.13078123331069946\n",
      "TEST accuracy at end of epoch 91: 0.8819\n",
      "TRAIN accuracy at end of epoch 91: 0.99\n",
      "                          epoch 92\n",
      "iteration 36000, loss 0.08866806328296661\n",
      "iteration 36100, loss 0.08871808648109436\n",
      "iteration 36200, loss 0.11323952674865723\n",
      "iteration 36300, loss 0.053635697811841965\n",
      "TEST accuracy at end of epoch 92: 0.8663\n",
      "TRAIN accuracy at end of epoch 92: 0.97\n",
      "                          epoch 93\n",
      "iteration 36400, loss 0.05075886845588684\n",
      "iteration 36500, loss 0.036861639469861984\n",
      "iteration 36600, loss 0.0854274332523346\n",
      "iteration 36700, loss 0.11169414222240448\n",
      "TEST accuracy at end of epoch 93: 0.8862\n",
      "TRAIN accuracy at end of epoch 93: 0.97\n",
      "                          epoch 94\n",
      "iteration 36800, loss 0.1143944039940834\n",
      "iteration 36900, loss 0.060655225068330765\n",
      "iteration 37000, loss 0.08784442394971848\n",
      "iteration 37100, loss 0.16048352420330048\n",
      "TEST accuracy at end of epoch 94: 0.8807\n",
      "TRAIN accuracy at end of epoch 94: 0.99\n",
      "                          epoch 95\n",
      "iteration 37200, loss 0.14218366146087646\n",
      "iteration 37300, loss 0.17085818946361542\n",
      "iteration 37400, loss 0.09946630895137787\n",
      "iteration 37500, loss 0.06809113919734955\n",
      "TEST accuracy at end of epoch 95: 0.8823\n",
      "TRAIN accuracy at end of epoch 95: 0.97\n",
      "                          epoch 96\n",
      "iteration 37600, loss 0.028105899691581726\n",
      "iteration 37700, loss 0.06078775227069855\n",
      "iteration 37800, loss 0.10554333031177521\n",
      "iteration 37900, loss 0.09729635715484619\n",
      "TEST accuracy at end of epoch 96: 0.8865\n",
      "TRAIN accuracy at end of epoch 96: 0.98\n",
      "                          epoch 97\n",
      "iteration 38000, loss 0.09373132884502411\n",
      "iteration 38100, loss 0.07913750410079956\n",
      "iteration 38200, loss 0.10784070938825607\n",
      "iteration 38300, loss 0.05650103837251663\n",
      "TEST accuracy at end of epoch 97: 0.8833\n",
      "TRAIN accuracy at end of epoch 97: 0.98\n",
      "                          epoch 98\n",
      "iteration 38400, loss 0.17356780171394348\n",
      "iteration 38500, loss 0.11171256005764008\n",
      "iteration 38600, loss 0.08599410951137543\n",
      "iteration 38700, loss 0.07101279497146606\n",
      "TEST accuracy at end of epoch 98: 0.8773\n",
      "TRAIN accuracy at end of epoch 98: 0.96\n",
      "                          epoch 99\n",
      "iteration 38800, loss 0.05108656361699104\n",
      "iteration 38900, loss 0.02964375540614128\n",
      "iteration 39000, loss 0.058889247477054596\n",
      "iteration 39100, loss 0.13617441058158875\n",
      "TEST accuracy at end of epoch 99: 0.8832\n",
      "TRAIN accuracy at end of epoch 99: 0.98\n",
      "                          epoch 100\n",
      "iteration 39200, loss 0.04688500612974167\n",
      "iteration 39300, loss 0.13368020951747894\n",
      "iteration 39400, loss 0.04612939804792404\n",
      "TEST accuracy at end of epoch 100: 0.8826\n",
      "TRAIN accuracy at end of epoch 100: 0.96\n",
      "                          epoch 101\n",
      "iteration 39500, loss 0.07666777074337006\n",
      "iteration 39600, loss 0.06833495199680328\n",
      "iteration 39700, loss 0.021204225718975067\n",
      "iteration 39800, loss 0.08060561120510101\n",
      "TEST accuracy at end of epoch 101: 0.8769\n",
      "TRAIN accuracy at end of epoch 101: 1.0\n",
      "                          epoch 102\n",
      "iteration 39900, loss 0.042956653982400894\n",
      "iteration 40000, loss 0.05549325421452522\n",
      "iteration 40100, loss 0.05916961282491684\n",
      "iteration 40200, loss 0.053015366196632385\n",
      "TEST accuracy at end of epoch 102: 0.8818\n",
      "TRAIN accuracy at end of epoch 102: 0.96\n",
      "                          epoch 103\n",
      "iteration 40300, loss 0.07397766411304474\n",
      "iteration 40400, loss 0.07254482805728912\n",
      "iteration 40500, loss 0.059383682906627655\n",
      "iteration 40600, loss 0.08420389145612717\n",
      "TEST accuracy at end of epoch 103: 0.8791\n",
      "TRAIN accuracy at end of epoch 103: 0.95\n",
      "                          epoch 104\n",
      "iteration 40700, loss 0.04956100881099701\n",
      "iteration 40800, loss 0.08779789507389069\n",
      "iteration 40900, loss 0.05998508632183075\n",
      "iteration 41000, loss 0.13245585560798645\n",
      "TEST accuracy at end of epoch 104: 0.8618\n",
      "TRAIN accuracy at end of epoch 104: 0.94\n",
      "                          epoch 105\n",
      "iteration 41100, loss 0.08779413998126984\n",
      "iteration 41200, loss 0.11933431029319763\n",
      "iteration 41300, loss 0.08396640419960022\n",
      "iteration 41400, loss 0.07084319740533829\n",
      "TEST accuracy at end of epoch 105: 0.8867\n",
      "TRAIN accuracy at end of epoch 105: 0.99\n",
      "                          epoch 106\n",
      "iteration 41500, loss 0.054374150931835175\n",
      "iteration 41600, loss 0.0323815643787384\n",
      "iteration 41700, loss 0.0664922222495079\n",
      "iteration 41800, loss 0.045992836356163025\n",
      "TEST accuracy at end of epoch 106: 0.8832\n",
      "TRAIN accuracy at end of epoch 106: 0.97\n",
      "                          epoch 107\n",
      "iteration 41900, loss 0.03469943255186081\n",
      "iteration 42000, loss 0.0737992450594902\n",
      "iteration 42100, loss 0.12037820369005203\n",
      "iteration 42200, loss 0.06827683001756668\n",
      "TEST accuracy at end of epoch 107: 0.8712\n",
      "TRAIN accuracy at end of epoch 107: 0.95\n",
      "                          epoch 108\n",
      "iteration 42300, loss 0.049139369279146194\n",
      "iteration 42400, loss 0.045103348791599274\n",
      "iteration 42500, loss 0.04444052278995514\n",
      "iteration 42600, loss 0.02662859484553337\n",
      "TEST accuracy at end of epoch 108: 0.8908\n",
      "TRAIN accuracy at end of epoch 108: 0.97\n",
      "                          epoch 109\n",
      "iteration 42700, loss 0.04432722181081772\n",
      "iteration 42800, loss 0.07334712892770767\n",
      "iteration 42900, loss 0.04625631868839264\n",
      "iteration 43000, loss 0.05029592663049698\n",
      "TEST accuracy at end of epoch 109: 0.8747\n",
      "TRAIN accuracy at end of epoch 109: 0.98\n",
      "                          epoch 110\n",
      "iteration 43100, loss 0.11499239504337311\n",
      "iteration 43200, loss 0.047966502606868744\n",
      "iteration 43300, loss 0.06541682034730911\n",
      "iteration 43400, loss 0.17035777866840363\n",
      "TEST accuracy at end of epoch 110: 0.8807\n",
      "TRAIN accuracy at end of epoch 110: 0.99\n",
      "                          epoch 111\n",
      "iteration 43500, loss 0.10640959441661835\n",
      "iteration 43600, loss 0.09553316980600357\n",
      "iteration 43700, loss 0.09133584797382355\n",
      "TEST accuracy at end of epoch 111: 0.8686\n",
      "TRAIN accuracy at end of epoch 111: 0.96\n",
      "                          epoch 112\n",
      "iteration 43800, loss 0.024883676320314407\n",
      "iteration 43900, loss 0.06635375320911407\n",
      "iteration 44000, loss 0.17628005146980286\n",
      "iteration 44100, loss 0.04356371611356735\n",
      "TEST accuracy at end of epoch 112: 0.8838\n",
      "TRAIN accuracy at end of epoch 112: 1.0\n",
      "                          epoch 113\n",
      "iteration 44200, loss 0.09177003800868988\n",
      "iteration 44300, loss 0.048988670110702515\n",
      "iteration 44400, loss 0.09351757168769836\n",
      "iteration 44500, loss 0.06634556502103806\n",
      "TEST accuracy at end of epoch 113: 0.8849\n",
      "TRAIN accuracy at end of epoch 113: 0.99\n",
      "                          epoch 114\n",
      "iteration 44600, loss 0.06361819803714752\n",
      "iteration 44700, loss 0.07613000273704529\n",
      "iteration 44800, loss 0.08067485690116882\n",
      "iteration 44900, loss 0.014745247550308704\n",
      "TEST accuracy at end of epoch 114: 0.8874\n",
      "TRAIN accuracy at end of epoch 114: 1.0\n",
      "                          epoch 115\n",
      "iteration 45000, loss 0.045715123414993286\n",
      "iteration 45100, loss 0.06751339137554169\n",
      "iteration 45200, loss 0.0341353565454483\n",
      "iteration 45300, loss 0.0813206285238266\n",
      "TEST accuracy at end of epoch 115: 0.885\n",
      "TRAIN accuracy at end of epoch 115: 0.98\n",
      "                          epoch 116\n",
      "iteration 45400, loss 0.05679928511381149\n",
      "iteration 45500, loss 0.08748386800289154\n",
      "iteration 45600, loss 0.12524080276489258\n",
      "iteration 45700, loss 0.023020638152956963\n",
      "TEST accuracy at end of epoch 116: 0.8897\n",
      "TRAIN accuracy at end of epoch 116: 0.99\n",
      "                          epoch 117\n",
      "iteration 45800, loss 0.09911653399467468\n",
      "iteration 45900, loss 0.04257027059793472\n",
      "iteration 46000, loss 0.05285099893808365\n",
      "iteration 46100, loss 0.0738256648182869\n",
      "TEST accuracy at end of epoch 117: 0.8877\n",
      "TRAIN accuracy at end of epoch 117: 0.95\n",
      "                          epoch 118\n",
      "iteration 46200, loss 0.08812075853347778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 46300, loss 0.05756495147943497\n",
      "iteration 46400, loss 0.03203877434134483\n",
      "iteration 46500, loss 0.05609055608510971\n",
      "TEST accuracy at end of epoch 118: 0.8867\n",
      "TRAIN accuracy at end of epoch 118: 1.0\n",
      "                          epoch 119\n",
      "iteration 46600, loss 0.01152244582772255\n",
      "iteration 46700, loss 0.037555672228336334\n",
      "iteration 46800, loss 0.05246187373995781\n",
      "iteration 46900, loss 0.10225838422775269\n",
      "TEST accuracy at end of epoch 119: 0.884\n",
      "TRAIN accuracy at end of epoch 119: 0.97\n",
      "                          epoch 120\n",
      "iteration 47000, loss 0.047967419028282166\n",
      "iteration 47100, loss 0.030571136623620987\n",
      "iteration 47200, loss 0.059345126152038574\n",
      "iteration 47300, loss 0.05384526774287224\n",
      "TEST accuracy at end of epoch 120: 0.8848\n",
      "TRAIN accuracy at end of epoch 120: 0.99\n",
      "                          epoch 121\n",
      "iteration 47400, loss 0.07726170867681503\n",
      "iteration 47500, loss 0.05311797559261322\n",
      "iteration 47600, loss 0.03548964485526085\n",
      "iteration 47700, loss 0.02913610264658928\n",
      "TEST accuracy at end of epoch 121: 0.8911\n",
      "TRAIN accuracy at end of epoch 121: 0.99\n",
      "                          epoch 122\n",
      "iteration 47800, loss 0.06416851282119751\n",
      "iteration 47900, loss 0.023543938994407654\n",
      "iteration 48000, loss 0.0593271441757679\n",
      "TEST accuracy at end of epoch 122: 0.8854\n",
      "TRAIN accuracy at end of epoch 122: 0.98\n",
      "                          epoch 123\n",
      "iteration 48100, loss 0.055438678711652756\n",
      "iteration 48200, loss 0.04584228992462158\n",
      "iteration 48300, loss 0.04235750809311867\n",
      "iteration 48400, loss 0.04336436092853546\n",
      "TEST accuracy at end of epoch 123: 0.8871\n",
      "TRAIN accuracy at end of epoch 123: 1.0\n",
      "                          epoch 124\n",
      "iteration 48500, loss 0.01206088624894619\n",
      "iteration 48600, loss 0.04473165050148964\n",
      "iteration 48700, loss 0.059702638536691666\n",
      "iteration 48800, loss 0.047138456255197525\n",
      "TEST accuracy at end of epoch 124: 0.8872\n",
      "TRAIN accuracy at end of epoch 124: 0.99\n",
      "                          epoch 125\n",
      "iteration 48900, loss 0.032697465270757675\n",
      "iteration 49000, loss 0.05952365696430206\n",
      "iteration 49100, loss 0.03061198815703392\n",
      "iteration 49200, loss 0.07285082340240479\n",
      "TEST accuracy at end of epoch 125: 0.89\n",
      "TRAIN accuracy at end of epoch 125: 0.98\n",
      "                          epoch 126\n",
      "iteration 49300, loss 0.03242729231715202\n",
      "iteration 49400, loss 0.053103916347026825\n",
      "iteration 49500, loss 0.029203899204730988\n",
      "iteration 49600, loss 0.01859264075756073\n",
      "TEST accuracy at end of epoch 126: 0.8878\n",
      "TRAIN accuracy at end of epoch 126: 0.97\n",
      "                          epoch 127\n",
      "iteration 49700, loss 0.044623393565416336\n",
      "iteration 49800, loss 0.021731417626142502\n",
      "iteration 49900, loss 0.05309414491057396\n",
      "iteration 50000, loss 0.10108036547899246\n",
      "TEST accuracy at end of epoch 127: 0.8886\n",
      "TRAIN accuracy at end of epoch 127: 1.0\n",
      "                          epoch 128\n",
      "iteration 50100, loss 0.1355370581150055\n",
      "iteration 50200, loss 0.03422803431749344\n",
      "iteration 50300, loss 0.053772877901792526\n",
      "iteration 50400, loss 0.03207756206393242\n",
      "TEST accuracy at end of epoch 128: 0.8828\n",
      "TRAIN accuracy at end of epoch 128: 0.99\n",
      "                          epoch 129\n",
      "iteration 50500, loss 0.026048075407743454\n",
      "iteration 50600, loss 0.031277500092983246\n",
      "iteration 50700, loss 0.020082658156752586\n",
      "iteration 50800, loss 0.06169892102479935\n",
      "TEST accuracy at end of epoch 129: 0.8876\n",
      "TRAIN accuracy at end of epoch 129: 0.99\n",
      "                          epoch 130\n",
      "iteration 50900, loss 0.03252599760890007\n",
      "iteration 51000, loss 0.04757487401366234\n",
      "iteration 51100, loss 0.07399749010801315\n",
      "iteration 51200, loss 0.0544380247592926\n",
      "TEST accuracy at end of epoch 130: 0.8878\n",
      "TRAIN accuracy at end of epoch 130: 1.0\n",
      "                          epoch 131\n",
      "iteration 51300, loss 0.024759210646152496\n",
      "iteration 51400, loss 0.05782104283571243\n",
      "iteration 51500, loss 0.016895942389965057\n",
      "iteration 51600, loss 0.06384436041116714\n",
      "TEST accuracy at end of epoch 131: 0.8889\n",
      "TRAIN accuracy at end of epoch 131: 0.99\n",
      "                          epoch 132\n",
      "iteration 51700, loss 0.072661854326725\n",
      "iteration 51800, loss 0.02670462056994438\n",
      "iteration 51900, loss 0.03751005977392197\n",
      "iteration 52000, loss 0.06982511281967163\n",
      "TEST accuracy at end of epoch 132: 0.8849\n",
      "TRAIN accuracy at end of epoch 132: 0.98\n",
      "                          epoch 133\n",
      "iteration 52100, loss 0.026311829686164856\n",
      "iteration 52200, loss 0.04098789393901825\n",
      "iteration 52300, loss 0.04097443073987961\n",
      "TEST accuracy at end of epoch 133: 0.8921\n",
      "TRAIN accuracy at end of epoch 133: 1.0\n",
      "                          epoch 134\n",
      "iteration 52400, loss 0.037566132843494415\n",
      "iteration 52500, loss 0.053572531789541245\n",
      "iteration 52600, loss 0.006946747191250324\n",
      "iteration 52700, loss 0.0188527163118124\n",
      "TEST accuracy at end of epoch 134: 0.887\n",
      "TRAIN accuracy at end of epoch 134: 1.0\n",
      "                          epoch 135\n",
      "iteration 52800, loss 0.027378296479582787\n",
      "iteration 52900, loss 0.04775744676589966\n",
      "iteration 53000, loss 0.01296900399029255\n",
      "iteration 53100, loss 0.01134829968214035\n",
      "TEST accuracy at end of epoch 135: 0.8904\n",
      "TRAIN accuracy at end of epoch 135: 1.0\n",
      "                          epoch 136\n",
      "iteration 53200, loss 0.05067993700504303\n",
      "iteration 53300, loss 0.01895775832235813\n",
      "iteration 53400, loss 0.0222039632499218\n",
      "iteration 53500, loss 0.04426240921020508\n",
      "TEST accuracy at end of epoch 136: 0.8907\n",
      "TRAIN accuracy at end of epoch 136: 0.99\n",
      "                          epoch 137\n",
      "iteration 53600, loss 0.0732419416308403\n",
      "iteration 53700, loss 0.026401154696941376\n",
      "iteration 53800, loss 0.03625878691673279\n",
      "iteration 53900, loss 0.04286140576004982\n",
      "TEST accuracy at end of epoch 137: 0.8936\n",
      "TRAIN accuracy at end of epoch 137: 0.98\n",
      "                          epoch 138\n",
      "iteration 54000, loss 0.018237046897411346\n",
      "iteration 54100, loss 0.03556311875581741\n",
      "iteration 54200, loss 0.015656128525733948\n",
      "iteration 54300, loss 0.03157506883144379\n",
      "TEST accuracy at end of epoch 138: 0.8925\n",
      "TRAIN accuracy at end of epoch 138: 0.98\n",
      "                          epoch 139\n",
      "iteration 54400, loss 0.0071417540311813354\n",
      "iteration 54500, loss 0.06384488940238953\n",
      "iteration 54600, loss 0.016601212322711945\n",
      "iteration 54700, loss 0.04829421639442444\n",
      "TEST accuracy at end of epoch 139: 0.8917\n",
      "TRAIN accuracy at end of epoch 139: 0.99\n",
      "                          epoch 140\n",
      "iteration 54800, loss 0.07505016028881073\n",
      "iteration 54900, loss 0.011357883922755718\n",
      "iteration 55000, loss 0.051462315022945404\n",
      "iteration 55100, loss 0.01992410607635975\n",
      "TEST accuracy at end of epoch 140: 0.8937\n",
      "TRAIN accuracy at end of epoch 140: 1.0\n",
      "                          epoch 141\n",
      "iteration 55200, loss 0.009822798892855644\n",
      "iteration 55300, loss 0.0972936600446701\n",
      "iteration 55400, loss 0.024638015776872635\n",
      "iteration 55500, loss 0.0270080529153347\n",
      "TEST accuracy at end of epoch 141: 0.8939\n",
      "TRAIN accuracy at end of epoch 141: 1.0\n",
      "                          epoch 142\n",
      "iteration 55600, loss 0.0044388179667294025\n",
      "iteration 55700, loss 0.009345491416752338\n",
      "iteration 55800, loss 0.015706995502114296\n",
      "iteration 55900, loss 0.03343356400728226\n",
      "TEST accuracy at end of epoch 142: 0.8923\n",
      "TRAIN accuracy at end of epoch 142: 1.0\n",
      "                          epoch 143\n",
      "iteration 56000, loss 0.04203446954488754\n",
      "iteration 56100, loss 0.008329265750944614\n",
      "iteration 56200, loss 0.07255657017230988\n",
      "iteration 56300, loss 0.02256760746240616\n",
      "TEST accuracy at end of epoch 143: 0.8924\n",
      "TRAIN accuracy at end of epoch 143: 0.99\n",
      "                          epoch 144\n",
      "iteration 56400, loss 0.022831682115793228\n",
      "iteration 56500, loss 0.03416738659143448\n",
      "iteration 56600, loss 0.01713678054511547\n",
      "TEST accuracy at end of epoch 144: 0.8944\n",
      "TRAIN accuracy at end of epoch 144: 1.0\n",
      "                          epoch 145\n",
      "iteration 56700, loss 0.091295525431633\n",
      "iteration 56800, loss 0.034277964383363724\n",
      "iteration 56900, loss 0.03805689513683319\n",
      "iteration 57000, loss 0.025072909891605377\n",
      "TEST accuracy at end of epoch 145: 0.8914\n",
      "TRAIN accuracy at end of epoch 145: 1.0\n",
      "                          epoch 146\n",
      "iteration 57100, loss 0.029548192396759987\n",
      "iteration 57200, loss 0.05523663014173508\n",
      "iteration 57300, loss 0.02137521095573902\n",
      "iteration 57400, loss 0.0419827476143837\n",
      "TEST accuracy at end of epoch 146: 0.8941\n",
      "TRAIN accuracy at end of epoch 146: 1.0\n",
      "                          epoch 147\n",
      "iteration 57500, loss 0.01341264508664608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 57600, loss 0.10150399059057236\n",
      "iteration 57700, loss 0.047487806528806686\n",
      "iteration 57800, loss 0.007870638743042946\n",
      "TEST accuracy at end of epoch 147: 0.8946\n",
      "TRAIN accuracy at end of epoch 147: 1.0\n",
      "                          epoch 148\n",
      "iteration 57900, loss 0.020362116396427155\n",
      "iteration 58000, loss 0.019925817847251892\n",
      "iteration 58100, loss 0.018732452765107155\n",
      "iteration 58200, loss 0.006574984639883041\n",
      "TEST accuracy at end of epoch 148: 0.8917\n",
      "TRAIN accuracy at end of epoch 148: 1.0\n",
      "                          epoch 149\n",
      "iteration 58300, loss 0.018529290333390236\n",
      "iteration 58400, loss 0.026180462911725044\n",
      "iteration 58500, loss 0.019600164145231247\n",
      "iteration 58600, loss 0.006378712132573128\n",
      "TEST accuracy at end of epoch 149: 0.8943\n",
      "TRAIN accuracy at end of epoch 149: 1.0\n",
      "                          epoch 150\n",
      "iteration 58700, loss 0.032740212976932526\n",
      "iteration 58800, loss 0.007916470058262348\n",
      "iteration 58900, loss 0.038705311715602875\n",
      "iteration 59000, loss 0.014865572564303875\n",
      "TEST accuracy at end of epoch 150: 0.8916\n",
      "TRAIN accuracy at end of epoch 150: 1.0\n",
      "                          epoch 151\n",
      "iteration 59100, loss 0.01335056871175766\n",
      "iteration 59200, loss 0.021929197013378143\n",
      "iteration 59300, loss 0.022244110703468323\n",
      "iteration 59400, loss 0.009569520130753517\n",
      "TEST accuracy at end of epoch 151: 0.8956\n",
      "TRAIN accuracy at end of epoch 151: 0.99\n",
      "                          epoch 152\n",
      "iteration 59500, loss 0.012408791109919548\n",
      "iteration 59600, loss 0.018586594611406326\n",
      "iteration 59700, loss 0.04265550151467323\n",
      "iteration 59800, loss 0.030202288180589676\n",
      "TEST accuracy at end of epoch 152: 0.8932\n",
      "TRAIN accuracy at end of epoch 152: 1.0\n",
      "                          epoch 153\n",
      "iteration 59900, loss 0.0122635867446661\n",
      "iteration 60000, loss 0.10948628187179565\n",
      "iteration 60100, loss 0.03367431089282036\n",
      "iteration 60200, loss 0.04308465123176575\n",
      "TEST accuracy at end of epoch 153: 0.8964\n",
      "TRAIN accuracy at end of epoch 153: 1.0\n",
      "                          epoch 154\n",
      "iteration 60300, loss 0.008971480652689934\n",
      "iteration 60400, loss 0.02420360967516899\n",
      "iteration 60500, loss 0.0514787994325161\n",
      "iteration 60600, loss 0.04263600707054138\n",
      "TEST accuracy at end of epoch 154: 0.8925\n",
      "TRAIN accuracy at end of epoch 154: 0.99\n",
      "                          epoch 155\n",
      "iteration 60700, loss 0.03480370715260506\n",
      "iteration 60800, loss 0.005610336549580097\n",
      "iteration 60900, loss 0.008413087576627731\n",
      "TEST accuracy at end of epoch 155: 0.8944\n",
      "TRAIN accuracy at end of epoch 155: 1.0\n",
      "                          epoch 156\n",
      "iteration 61000, loss 0.05007309839129448\n",
      "iteration 61100, loss 0.018806464970111847\n",
      "iteration 61200, loss 0.018100548535585403\n",
      "iteration 61300, loss 0.009970451705157757\n",
      "TEST accuracy at end of epoch 156: 0.8922\n",
      "TRAIN accuracy at end of epoch 156: 1.0\n",
      "                          epoch 157\n",
      "iteration 61400, loss 0.009247908368706703\n",
      "iteration 61500, loss 0.0027049737982451916\n",
      "iteration 61600, loss 0.019012998789548874\n",
      "iteration 61700, loss 0.047912754118442535\n",
      "TEST accuracy at end of epoch 157: 0.8948\n",
      "TRAIN accuracy at end of epoch 157: 1.0\n",
      "                          epoch 158\n",
      "iteration 61800, loss 0.022708192467689514\n",
      "iteration 61900, loss 0.016518408432602882\n",
      "iteration 62000, loss 0.013751594349741936\n",
      "iteration 62100, loss 0.047637343406677246\n",
      "TEST accuracy at end of epoch 158: 0.8947\n",
      "TRAIN accuracy at end of epoch 158: 1.0\n",
      "                          epoch 159\n",
      "iteration 62200, loss 0.045320820063352585\n",
      "iteration 62300, loss 0.01867547258734703\n",
      "iteration 62400, loss 0.0220982376486063\n",
      "iteration 62500, loss 0.020298020914196968\n",
      "TEST accuracy at end of epoch 159: 0.8934\n",
      "TRAIN accuracy at end of epoch 159: 1.0\n",
      "                          epoch 160\n",
      "iteration 62600, loss 0.032186537981033325\n",
      "iteration 62700, loss 0.11439190804958344\n",
      "iteration 62800, loss 0.0017369259148836136\n",
      "iteration 62900, loss 0.014273514971137047\n",
      "TEST accuracy at end of epoch 160: 0.8937\n",
      "TRAIN accuracy at end of epoch 160: 1.0\n",
      "                          epoch 161\n",
      "iteration 63000, loss 0.023022137582302094\n",
      "iteration 63100, loss 0.01579304412007332\n",
      "iteration 63200, loss 0.010575724765658379\n",
      "iteration 63300, loss 0.05601732060313225\n",
      "TEST accuracy at end of epoch 161: 0.8958\n",
      "TRAIN accuracy at end of epoch 161: 1.0\n",
      "                          epoch 162\n",
      "iteration 63400, loss 0.012144450098276138\n",
      "iteration 63500, loss 0.02697184681892395\n",
      "iteration 63600, loss 0.012743894010782242\n",
      "iteration 63700, loss 0.07705426961183548\n",
      "TEST accuracy at end of epoch 162: 0.8961\n",
      "TRAIN accuracy at end of epoch 162: 1.0\n",
      "                          epoch 163\n",
      "iteration 63800, loss 0.017600759863853455\n",
      "iteration 63900, loss 0.010145168751478195\n",
      "iteration 64000, loss 0.018977636471390724\n",
      "iteration 64100, loss 0.01207333616912365\n",
      "TEST accuracy at end of epoch 163: 0.8939\n",
      "TRAIN accuracy at end of epoch 163: 1.0\n",
      "                          epoch 164\n",
      "iteration 64200, loss 0.005429960787296295\n",
      "iteration 64300, loss 0.039176180958747864\n",
      "iteration 64400, loss 0.025723570957779884\n",
      "iteration 64500, loss 0.011008599773049355\n",
      "TEST accuracy at end of epoch 164: 0.8953\n",
      "TRAIN accuracy at end of epoch 164: 1.0\n",
      "                          epoch 165\n",
      "iteration 64600, loss 0.0035789243411272764\n",
      "iteration 64700, loss 0.011097298935055733\n",
      "iteration 64800, loss 0.035832054913043976\n",
      "iteration 64900, loss 0.03064161352813244\n",
      "TEST accuracy at end of epoch 165: 0.8944\n",
      "TRAIN accuracy at end of epoch 165: 1.0\n",
      "                          epoch 166\n",
      "iteration 65000, loss 0.0477297343313694\n",
      "iteration 65100, loss 0.016994604840874672\n",
      "iteration 65200, loss 0.0355818010866642\n",
      "TEST accuracy at end of epoch 166: 0.8947\n",
      "TRAIN accuracy at end of epoch 166: 1.0\n",
      "                          epoch 167\n",
      "iteration 65300, loss 0.02083110809326172\n",
      "iteration 65400, loss 0.006342646665871143\n",
      "iteration 65500, loss 0.009441206231713295\n",
      "iteration 65600, loss 0.026421114802360535\n",
      "TEST accuracy at end of epoch 167: 0.8957\n",
      "TRAIN accuracy at end of epoch 167: 1.0\n",
      "                          epoch 168\n",
      "iteration 65700, loss 0.012323899194598198\n",
      "iteration 65800, loss 0.017744405195116997\n",
      "iteration 65900, loss 0.01480657048523426\n",
      "iteration 66000, loss 0.018138490617275238\n",
      "TEST accuracy at end of epoch 168: 0.8924\n",
      "TRAIN accuracy at end of epoch 168: 1.0\n",
      "                          epoch 169\n",
      "iteration 66100, loss 0.05918761342763901\n",
      "iteration 66200, loss 0.021444812417030334\n",
      "iteration 66300, loss 0.016457606106996536\n",
      "iteration 66400, loss 0.028085768222808838\n",
      "TEST accuracy at end of epoch 169: 0.8957\n",
      "TRAIN accuracy at end of epoch 169: 1.0\n",
      "                          epoch 170\n",
      "iteration 66500, loss 0.005385966971516609\n",
      "iteration 66600, loss 0.015523556619882584\n",
      "iteration 66700, loss 0.005956624634563923\n",
      "iteration 66800, loss 0.015050730668008327\n",
      "TEST accuracy at end of epoch 170: 0.8954\n",
      "TRAIN accuracy at end of epoch 170: 1.0\n",
      "                          epoch 171\n",
      "iteration 66900, loss 0.03537755087018013\n",
      "iteration 67000, loss 0.004896003287285566\n",
      "iteration 67100, loss 0.0050407899543643\n",
      "iteration 67200, loss 0.026057135313749313\n",
      "TEST accuracy at end of epoch 171: 0.8943\n",
      "TRAIN accuracy at end of epoch 171: 1.0\n",
      "                          epoch 172\n",
      "iteration 67300, loss 0.04768978804349899\n",
      "iteration 67400, loss 0.05342428758740425\n",
      "iteration 67500, loss 0.037774235010147095\n",
      "iteration 67600, loss 0.0282755009829998\n",
      "TEST accuracy at end of epoch 172: 0.8958\n",
      "TRAIN accuracy at end of epoch 172: 1.0\n",
      "                          epoch 173\n",
      "iteration 67700, loss 0.013404280878603458\n",
      "iteration 67800, loss 0.004489576444029808\n",
      "iteration 67900, loss 0.008824199438095093\n",
      "iteration 68000, loss 0.07053865492343903\n",
      "TEST accuracy at end of epoch 173: 0.895\n",
      "TRAIN accuracy at end of epoch 173: 1.0\n",
      "                          epoch 174\n",
      "iteration 68100, loss 0.007675220724195242\n",
      "iteration 68200, loss 0.019397558644413948\n",
      "iteration 68300, loss 0.03234638646245003\n",
      "iteration 68400, loss 0.021962052211165428\n",
      "TEST accuracy at end of epoch 174: 0.8958\n",
      "TRAIN accuracy at end of epoch 174: 1.0\n",
      "                          epoch 175\n",
      "iteration 68500, loss 0.010957345366477966\n",
      "iteration 68600, loss 0.009018038399517536\n",
      "iteration 68700, loss 0.01898466981947422\n",
      "iteration 68800, loss 0.021887073293328285\n",
      "TEST accuracy at end of epoch 175: 0.895\n",
      "TRAIN accuracy at end of epoch 175: 1.0\n",
      "                          epoch 176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 68900, loss 0.03824051469564438\n",
      "iteration 69000, loss 0.004726950544863939\n",
      "iteration 69100, loss 0.007577845361083746\n",
      "iteration 69200, loss 0.03926466777920723\n",
      "TEST accuracy at end of epoch 176: 0.8951\n",
      "TRAIN accuracy at end of epoch 176: 1.0\n",
      "                          epoch 177\n",
      "iteration 69300, loss 0.03045857697725296\n",
      "iteration 69400, loss 0.015960827469825745\n",
      "iteration 69500, loss 0.038914382457733154\n",
      "TEST accuracy at end of epoch 177: 0.8964\n",
      "TRAIN accuracy at end of epoch 177: 1.0\n",
      "                          epoch 178\n",
      "iteration 69600, loss 0.011934810318052769\n",
      "iteration 69700, loss 0.0034692431800067425\n",
      "iteration 69800, loss 0.006783662363886833\n",
      "iteration 69900, loss 0.009276960976421833\n",
      "TEST accuracy at end of epoch 178: 0.8947\n",
      "TRAIN accuracy at end of epoch 178: 1.0\n",
      "                          epoch 179\n",
      "iteration 70000, loss 0.014161511324346066\n",
      "iteration 70100, loss 0.012605207040905952\n",
      "iteration 70200, loss 0.007767150644212961\n",
      "iteration 70300, loss 0.028550084680318832\n",
      "TEST accuracy at end of epoch 179: 0.8952\n",
      "TRAIN accuracy at end of epoch 179: 1.0\n",
      "                          epoch 180\n",
      "iteration 70400, loss 0.013865608721971512\n",
      "iteration 70500, loss 0.024390269070863724\n",
      "iteration 70600, loss 0.014288989827036858\n",
      "iteration 70700, loss 0.025431126356124878\n",
      "TEST accuracy at end of epoch 180: 0.8949\n",
      "TRAIN accuracy at end of epoch 180: 1.0\n",
      "                          epoch 181\n",
      "iteration 70800, loss 0.02338624931871891\n",
      "iteration 70900, loss 0.020512938499450684\n",
      "iteration 71000, loss 0.033933158963918686\n",
      "iteration 71100, loss 0.01462831161916256\n",
      "TEST accuracy at end of epoch 181: 0.8967\n",
      "TRAIN accuracy at end of epoch 181: 1.0\n",
      "                          epoch 182\n",
      "iteration 71200, loss 0.02685745432972908\n",
      "iteration 71300, loss 0.00489274226129055\n",
      "iteration 71400, loss 0.06307949870824814\n",
      "iteration 71500, loss 0.03397057577967644\n",
      "TEST accuracy at end of epoch 182: 0.8958\n",
      "TRAIN accuracy at end of epoch 182: 1.0\n",
      "                          epoch 183\n",
      "iteration 71600, loss 0.03565981239080429\n",
      "iteration 71700, loss 0.019671419635415077\n",
      "iteration 71800, loss 0.004581798799335957\n",
      "iteration 71900, loss 0.036746665835380554\n",
      "TEST accuracy at end of epoch 183: 0.8957\n",
      "TRAIN accuracy at end of epoch 183: 1.0\n",
      "                          epoch 184\n",
      "iteration 72000, loss 0.022709621116518974\n",
      "iteration 72100, loss 0.004054771736264229\n",
      "iteration 72200, loss 0.01587633602321148\n",
      "iteration 72300, loss 0.004932937677949667\n",
      "TEST accuracy at end of epoch 184: 0.8959\n",
      "TRAIN accuracy at end of epoch 184: 1.0\n",
      "                          epoch 185\n",
      "iteration 72400, loss 0.01255490817129612\n",
      "iteration 72500, loss 0.0029134368523955345\n",
      "iteration 72600, loss 0.009450760670006275\n",
      "iteration 72700, loss 0.010624232701957226\n",
      "TEST accuracy at end of epoch 185: 0.8963\n",
      "TRAIN accuracy at end of epoch 185: 0.99\n",
      "                          epoch 186\n",
      "iteration 72800, loss 0.016017040237784386\n",
      "iteration 72900, loss 0.0694621205329895\n",
      "iteration 73000, loss 0.0024769320152699947\n",
      "iteration 73100, loss 0.01860380545258522\n",
      "TEST accuracy at end of epoch 186: 0.8955\n",
      "TRAIN accuracy at end of epoch 186: 1.0\n",
      "                          epoch 187\n",
      "iteration 73200, loss 0.008609682321548462\n",
      "iteration 73300, loss 0.020373472943902016\n",
      "iteration 73400, loss 0.040546756237745285\n",
      "iteration 73500, loss 0.02547718584537506\n",
      "TEST accuracy at end of epoch 187: 0.8968\n",
      "TRAIN accuracy at end of epoch 187: 1.0\n",
      "                          epoch 188\n",
      "iteration 73600, loss 0.022081464529037476\n",
      "iteration 73700, loss 0.026194998994469643\n",
      "iteration 73800, loss 0.049225445836782455\n",
      "TEST accuracy at end of epoch 188: 0.8956\n",
      "TRAIN accuracy at end of epoch 188: 1.0\n",
      "                          epoch 189\n",
      "iteration 73900, loss 0.011309453286230564\n",
      "iteration 74000, loss 0.03512874245643616\n",
      "iteration 74100, loss 0.07414665818214417\n",
      "iteration 74200, loss 0.00312921404838562\n",
      "TEST accuracy at end of epoch 189: 0.8953\n",
      "TRAIN accuracy at end of epoch 189: 1.0\n",
      "                          epoch 190\n",
      "iteration 74300, loss 0.02996131405234337\n",
      "iteration 74400, loss 0.00666416622698307\n",
      "iteration 74500, loss 0.00875432975590229\n",
      "iteration 74600, loss 0.027555236592888832\n",
      "TEST accuracy at end of epoch 190: 0.895\n",
      "TRAIN accuracy at end of epoch 190: 1.0\n",
      "                          epoch 191\n",
      "iteration 74700, loss 0.020153949037194252\n",
      "iteration 74800, loss 0.01263950765132904\n",
      "iteration 74900, loss 0.0251866914331913\n",
      "iteration 75000, loss 0.019434545189142227\n",
      "TEST accuracy at end of epoch 191: 0.8949\n",
      "TRAIN accuracy at end of epoch 191: 1.0\n",
      "                          epoch 192\n",
      "iteration 75100, loss 0.015274027362465858\n",
      "iteration 75200, loss 0.07451719045639038\n",
      "iteration 75300, loss 0.00527519267052412\n",
      "iteration 75400, loss 0.04193335399031639\n",
      "TEST accuracy at end of epoch 192: 0.8952\n",
      "TRAIN accuracy at end of epoch 192: 1.0\n",
      "                          epoch 193\n",
      "iteration 75500, loss 0.011787480674684048\n",
      "iteration 75600, loss 0.008617985993623734\n",
      "iteration 75700, loss 0.022741202265024185\n",
      "iteration 75800, loss 0.017130037769675255\n",
      "TEST accuracy at end of epoch 193: 0.8957\n",
      "TRAIN accuracy at end of epoch 193: 1.0\n",
      "                          epoch 194\n",
      "iteration 75900, loss 0.03299714997410774\n",
      "iteration 76000, loss 0.0020151743665337563\n",
      "iteration 76100, loss 0.02061721682548523\n",
      "iteration 76200, loss 0.037699565291404724\n",
      "TEST accuracy at end of epoch 194: 0.8952\n",
      "TRAIN accuracy at end of epoch 194: 1.0\n",
      "                          epoch 195\n",
      "iteration 76300, loss 0.034550875425338745\n",
      "iteration 76400, loss 0.0363892987370491\n",
      "iteration 76500, loss 0.012843347154557705\n",
      "iteration 76600, loss 0.007363823242485523\n",
      "TEST accuracy at end of epoch 195: 0.895\n",
      "TRAIN accuracy at end of epoch 195: 1.0\n",
      "                          epoch 196\n",
      "iteration 76700, loss 0.03501228615641594\n",
      "iteration 76800, loss 0.016408178955316544\n",
      "iteration 76900, loss 0.0020636566914618015\n",
      "iteration 77000, loss 0.012432178482413292\n",
      "TEST accuracy at end of epoch 196: 0.8953\n",
      "TRAIN accuracy at end of epoch 196: 1.0\n",
      "                          epoch 197\n",
      "iteration 77100, loss 0.025772009044885635\n",
      "iteration 77200, loss 0.04139772430062294\n",
      "iteration 77300, loss 0.019079994410276413\n",
      "iteration 77400, loss 0.04325109347701073\n",
      "TEST accuracy at end of epoch 197: 0.8951\n",
      "TRAIN accuracy at end of epoch 197: 1.0\n",
      "                          epoch 198\n",
      "iteration 77500, loss 0.007362085860222578\n",
      "iteration 77600, loss 0.04294051602482796\n",
      "iteration 77700, loss 0.002396991243585944\n",
      "iteration 77800, loss 0.029853280633687973\n",
      "TEST accuracy at end of epoch 198: 0.8955\n",
      "TRAIN accuracy at end of epoch 198: 1.0\n",
      "                          epoch 199\n",
      "iteration 77900, loss 0.003598886076360941\n",
      "iteration 78000, loss 0.010051941499114037\n",
      "iteration 78100, loss 0.006087485235184431\n",
      "iteration 78200, loss 0.009060792624950409\n",
      "TEST accuracy at end of epoch 199: 0.8952\n",
      "TRAIN accuracy at end of epoch 199: 1.0\n",
      "╰┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╯\n"
     ]
    }
   ],
   "source": [
    "all_loss = {}\n",
    "all_acc_te = {}\n",
    "all_acc_tr = {}\n",
    "for res in [True, False]:\n",
    "    for n in reversed([3, 5, 7, 9]):\n",
    "        print(\"╭┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╮\")\n",
    "        print(f\"                         n: {n}, res: {res}\")\n",
    "        if res:\n",
    "            name = f\"res{n}\"\n",
    "        else:\n",
    "            name = f\"plain{n}\"\n",
    "            \n",
    "        g = tf.Graph()\n",
    "        with g.as_default():\n",
    "            nodes = create_graph(name, n=n, residual=res)\n",
    "\n",
    "        with tf.Session(graph = g) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            loss, acc_te, acc_tr = train(sess, nodes, n_epochs=200)\n",
    "        print(\"╰┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╯\")\n",
    "\n",
    "        all_loss[name] = loss\n",
    "        all_acc_te[name] = acc_te\n",
    "        all_acc_tr[name] = acc_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Plain\" convnets\n",
    "As expected, as we increase the size of the neural net, training loss increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f69f47c6400>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thecroc/.pyenv/versions/jupyter3/lib/python3.6/site-packages/IPython/core/events.py:73: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  func(*args, **kwargs)\n",
      "/home/thecroc/.pyenv/versions/jupyter3/lib/python3.6/site-packages/IPython/core/pylabtools.py:122: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig=plt.figure(figsize=(16, 4))\n",
    "plt.plot(all_loss['plain3'], alpha=.5)\n",
    "plt.plot(all_loss['plain5'], alpha=.5)\n",
    "plt.plot(all_loss['plain7'], alpha=.5)\n",
    "plt.plot(all_loss['plain9'], alpha=.5)\n",
    "plt.legend([f'plain{i}' for i in [3, 5, 7, 9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### residual networks\n",
    "It's harder to see here, but larger neural nets train better, which is the opposite of the \"plain\" networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f69f1d4c8d0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thecroc/.pyenv/versions/jupyter3/lib/python3.6/site-packages/IPython/core/events.py:73: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  func(*args, **kwargs)\n",
      "/home/thecroc/.pyenv/versions/jupyter3/lib/python3.6/site-packages/IPython/core/pylabtools.py:122: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig=plt.figure(figsize=(16, 4))\n",
    "plt.plot(all_loss['res3'], alpha=.5)\n",
    "plt.plot(all_loss['res5'], alpha=.5)\n",
    "plt.plot(all_loss['res7'], alpha=.5)\n",
    "plt.plot(all_loss['res9'], alpha=.5)\n",
    "plt.legend([f'res{i}' for i in [3, 5, 7, 9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproduction of He et al 2015, Fig 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 39.0625, 78.125, 117.1875, 156.25, 195.3125]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig=plt.figure(figsize=(16, 8))\n",
    "plt.plot(1 - np.array(all_acc_te['res3']), lineWidth=2, color='blue')\n",
    "plt.plot(1 - np.array(all_acc_te['res5']), lineWidth=2, color='green')\n",
    "plt.plot(1 - np.array(all_acc_te['res7']), lineWidth=2, color='orange')\n",
    "plt.plot(1 - np.array(all_acc_te['res9']), lineWidth=2, color='red')\n",
    "\n",
    "plt.plot(1 - np.array(all_acc_te['plain3']), '--', lineWidth=1, color='blue')\n",
    "plt.plot(1 - np.array(all_acc_te['plain5']), '--', lineWidth=1, color='green')\n",
    "plt.plot(1 - np.array(all_acc_te['plain7']), '--', lineWidth=1, color='orange')\n",
    "plt.plot(1 - np.array(all_acc_te['plain9']), '--', lineWidth=1, color='red')\n",
    "\n",
    "plt.legend([f'{ty}, n={i}' for ty in ['res', 'plain'] for i in [3, 5, 7, 9]])\n",
    "xlim = plt.xlim([0, len(all_acc_tr['res9'])])\n",
    "plt.plot(xlim, [0.2, 0.2], '--', lineWidth=.5, color='gray')\n",
    "plt.plot(xlim, [0.1, 0.1], '--', lineWidth=.5, color='gray')\n",
    "plt.plot(xlim, [0.05, 0.05], '--', lineWidth=.5, color='gray')\n",
    "plt.ylim([0, .21])\n",
    "locs = [x * 50000 / 128 / 10000 for x in range(0, 6000, 1000)]\n",
    "labels = range(5)\n",
    "plt.xticks(locs, labels)\n",
    "locs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
